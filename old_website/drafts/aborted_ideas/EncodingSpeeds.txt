## Encoding A Corpus

## Introduction

One of the least glamorous things that you get the privlidge of doing in computational musicology is encoding a corpus.
It has got to be one of the most boring, yet most nessecary tasks a music researcher can do.
In this post I'd like to accomplish two things:
First, I want to highlight the importance of why there is a constant need to be encoding new data for music research to grow as a research area. (generalization and representation and London)(And show some respect for those who do do this!) 
Second, having just completed a corpus for some of my dissertation work, I want go give a realistic idea of what goes into encoding a corpus in order to hopefully inspire more people to start with their own proejcts 

## Background To Enter Conversation | Digitial Representations

### What is current state, what is problem.

The field of computational musicology is absolutly dependent on having digitial representations of melodies in order to do large scale analyses.
HURON PAPER HERE
Currently there are lots of collections that do exits (kern repo link), but any musicologist worth their salt would probably notice a similar phenomena when going through collections like the kernrepo.
Much of the data comes from old, dead, white composers living in Europe from 1600--1800.
Either that or data comes from the essen collection.
Note thing about pop music and corpora--

Having a narrower focus presents two problems.
1. Making comparative claims are not as robust if you do not have adequate representation, especially cultural ones.
2. Not having new data constantly leads to ovefitting and a lack of ability to generalize.

For example, look at all of these studies making claims about Essen.
If for example this claim about XYX is going to hold (especially if we are taking a NHST epistomological approach) need to get some new samples from the same population (in this case, europen folk songs).
Until we do that going to suffer again from fact that Music Theory has yet again over explained a topic.


## What I did and Pretty Pictures that Explain it

Hopefully I have established the need for constanlty creating new data so that effects we think to exist are there.
What I am interested in studying is memory for melodies and as non-mooch, I want to be able to contribute.
So what I did was encode the entire Berkowitz sight singing melody book into kern.
Not availble for public use, but sample of 647 hopefully reflects individual operservations samples from larger populaiton of melodies used in signing classes (a big circular, I know).

Being a good systematic musicologist, i thought it would be intersted to time how long it took me to do the whole corpus and even went a step further to see how long did each individual melody take to encode?
It's a bit type A, but as I have written about elsewhere, think it was impotant to have firm boundaries between working and not.

I took all that information and have plotted it below.

Things you can see from the graph:
-Entire time it took to plot each melody
-Cumulative time of the entire corpus
-Increaes in encoding time with increase in complexity of melodies
-Decreaes in standardize time reflecting getting better as I did more melodies.


MEGAPLOT

Picture is worht a thousand words, but big take aways are 
- This takes time but will be worth it
- have own dataset that I can get a lot out of, especially when I combine it with other tools
- Have good, represenative sample for what I need, and sufficent N to make claims seem better
- Not working in a data poor environment here

## How to get started yourself

- Pick question that you want to answer first
- Is there a composer you are in love with and feel like other people might want to have? 
- Last year Bartok went into public domain?
- Feel there is an underreprented group that brings something to table -- see Shanahan and Shanahan 2014

- Not only helps reserach community and questions (if make it publicly availbile)
- Going to learn A LOT about computers by dipping toes into this area
- Great for students that need to learn GUI notation softare
- Ideal world if every undergrad had encoded just 1 piece during their time, quickly have representative canon
- Move towards a common good 

## Conclusion

i. people who encode deserve way more credit 
ii. generalization and representation matter
iii. think people should start their own projects, make your students encode!
iv. students-- learn a lot about computers, GUI software, think about WHY it's important to have lots of data. 

Since a large part of my upcoming dissertation is going to be talking about relationships between symbolic data and behavior, I thought it'd be good to talk about this process earlier on in the writing.
And because the world of computational musicology is not that big, it's worth taking a second to give a brief overview of why it's important to constantly be creating new data for our field.
The first, and probably most important reason, is that in computational musicology encoded melodies are our data!
Whereas reserach in psychology is often after some sort of effect dealing with reaction times or ratings on likert scales, the computational musicologist is interested in how the music as best represented by western notation, yes there are problems with that, can be used to help answer questions about musis.
For example, in answering questions about melodic arches .... and found that.
This study was great because it used huge number of melodies to suggest evidence for a certain hypothesis.
Or take for example this study that used ESSEN Collection.
Or this study that USED ESSEN COLLECTION and found that.
There's clearly a bit of a trend here... a lot of studies tend to rely on the same data to reach their conclusions.
If we are all basing our claims on the same dataset (which is also a big problematic ask Andrew Brinkman), then our claims about how these findings are supposed to generalize are diminished.

I. Lame-o hook to state what people are going to be spending their next ten minutes reading about
II. The brief background they need to enter the conversation or know why I care, hopefully to get them to care
III. What I did and the pretty pictures that explain it.
IV. The call 



## Encoding Speeds

If there is anything that Dan Shanahan and I disagree the most it is what is the most effective way to encoding melodies.

With my dissertation coming up in the next few months, I figured that if I were going to be spending a lot of time sitting behind my comptuer screen taking melodies and turing them into our beloved **kern format, I should have some sort of rule of thumb for how long a melody takes to encode.
This way I'll be able to ballpark how long it will take me to encode all of the melodies that I am hoping to use as the corpus for my dissertation.

Since the world of computational musicology is not that big, it's worth taking a second to give a brief overview of why it's important to constantly be creating new data for our field.
The first, and probably most important reason, is that in computational musicology encoded melodies are our data!
Whereas reserach in psychology is often after some sort of effect dealing with reaction times or ratings on likert scales, the computational musicologist is interested in how the music as best represented by western notation, yes there are problems with that, can be used to help answer questions about musis.
For example, in answering questions about melodic arches .... and found that.
This study was great because it used huge number of melodies to suggest evidence for a certain hypothesis.
Or take for example this study that used ESSEN Collection.
Or this study that USED ESSEN COLLECTION and found that.
There's clearly a bit of a trend here... a lot of studies tend to rely on the same data to reach their conclusions.
If we are all basing our claims on the same dataset (which is also a big problematic ask Andrew Brinkman), then our claims about how these findings are supposed to generalize are diminished.

If the integrity of our data and thus findings is not reason enough, another reason to encode more melodies is so that when we attempt to make claims about music perception or production, we are not falling into WEIRD research and thinking that because we used a sample of Western music that it is somehow representitive of all groups of people.
Some researchers have been noteably leading this effort (Shanahan and Shanahan, 2014), and it's essential to the field to have a diverse representaiton of what we claim to be looking at as argued for in London (representitive corpus).

I'm sure I will talk more about computational musicology in the future (big plans for this blog!), but the question that spurred this post is 
> Given the fact that we need more melodies all the time, what is the best way to do this?

My meetings with Dan pretty much boil down to two paths, each with their own mini paths.

1. Encode everything by hand by looking at the score, then entering it directly into either MuseScore 2 or (for the masochists) directly into the terminal using vi. 
2. Cut up a book, scan it, run it through optical recogition software, then spend your time cleaning up the errors before exporting it to XML, and then using humdrumextras to convert it to **kern.

Dan is a firm beliver that working directly with text file and vim is the way to go.
I on the other hand think that using something like MuseScore is a bit more sane.

Of course the only way to figure this out properly is to set up an experiment and analyze the results!

## The Experiment

Step one in setting up any experiment is picking a good experimental design given what you have.
Luckily at MCCL we have a few graduate students that were willing to help with this experiment.
Since this was just for fun and I probably should be finding better ways to spend my time, I only set the experiment up with two factors of interest.
1. If vim or musescore would be faster
2.  

 
