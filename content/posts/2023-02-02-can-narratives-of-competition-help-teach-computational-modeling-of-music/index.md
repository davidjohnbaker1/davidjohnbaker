---
title: Can narratives of competition help teach computational modeling of music?
author: Package Build
date: '2023-02-02'
slug: []
categories: []
tags: []
draft: yes
---

> Sketch of impluse talk I planned to deliver at [Monte Verita Workshop]()

One axis to conceptualize teaching is on a continuum from the procedural to the abstract. 
When I refer to procedural teaching, I am referring to teaching someone how to _do something_.
For example, how to run an experimental procedure with a participant, how make a scatter plot in R, how to program an experiment in Python, or how to analyse a specific type of experimental design.
With procedural learning, the learning objective or outcome, I find, is typically much clearer.
You either were able to produce the scatter plot, or not.
The experiment runs, or not. 

On the other hand are our more abstract goals we wish to teach. 
As in, what is the goal of music analysis?
What does it mean to do science well?
How do you learn to ask a good question? 
How can we better understand music perception via computational models? 

I think we can try to come up with clear learning objectives of what it means to do _good science_, but in some ways that does a disservice our students.
When doing science, we don't always know exactly how it will turn out. 
We don't know what the clear goal to check off is.

The question is: how do we check for understanding that student grasps the many moving parts needed for computational modeling of music?

Ideally we want to show what we as as scientists _do_.
We can take the form of reading papers where model is tested against data, but students often wonder why.
We have a lot of assumptions to even get to this point.

This is not helped by  always begin explicitly clear as we work our way from theory down to testable assertion of the world (hypothesis).
We need a better point of entry to show students WHAT we do, and WHY we do it.
Then when in this place, they find themselves in larger narrative. 

What is this narrative?
We might take our cue from the world of competition. 
I think this is a good idea because even outside of academia, nerds are familiar with things like The Netflix prize, Kaggle where there is an explicit goal: make a model that does X, it is judged by Y.

The field of MIR has this with MIREX.
It allows us to point to the state of the art in models.
There are winners, losers, progress, and **history** here.
It also gives us a narrative that tethers model to theories.

This of course means deciding on the questions we think should be asked.
Pooling resources for common format.
I think this would show a sign of maturing field. 
