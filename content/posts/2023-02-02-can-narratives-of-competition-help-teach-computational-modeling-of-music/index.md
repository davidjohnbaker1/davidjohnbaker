---
title: Can narratives of competition help teach computational modeling of music?
author: Package Build
date: '2023-02-16'
slug: []
categories: [impulse talk, music science, computation, pedagogy]
tags: [impulse talk, music science, computation, pedagogy]
draft: no
---

> Sketch of impluse talk I planned to deliver at [Monte Verita Workshop](https://davidjohnbaker.rbind.io/posts/2023-02-09-monte-verit-workshop-2023/)

One axis to conceptualize teaching is on a continuum from the procedural to the abstract. 
When I refer to procedural teaching, I am referring to teaching someone how to _do something_.
For example, how to run an experimental procedure with a participant, how make a scatter plot, how to program an experiment in Python, how to make a call to the Spotify API, or how to analyse a specific type of experimental design.
With procedural learning, the learning objective or outcome, I find, is typically much clearer.
You either were able to produce the scatter plot, or not.
The experiment runs, or not. 

On the other hand are our more abstract goals we wish to teach. 
As in, what is the goal of music analysis?
What does it mean to do science well?
How do you learn to ask a good question? 
How can we better understand music perception via computational models? 

I think we can try to come up with clear learning objectives of what it means to do _good science_, but in some ways that does a disservice our students.
When doing science, we don't always know exactly how it will turn out. 
It's not always clear what it means to make progress on a specific subject of study.

One problem I feel we have is that its sometimes difficult to link these more procedural learning goals with our more abstract ones when it comes to the computational modeling of music.

One possible solution to better link these is to focus on showing students what we as as scientists _do_.
This doesn't mean writing a paper that is ultimately published in a journal, but having a more surface level activity that we can point them to.

Instead of pointing them to a paper, instead maybe we point them to a competition-- something like [Kaggle](https://www.kaggle.com/), the [Netflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize), or the [MIREX tasks](https://www.music-ir.org/mirex/wiki/MIREX_HOME)-- that very directly shows how a group of researchers attempts to solve a clearly established problem. 

In addition to having a more tangible point of entry as to what computational modelers of music, _do_, having a resource would like this would allow for students to see how different questions have historically been addressed by researchers with the field.

This of course means centralizing resources and questions that are asked, pooling more resources and dealing with the evergreen problems of agreeing on data formats, but were we to have this, I think it would be a sign of maturing field. 