+++
abstract = "Our  understanding  of—and  preference  for—music  is dependent  upon  the  perception  of  human  agency.  Listeners  often speak  of how  computer-based  performances  lack  the  “soul”  of  a human  performer.  At  the  heart  of  perceived  animacy is  causality, which in music might be thought of as rubato, and other variations in timing. This study focuses on the role of variations in microtiming on the  perceived  animacy  of  a  musical  performance.    Recent  work has shown  that  the  perception  of  visual  animacy  is  likely  categorical, rather  than  gradual.  Although  a  number  of  studies have  examined auditory animacy, there has been very little research done on whether it might be thought of as a dichotomy of alive/not alive, rather than a continuum.  The  current  study  examines  the  specific  intricacies  of musical  animacy,  specifically,  how  microtiming  variations  of  inter-onset  intervals  contribute  to  the  perception  that  a  piece  was  human performed.  Additionally,  this  study  aims  to  examine  the  possible nature  of  categorical/continuous  perception  of  musical  animacy.  In Experiment  1:  “Rohum”,  computer  sequenced  MIDI  renditions  were manipulated   to   contain   set   random   fluctuations   of   inter-onset intervals.  In  Experiment  2:  “Humbot”,  participants  were  presented with  human  performances  digitally  recorded  using  MIDI  keyboards, and  were  asked  how  “alive”  each  performance  sounded  using  a  7-point   Likert   scale.   Human   performances   were   divided   into   ten degrees of quantization strength, increasing from raw performance to 100%  quantization.  Results  suggest  an  optimal  level  of  quantization strength  that  is  correlated  with  higher  perceived  animacy,  and  fixed random  fluctuations  of  IOI  are  not  a  good  indicator  of  human performance.   This   paper   discusses   the   role   of   external   stylistic assumptions  on  perceived  performances,  and  also  takes  into  account musical sophistication indices and experience."
authors = ["A Blust, D Baker, K Richard, D Shanahan"]
date = "2016-08-01"
image_preview = ""
math = true
publication_types = ["1"]
publication = "In *Proceedings of the 14th International Conference for Music Perception and Cognition*"
publication_short = ""
selected = false
title = "Music, animacy, and rubato: what makes music sound human?"
url_code = ""
url_dataset = ""
url_pdf = "https://www.researchgate.net/profile/Daniel_Shanahan/publication/307981632_Music_animacy_and_rubato_what_makes_music_sound_human/links/57d5faff08ae5f03b493298d/Music-animacy-and-rubato-what-makes-music-sound-human.pdf?_iepl%5BhomeFeedViewId%5D=wMDPHRQfz0itsNH9HHEs1Yub&_iepl%5Bcontexts%5D%5B0%5D=pcfhf&_iepl%5BinteractionType%5D=publicationDownload&origin=publication_detail&ev=pub_int_prw_xdl&msrp=MnH2B0-N4qsWyThrjlfIATtTP3uYoWcTpEBpCrarpFdnap3yJ0W52ScvPG-OMxXN1AqKUVSUoBxguIxyMgoSlqK1bMP8JiYaGZS_7ps-iYcDbqYWGleTcCOd.L8OrivmhsjGzD6PWOYYaTAGpf5tNUU7VK2mgoGoURVd74Z8fT6aYjidCaHK3YUQaXiLAGIMcUmQYULBi2SAT0FbXnpodjIemzbTNeA.rrfKVQCEP1rnDt6gxi5fOgyefMx-6sL63AxuQ03ELlnnUuWRC_HDueWQ7bS7FOWQTwmlA9LHSs7hW-M8k0ald58eDfLUoygm_rGWrA._M2whe7XCPHYGhSKus9W3Ts78jXHAZM9QJSJAre81nM0Yc0VoZa4V4V0QFCHlCd9c2YP1XyiISAt2EZ9iicy431Ih7UrYPZMsmlgoA"
url_project = ""
url_slides = ""
url_video = ""

+++

