---
title: "hire-me-ds-3"
author: "David John Baker"
date: "January 16, 2018"
output: html_document
---

```{r}
library(ggplot2)
library(data.table)
beer <- fread("../content/post/data/beer_reviews.csv") 
```


Two questions down, two to go!
For the third post in this series, I'll explore the question:

> Which of the factors (aroma, taste, appearance, palette) are most important in determining the overall quality of a beer?

Whereas the posts before were questions of sorting data, this is our first attempt to make some statistical models!
When asking questions of how much do certain factors contribute to determining another one, you are essentially dealing with a regression problem.

## Regression Problems 

While I assume most people who are reading this know what regression is, it's worth taking a second to talk about since according to all the blog posts that I have read one of attributes of being a good data scientist is being able to explain seemingly complicated, yet unfamiliar terms to equally as intelligent team members who just don't do math all the time.

I think the best way to explain regression models are that they are ways of making decent guesses about the world.
Let's say for example we were to play a game where I would give you some sort reward for being really good at guessing the ABV of a beer I picked at random from the 56,857 unique beers in this dataset.
If you had no other informaiton and knew that you were to get more of a reward for getting the closest to it, your best bet would probably be to pick the average ABV in all the beers.
It's a simple model, but in the long run it would be best bet.

Now let's say I tell you that you're allowed to know what kind of beer it is.
I give you a chart of all the average ABVs of each beer type, then before you make your guess, I tell you the type of beer that I am about to reveal and then you can consult your chart and change your guess accordingly.
In this situation, if you wanted to increase your chances of getting closer (or in statisitcal terms the opposite would be minimizing your residuals) you would be foolish not to guess the average ABV of that beer type since we know from the last post that beers vary based on their type!

You could imagine this process happening again and again with new charts I gave you and with each one you could get more and more accurate answers.
Some charts would be more helpful than others.
For example a chart on the average ABV of that brewery might be better than knowing where the beer is brewed.

Considering what charts would help you get closer to your guess is essentially what it's like to build regression models.
Of course it's a bit more complicated than that, but having this example will help explain my answer to question #3 later on.

## Tackling the Problem

So how do we predict the overal quality of a beer?
What informaiton would be good to know if we really wanted to have accurate guesses?
Well in this situation we can't be too picky since we only have a few different variables!

We know that our dependent varible is overall quality and that our independent variables are aroma, taste, appearance, palette.
At this point it's worht mentioning two things.

One is that there is a problem in this dataset that our data is not independent; we have many examples of the same preson rating multiple beers.
The reason this is a big deal is because you want your model to account for how different people tend to rate things.
We all have that one friend who thinks every beer is the best beer ever, and another one who just thinks everything is garbage.
There will be variability in their ratings, but both will be living in their own rating world.
And by mixing them all up, all that nice nuance is going to get washed out.
Or even worse you'll run into something like [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox).

But that's getting pretty technical.

The other point worth mentioning is the order we build these models.
There are generally two ways.
We can throw in all the variables at once and see what sticks.
Or if we have a good theory of what we _think_ will make good predictors, we enter them step by step.
The latter is by far the preferred way for theory-testing, but in this example we are just trying to explain what are the important predcitors out of the few we have. 

So after all of that, I am going to break both of the rules of what I said (for now! maybe I will do another post running this as a mixed effects model) and just throw in all the predictors into the model, not care that they are correlated, and see what our model tells us. 

Let's do some modeling!

### Modeling 

The first thing worth checking in this situation is how your variables are distributed.
Normally you want nice [Guassian distributions](https://en.wikipedia.org/wiki/Normal_distribution) with your IVs.
We see that in each of our variables here and also take note that our raters are not taking advantage of the whole scale they have.


```{r}
mean(beer$review_overall)
hist(beer$review_overall)
```

The average rating of all beers is 3.82 (most beers are "above average") and it looks like we a ceiling problem with the data in that the top end of the ratings doesn't slope off like the lower ratings do.
We _could_ transform the data, but the problem with doing that is that you then have think about how you interpret your answers on that scale.

I then run a couple of other metrics on our IVs and inspect them visually.

```{r}
#======================================================================================================
## See how variables relate to each other , collinearity 

hist(beer$review_aroma)
hist(beer$review_appearance)
hist(beer$review_palate)
hist(beer$review_taste)

apply(beer[, .(review_overall, review_aroma, review_appearance, review_palate, review_taste)],2, skew)
apply(beer[, .(review_overall, review_aroma, review_appearance, review_palate, review_taste)],2, kurtosi)
```

Some of the other things worht checking here are for collinarity amongst variables (how much overlap is there between two) and we see from the correrlation tables that most things don't really overlap that much.

```{r}

cor(beer[, .(review_overall, review_aroma, review_appearance, review_palate, review_taste)])

```

We've at least done a preliminary check of the assumptions of regression and only broke a few of them, so we need to be aware of that when we get into model interpretation. 
For our first model, let's just dump everything in and see what comes up as significant. 

```{r}
predict.overall <- lm(review_overall ~ review_aroma + review_appearance + review_palate + review_taste, data = beer)
summary(predict.overall)
## aroma and appearance contribute, but might just bbe because of high numbers
```

Looking at the output every predictor is significant!
Though it's not really that big of a deal because our dataset is HUGE and all that a p < .05 is telling you is that the predictor you used is non-zero. 
That's it.

More important to look at is the size of the estimates.
We see that the estimates for ```review_palate``` and ```review_taste``` are much larger in size than the ```review_aroma``` and ```review_appearance```.
Since all of these variables were orignally measured on the same scale, we can directly compare them.
We also notice that our Adjusted R-squared is .6581 meaning that this model accounts for over 65% of the variance, not bad.

Let's try to run another model, but this time without those two garbage predictors and see how much predictive power we loose. 


```{r}

predict.overall.occam <- lm(review_overall ~ review_palate + review_taste, data = beer)
summary(predict.overall.occam)
# Adjusted R2 barely changed, def remove them from the model

# Problem here is that we still have collinearity.
```
Our two IVs are still significant, but our R squared droped to .6561 which is pretty negligiable.
If it were up to me, I would say to just retain a two variable model. 

One thing that has always irked me about reading about 'models' is that a lot of the time articles won't mention what the final model is and what it means.

If we were to say what this model was and meant we would write it out going back to 8th grade algerbra as a line.

Overall Taste = Score of what someone rates as Palate x 0.28 + Score of Taste * .59 + .54

So if you knew that someone rated a beer as a 4 on Palate and a 3 on taste, you would put those numbers into the equation and what it would predict after doing some basic math ( 4 \* 0.28 + 3 \*.59 + .54 ) it would guess the beer's overall score would be a 3.43.



