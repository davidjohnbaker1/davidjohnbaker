## Encoding A Corpus

## Introduction

One of the less glamorous things that you get the privlidge of doing if you are interested in computational musicology is encoding a corpus.
It has got to be one of the most boring, yet most nessecary things that can be done for the field. 
In this post I want to accomplish two things:
First, I want to highlight the importance of why there is a constant need to be encoding new data (generalization and representation and London)
Secondly, having just completed a corpus for some of my dissertation work, I want go give a realistic idea of what goes into encoding a corpus, also to give major props to those who are doing it.
Hopefully posts like this will encourge other researchers to start with their own proejcts and maybe the community at large will eventually be able to utalize more resources we have.

## Background To Enter Conversation


## What I did and Pretty Pictures that Explain it

## Conclusion

i. people who encode deserve way more credit 
ii. generalization and representation matter
iii. think people should start their own projects, make your students encode!
iv. students-- learn a lot about computers, GUI software, think about WHY it's important to have lots of data. 

Since a large part of my upcoming dissertation is going to be talking about relationships between symbolic data and behavior, I thought it'd be good to talk about this process earlier on in the writing.
And because the world of computational musicology is not that big, it's worth taking a second to give a brief overview of why it's important to constantly be creating new data for our field.
The first, and probably most important reason, is that in computational musicology encoded melodies are our data!
Whereas reserach in psychology is often after some sort of effect dealing with reaction times or ratings on likert scales, the computational musicologist is interested in how the music as best represented by western notation, yes there are problems with that, can be used to help answer questions about musis.
For example, in answering questions about melodic arches .... and found that.
This study was great because it used huge number of melodies to suggest evidence for a certain hypothesis.
Or take for example this study that used ESSEN Collection.
Or this study that USED ESSEN COLLECTION and found that.
There's clearly a bit of a trend here... a lot of studies tend to rely on the same data to reach their conclusions.
If we are all basing our claims on the same dataset (which is also a big problematic ask Andrew Brinkman), then our claims about how these findings are supposed to generalize are diminished.

I. Lame-o hook to state what people are going to be spending their next ten minutes reading about
II. The brief background they need to enter the conversation or know why I care, hopefully to get them to care
III. What I did and the pretty pictures that explain it.
IV. The call 



## Encoding Speeds

If there is anything that Dan Shanahan and I disagree the most it is what is the most effective way to encoding melodies.

With my dissertation coming up in the next few months, I figured that if I were going to be spending a lot of time sitting behind my comptuer screen taking melodies and turing them into our beloved **kern format, I should have some sort of rule of thumb for how long a melody takes to encode.
This way I'll be able to ballpark how long it will take me to encode all of the melodies that I am hoping to use as the corpus for my dissertation.

Since the world of computational musicology is not that big, it's worth taking a second to give a brief overview of why it's important to constantly be creating new data for our field.
The first, and probably most important reason, is that in computational musicology encoded melodies are our data!
Whereas reserach in psychology is often after some sort of effect dealing with reaction times or ratings on likert scales, the computational musicologist is interested in how the music as best represented by western notation, yes there are problems with that, can be used to help answer questions about musis.
For example, in answering questions about melodic arches .... and found that.
This study was great because it used huge number of melodies to suggest evidence for a certain hypothesis.
Or take for example this study that used ESSEN Collection.
Or this study that USED ESSEN COLLECTION and found that.
There's clearly a bit of a trend here... a lot of studies tend to rely on the same data to reach their conclusions.
If we are all basing our claims on the same dataset (which is also a big problematic ask Andrew Brinkman), then our claims about how these findings are supposed to generalize are diminished.

If the integrity of our data and thus findings is not reason enough, another reason to encode more melodies is so that when we attempt to make claims about music perception or production, we are not falling into WEIRD research and thinking that because we used a sample of Western music that it is somehow representitive of all groups of people.
Some researchers have been noteably leading this effort (Shanahan and Shanahan, 2014), and it's essential to the field to have a diverse representaiton of what we claim to be looking at as argued for in London (representitive corpus).

I'm sure I will talk more about computational musicology in the future (big plans for this blog!), but the question that spurred this post is 
> Given the fact that we need more melodies all the time, what is the best way to do this?

My meetings with Dan pretty much boil down to two paths, each with their own mini paths.

1. Encode everything by hand by looking at the score, then entering it directly into either MuseScore 2 or (for the masochists) directly into the terminal using vi. 
2. Cut up a book, scan it, run it through optical recogition software, then spend your time cleaning up the errors before exporting it to XML, and then using humdrumextras to convert it to **kern.

Dan is a firm beliver that working directly with text file and vim is the way to go.
I on the other hand think that using something like MuseScore is a bit more sane.

Of course the only way to figure this out properly is to set up an experiment and analyze the results!

## The Experiment

Step one in setting up any experiment is picking a good experimental design given what you have.
Luckily at MCCL we have a few graduate students that were willing to help with this experiment.
Since this was just for fun and I probably should be finding better ways to spend my time, I only set the experiment up with two factors of interest.
1. If vim or musescore would be faster
2.  

 
