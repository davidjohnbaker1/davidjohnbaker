<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on David John Baker</title>
    <link>/post/</link>
    <description>Recent content in Posts on David John Baker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 David John Baker</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Thank You, Toynbee</title>
      <link>/post/thank-you-toynbee/</link>
      <pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/thank-you-toynbee/</guid>
      <description>&lt;p&gt;After a year, my time is now up as a Residential Volunteer Worker at Toynbee Hall.
Given everything that’s happened this year (dissertation, travel, destroying my shoulder), I haven’t really got to blog about my experiences at Toynbee, but seeing as it’s been a sizable part of my life the past year, I wanted to take some time to reflect on what I have been up to the past year.
Keeping in line with my ideas on &lt;a href=&#34;https://davidjohnbaker.rbind.io/post/embracing-career-diversity/&#34;&gt;career diversity&lt;/a&gt;, many of my reflections here will be for my presumably more academic-ish readership.&lt;/p&gt;
&lt;p&gt;I want to talk about two things here:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What was I even doing here?&lt;/li&gt;
&lt;li&gt;What new ideas do I have that I now want to share?&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;what-was-i-doing-here&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What was I doing here?&lt;/h2&gt;
&lt;p&gt;To give a bit of background on this, a little more than a year ago I applied to be the ‘Research
and Evaluation Residential Volunteer Worker’ (RVW) at &lt;a href=&#34;https://www.toynbeehall.org.uk/&#34;&gt;Toynbee Hall&lt;/a&gt;.
As I’ve blogged about &lt;a href=&#34;https://davidjohnbaker.rbind.io/post/toynbee-hall-datathon/&#34;&gt;before&lt;/a&gt;, Toynbee Hall was in the process of re-booting this program that brought people on-site to volunteer at the charity (non-profit) in exchange for residency.
I was accepted along with three others, though they worked on the Heritage side of things on projects ranging from story telling, to art, to social justice.&lt;/p&gt;
&lt;p&gt;When I started, I thought I’d be roving between the Research and Evaluation teams, but while I was there Toynbee had a bit of a departmental re-shuffling and as a result I ended up spending all my time in Research.
As an RVW, and just like academia, I had a jack-of-all-trades position where I generally played to most of my strengths.
For me, this included doing all things data, writing, helping organize events, and doing some field research (in that order).
I’d imagine that my readership would notice that this laundry listing of things is actually quite similar to that of what people get up to in academia, yet without the overwhelming amount of teaching.
I worked on both larger projects as well as smaller ones.
Different from academia, the end goal of all projects was not to end up with the research in a peer-reviewed academic journal, but rather to have some sort of immediate impact on the local community.
More about this later.&lt;/p&gt;
&lt;div id=&#34;job-crafting-for-data-science&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Job Crafting for Data Science&lt;/h3&gt;
&lt;p&gt;I was quite lucky in this position in that I was part of the first wave of RVWs in a long time and was not an employee at all (just a volunteer), and had a supervisor who was very open to exploring what the role could be.
As a result of this, I was able to do a bit of &lt;a href=&#34;https://positiveorgs.bus.umich.edu/wp-content/uploads/What-is-Job-Crafting-and-Why-Does-it-Matter1.pdf&#34;&gt;job crafting&lt;/a&gt;.
Very early on last year, I tried to push many of my weekly goals towards all things data science so I could see first hand what of all skills I have been doing in my Ph.D research actually transferred outside my own little world.&lt;/p&gt;
&lt;p&gt;Turns out the biggest overlapping part of the Venn diagram was cleaning data.
I distinctly remember many days where I would put on my headphones (I also learned I do not function well in &lt;a href=&#34;https://twitter.com/tombellino/status/1139569345502142464&#34;&gt;open office designs&lt;/a&gt;) and spend hours taking pre-formatted Excel workbooks with awful column names with strangely coded values and formatting the data over to &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html&#34;&gt;tidy&lt;/a&gt; formats so I could learn about what was in the data.
After things were tidied, I then spent &lt;strong&gt;a lot&lt;/strong&gt; of time with &lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;ggplot2&lt;/a&gt; really getting to know the data sets that I was dealing with.
Doing this, I really started to get some light bulb moment about the &lt;a href=&#34;https://simplystatistics.org/2019/04/17/tukey-design-thinking-and-better-questions/&#34;&gt;importance of exploratory data analysis and thinking about what we are even doing with quantitative research&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hilariously, I made many data science rookie mistakes that I had read about on all the How-to-Data-Science blog posts I had been reading in the years prior to this position.
I wanted to show off all the machine learning I had been reading about in order to show of the power of these tools.
But in my first few exchanges of what I would go off and analyze and then share with others, I quickly found that my time and skills were much better spent making a lot of simple visualizations of our survey data (like the plot below) in order to help facilitate the start of conversations about the larger projects.
The data analysis I was doing really meant nothing unless I could directly point to why a finding was important to a team of very intelligent people, but without a background in statistics.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/age_typology.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This process also rid me of any delusions of making models that had overly predictive accuracy.
My job was to be critical researcher who used data, not a data shaman that created impressive predictive accuracy.&lt;/p&gt;
&lt;p&gt;In thinking about this, I also realized that there is not much you can do as the “numbers guy” when someone hands you a dataset you had no hand in designing.
Unlike academic research where you pretty much have an idea of all the variables you’re collecting in a cross-sectional study, working here you’re often just given a dataset with a potpourri of variables and asked to finding meaning and value.&lt;/p&gt;
&lt;p&gt;I should mention that when someone does just hand you a dataset with variables another person came up with, the job here is not to be “well you should have done XYZ”; you have to do your best to extract as much meaning and value from the dataset as you can, then actively advocate for the best quantitative data practices that the team can eventually adopt so that future team members will not run into the same dead-ends.&lt;/p&gt;
&lt;p&gt;So the TL;DR of much of this year is that I had a very nice crash course in what you read in the data science blogs versus what you need by Friday.&lt;/p&gt;
&lt;p&gt;Of course there were a lot of other “researcher”-y things that I also did that were not just hunching over my computer doing R.
I spent a lot of time writing reports.
Doing this much writing also affirmed my belief that as researchers, one’s ability to immediately create a clear and compelling narrative is one of the best ways you can be most impactful.&lt;/p&gt;
&lt;p&gt;Just like academia, it’s very important to know your audience, why you are writing, and what you hope to accomplish.
I also helped out with a couple small things here or there like going out to do interviews for projects and had the chance to run a training session on best quantitative researcher practices.
About a month or so ago, I also got to blog about the &lt;a href=&#34;https://davidjohnbaker.rbind.io/post/toynbee-hall-datathon/&#34;&gt;Datathon&lt;/a&gt;, which combined many of these other skills and enabled me to talk to other charities in the process.&lt;/p&gt;
&lt;p&gt;Importantly, thinking about how all of this fits in for my more academic readership, one thing that was great to experience is that many of the skills that I focused on in academia were both applicable and relevant.
In many ways, I was close in what I thought going into this, being that working in this context would be similar in form, but different content.
This was almost correct…
It’s almost a similar form, but with a totally different language and underlying value structure.
And it’s this last point that leads me to my next major section here: what is new and what have I learned over the course of the past year?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;new-ideas&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New Ideas&lt;/h2&gt;
&lt;p&gt;So as discussed in &lt;a href=&#34;https://davidjohnbaker.rbind.io/post/embracing-career-diversity/&#34;&gt;my previous post&lt;/a&gt;, what I was up to at Toynbee Hall this year was markedly different than what I get up to in my academic life.
And more importantly, my experience there has really shaped some new opinions on academic ideas.&lt;/p&gt;
&lt;p&gt;One of the first— and almost most obvious ones— worth addressing right off the bat is that if you
want to go out and help people or make the world a better place or whatever, just go help and volunteer time at a charity.
From my Twitter feed, it seems like there are lot of people who struggle with what they do in the context of everything else going on in the world right now.
What much of this seems to conveniently forget to mention is that 1) no one is making people stay in academia and 2) there is a whole division of labor in the world whose goal is to directly change the lives of people for the better (Higher Education does not have a monopoly on long term help).
This seems almost silly to type, but I feel like we as trained academics get serious career blinders about what we could do with our training.
I know I’m often guilty of this, but it’s one reason I’m thinking out loud here: this experience really made me question a lot of assumptions I had about my career going forward.&lt;/p&gt;
&lt;p&gt;To that, some people might react with a little mix of both academic Stockholm Syndrome and retort with the fact that moving from academia to another sector is not as easy as I am making it sound.
Of course, I am not blind to that, but one thing that I am hoping to accomplish in writing about this is the need to talk about career diversity for academics and how this might be possible.&lt;/p&gt;
&lt;p&gt;A secondary point I am hoping to make here is that instead of doubling down on academic research at feelings of not being able to help in the world, I’d bet more people would be better served if academics were to be able to say “No” to the ever growing bubbling over of demands of academia so that people do in fact have free time which they could use to help others in a more meaningful way (if this is something of interest).
Now I really don’t want this to come across as a “well this academic research doesn’t help anyone” type of sentiment.
I’m a firm believer in the need for researchers, especially in the humanities, to pursue questions that do not seem to have any immediate relevance whatsoever to “the real world”.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The reason I am thinking about this is because of that earlier dread I was talking about and feeling that the time I am investing in academia writing papers about memory for melodies is not really helping anyone immediatly at the end of the day and seems to consume my entire being.
I (secretly) tell myself that “Well, when you get that sweet”job&amp;quot; and all this gets published, it’ll aalllll be worth it and people will benefit from my work&amp;quot;!
But even IF that obvious lie I tell myself is true, the person who benefits the most from that long cycle of work the most, if I am being honest, is me.
I think it’s good research (or else I wouldn’t do it), but I don’t think I should kid myself about who really benefits from this work.&lt;/p&gt;
&lt;p&gt;And this idea of who benefits from the research that is done is also something central to the work done at Toynbee Hall.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;who-benefits&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Who Benefits?&lt;/h2&gt;
&lt;p&gt;To give a bit of context to this, it’s worth establishing that Toynbee Hall is a charity that strives to have deep roots with the local community it exists in.
It started as a University settlement where people could see the local area and become personally invested in it.
And this relationship is supposed to be symbiotic.
The local community is invested in many things Toynbee Hall.
A lot of people in the area, especially older people, go to Toynbee Hall’s community center for a sense of
belonging and meaning in their lives.
Toynbee Hall is part of their lives, but it also seeks to improve the quality of their lives (through research in this case).
.
But this introduces an interesting question of power, primarily, what does it mean to do research on people that you have both a vested interest in and personal relationship with?&lt;/p&gt;
&lt;p&gt;The stereotypical ‘academic’ approach according to popular imagination attempts to address this question the long way.
To truly learn about your local community, one must put oneself at a distance, try too stay objective about the questions at hand, look to the literature to establish the central discussions on the topic, conduct research methods with the established and recognized tools of the field, then submit said findings for publication in a peer reviewed journal.
From here this apolitical-as-possible piece of research (of course written in the passive voice to absolve the writer of even more bias!!) then can be used as a small building block
to influence policy change that the higher levels.
This long, circuitous route is certainly the narrative many academics tell themselves that they do.
But it is not the only route.&lt;/p&gt;
&lt;p&gt;In contrast, researchers can dispel themselves of the illusion that what they do is objective (as Qualitative researchers will tell you, so please let me keep going with this strawman here) and instead just get your hands “dirty” and fully embrace the subjectivity of the your research.
One way of doing this in a very hands-on way is to use &lt;a href=&#34;https://en.wikipedia.org/wiki/Participatory_action_research&#34;&gt;Participatory Action Research&lt;/a&gt; or PAR.
The idea here is to &lt;em&gt;instead&lt;/em&gt; just ask those affected by the research about their problems and include them as peers throughout the entire research process to guide the research rather than treat them as data points to be observed.&lt;/p&gt;
&lt;p&gt;In order to make this idea a bit clearer, instead of theoretically describing it, it’s better to
show than tell so let me show you an example of this from the past year.
Imagine that there is project where a local charity (non-profit) wants to directly address the needs of older
people in the local community.
One might be tempted to read deeply on the issue, conduct a proper lit review, develop yet another survey, yadayadayda.
Instead, the charity could try to rely on the already established community connections in order to start
to get in touch with the community from Day One.
Instead they could assemble a small team of local community members that all have an interest in having some sort of impact, (contrasted to the disinterested interest we read about in research methods) then as a team, come up with question that are impacting the community and in need of change,
work with those with more experience in creating better questions, then use this network of co-researchers to go on foot and reach over 500 people in the local community.&lt;/p&gt;
&lt;p&gt;Not only does this result in a better sampling than just throwing up a &lt;a href=&#34;https://www.mturk.com/&#34;&gt;Mechancial Turk&lt;/a&gt; (which this population in particular would probably not use), but in taking this approach the team is able to solve problems in the process.
For example, if “everybody knows” that access to information about local services and isolation is a problem for older people, why not additionally equip those adminstering the survey with tools to connect those being interviewed with information that could start to solve these problems in process.
Doing this jumps the model of waiting for rounds of peer review before research can be “used”.
And more importantly, when it finally does get published, who benefits more?
The academic with a new line on their CV or the people who were surveyed?&lt;/p&gt;
&lt;p&gt;Toynbee Hall did complete this project last year and you can &lt;a href=&#34;https://www.toynbeehall.org.uk/06/09/2018/new-peer-led-research-reveals-the-needs-of-older-people-in-tower-hamlets/&#34;&gt;read about it here&lt;/a&gt;.
What then results is a report where you get the best of both worlds.
You have both a quantitative, representative description a sample of the population (for the quant nerds) as well as narratives of individual voices and also developed a stronger community in the process (for people who don’t need data to be convinced this is something important&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This is all to say: I think that it’s important to take a very long, hard look at who benefits from the research choices we make on a daily basis.
This speaks to questions of outreach, ownership, power, and false illusions about objectivity.
But as someone who lives and breathes the quantitative stuff, I learned a lot via this experience and will be tabling many of these ideas in new research projects where I am a team member.&lt;/p&gt;
&lt;div id=&#34;an-answer&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;An Answer&lt;/h3&gt;
&lt;p&gt;So now taking this all back to a larger question about relationships between inside and outside the Ivory Tower: what can be done?
Having the privilege of this experience really reinforced an opinion that I was thinking about throughout my Ph.D in that I think Ph.D programs in the humanities would be really strengthened by having long (obviously paid) internship programs as part of their degree options.
I have a couple of reasons now as why I would argue for this.&lt;/p&gt;
&lt;p&gt;The first is that it gets you out of the Ivory Tower for a bit.
Not only can you actually begin to see what skills are transferable in a meaningful way&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, but you can also just see if it really is [the subject of your Ph.D] that you can’t live without, even though you haven’t ever experienced anything else.
Doing an internship like this (or however it’s cast) allows Ph.D students to learn a totally different language and value structure that without learning, will make applying for non-academic jobs &lt;em&gt;much&lt;/em&gt; more difficult.
I’d like to think that incorporating something like this would make for a stronger application when it comes to applying for academic jobs, but I guess that is something yet to be found out.&lt;/p&gt;
&lt;p&gt;It’s also a chance to live a lifestyle that is outside of the Ivory Tower grind.
Again, it might be that it really is this very specific subset of a work field called Music Theory (in my case) that you can’t live without.
Maybe it’s just that people like research and talking about the subject area at hand as a part of a community with a shared set of values.
This might not be charity as it was in my case, but I seriously think there is a lot that Music Ph.Ds (humanities) could offer in non-profits arts settings (thinking orchestral management, arts organizations).
I’d bet there are probably some arts organizatoins that could use a bit of help in summer from someone with rigerous research training, in-depth domain knowledge, and if you paid them an honest day’s work it’d be be more per hour than what’s expected in a Ph.D program.
Doing this would also have people put their money where their mouth is about career diversity and the real values and value of doing a Ph.D in the arts where the only real end goal is “the job”.&lt;/p&gt;
&lt;p&gt;Again, this is very easy for me to armchair theorize about&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;, but if cultivating these relationships from academia to outside the Tower is that hard, I feel like that might just speak to the insularity of academia.
We (as Ph.Ds) should be able to move back and forth between the bricks of the Ivory Tower.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;As I finish here, I can retrospectively see that this was a really a great opportunity.
I learned a lot and personally benefited a lot from having this chance and only hope that I was able to provide value beyond what I received.
Much of this would not have been possible without having my manager Dr. Xia Lin be such a great leader and let do that job-crafting I spoke of earlier.
I am really looking forward to finding out new ways to help out with this work in the future.
My time here has really changed how I think about my own career trajectory and who benefits from all the choices that I make.
I am excited to carry that forward in my own career and to hopefully use what I have in a way that
helps others in a truly meaningful way.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;AKA the possibility to improve neoliberal markets AKA how can this make me some money&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;code&gt;{r}emo::ji(keyword = &amp;quot;fire&amp;quot;)&lt;/code&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;and begin to ameliorate the exestential dread of what happens if I don’t get “a job”&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;What else are blogs good for other than mindless armchair theorizing and not thought out footnotes?!&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Embracing Career Diversity</title>
      <link>/post/embracing-career-diversity/</link>
      <pubDate>Wed, 12 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/embracing-career-diversity/</guid>
      <description>&lt;div id=&#34;things-are-different-now&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Things Are Different Now&lt;/h2&gt;
&lt;p&gt;Hello, first-blog-post-since-I-defended-my-Ph.D!
It’s wild to think that after five or so years (between starting the the MSc. and the final Ph.D. defense), I am finally done with graduate school.
There are no more exams, no more term deadlines, and no more giant documents to write.
The last few months felt like quite the pressure cooker between writing everything up, flying back to defend, then doing a bit of travel.
And now after the sprint at the end of the marathon, it’s finally over.
It kind of feels like a scene out of a movie where the soundtrack builds to a climax, but then it all just goes silent.
I feel a bit caught in suspended animation and have a lot of thoughts swirling around and no structure to place them within.&lt;/p&gt;
&lt;p&gt;And what’s a boy with a lot of feelings in 2019 to do?
Start a podcast.&lt;/p&gt;
&lt;p&gt;Just kidding, the last thing the world needs is another dude telling you his opinions about the world on a podcast.
I am not starting a podcast.
But I do want to blog more.
It’s something I personally find very helpful to do myself and have found reading other’s blogs to be &lt;strong&gt;very&lt;/strong&gt; helpful.
In this post, I want to do a bit of self-justification and mindless rambling.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bigsat&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;bigsat&lt;/h2&gt;
&lt;p&gt;So as noted above, I’m finally done with the Ph.D. (Yippe!)
It was a trudge to say the least.
Not only was it a lot of work, but living in a work environment for a few years as an intellectual underling is not exactly great for your self esteem.
The hours and pay were not great (if you think about the Ph.D. like a job), but more importantly, the grind of graduate school is not sustainable.
The rate that I was working in the months leading up to my dissertation submission consumed every part of my life.
Having spent so much time on my #bigsit and all the graduate school that lead up to that, I also was starting to feel a deeper dread that large portions of my life were passing me by and I was loosing time that I could not get back.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;
The good news is that retrospectively I don’t regret any of it.
As a result, I have a dissertation that I am very proud of on many levels.
I’ll be blogging about that soon as well.
But I also think that it is important to acknowledge all these thoughts and feelings and reflect on them.
And one thing that really helped me while I was a graduate student was reading about others who seem to have similar feelings.
So what I want to do here is explore those thoughts and feelings.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;thoughts-and-feelings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Thoughts and Feelings&lt;/h2&gt;
&lt;p&gt;What I am finding is there are a lot of those thoughts and feelings post-PhD.
In drafting this post, I found that I wanted to talk about EVERYTHING.
For this post alone, I thought about having sub-sections or digressions that talked about: finding personal validation through the structure of academia, feeling disconnected from academic peers as someone not going on to some fancy TT job this year, all the strange things people said at conferences last month, and this list goes on.
And in starting to sketch out what each of those digressions might look like, I realized here I had mountains of thoughts and feelings I wanted to share.
It was almost like that for every new fact I had learned in grad school, I was given two feelings to go along with it.&lt;/p&gt;
&lt;p&gt;But given all these feelings, and no academic cohort anymore, who can I then share all these niche thoughts with?
This question becomes even more important when you consider that you really only are who you think you are in relation to other people.
I noticed this the most when I moved to London last year and didn’t have daily access to my cohort at Louisiana State Univeristy.
You don’t realize that you’re the “that guy” of your group until you leave them.
If you can’t define yourself as “the guy who really likes music theory and music cognition” in your new group, it’s like that part of you goes away as well.
This is a pretty big deal, because for many aspiring academics like myself, this is a huge part of our identity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;crescendo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Crescendo&lt;/h2&gt;
&lt;p&gt;And the grim truth about this is realization is that post-PhD, these feelings might only increase with more time away from academic settings.
In a few weeks I am sure my &lt;code&gt;dbake29@lsu.edu&lt;/code&gt; email address will be disconnected.
I’ll move further away in many respects because I’m not taking that next logical career step of doing a post-doc, a VAP, or taking a “job”&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.
In taking another path for right now, I will drift a bit further away from the rhythm of the academic year.
And in many ways, I have already started to experience this.&lt;/p&gt;
&lt;p&gt;In the past few Theory conferences I attended, the invevitalble “What are you doing next year?” question conversation always got a bit awkward when I told them that I didn’t have a “job” for next year.
Many times I felt compelled that I had to add some sort of caveat to my response saying that I was planning on applying for more “job”s next year, but next time with greater effort!
Though what made it the most weird, is that many people seemed almost not interested in what I was planning on doing this next year or, even worse, actually told me that if I did not take other work that was as close as possible no matter what to being a Music Theory professor, I was just shooting myself in the foot for the rest of my academic career.
This was off putting to say the least, but talking about this sad state of affairs is not a digression worth going on.&lt;/p&gt;
&lt;p&gt;Though this whole situation brings up quite the conundrum.
As people move through the Ph.D system, not everyone who gets a Ph.D in [Insert Your Subject Here] is going to move on right away to a “job”.
If they don’t, the way things are currently set up they will start to spin a bit more out of the central orbit of the field.
At some point they might swing back in or they could drift off.
But it brings up what I think is an important question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What does the space look like for academics who are interested in all things academic, but don’t neatly fit into a comprehensible box of the current infrastructure?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Throughout my PhD, conversations about this really didn’t come up as the part of any formalized curriculum.
And saw it coming.
Yet here I am.
Or more appropriately– here we are.
This group of people is only an ever growing population.
And will continue to be since it’s numerically impossible to place every person in a “job” each year.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-answer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My Answer&lt;/h2&gt;
&lt;p&gt;Of course, this is a very large question and problem that exists far beyond that of Music Theory.
It might even be happening in Musicology as well.&lt;/p&gt;
&lt;p&gt;But regardless, there ought to exist some sort of space for people who are card carrying academics without a clubhouse to return to.
And I guess what I want to start thinking out-loud about is thinking about what this kind of career diversity looks like that is not just academic purgatory or a career waiting room.
I want to try to think about establishing myself (and others) in this space.
And not just talk about it so it’s a bit more normal, but also write about what it might look like to enjoy being in this space.&lt;/p&gt;
&lt;p&gt;I don’t really know what that will look like right now.
But in addition to all those thoughts and feelings that I want to talk about, I also have a lot of practical work I want to share.
And I think my twitter feed and website might be a good place to still stay in the loop and document experiences here or there.&lt;/p&gt;
&lt;p&gt;Drafting up what I want to talk about post-Ph.D (even if it’s just for me to be therapeutic thoughts) I want to blog about career diversity, life as a graduate student, computational musicology, music science, how music theory relates to everything, and a whole host of other things.&lt;/p&gt;
&lt;p&gt;So who knows.
I wanted to blog throughout much of my Ph.D, but just did not have the time.
Now on other side, I really want to explore all the topics I couldn’t justify while a grad student and use the whole process of a way as continuing to express my academic identity.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Also add in a bit of fear that everyone that I talked to that left academia willingly said doing so was one of the best choices they’ve made in their life…&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;For my non-academic readers, the “job” refers to a tenure track (TT) job in North America at a decent school near a city that you could imagine asking your partner to uproot their entire life so you can be a professor&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Toynbee Hall Datathon</title>
      <link>/post/toynbee-hall-datathon/</link>
      <pubDate>Tue, 23 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/toynbee-hall-datathon/</guid>
      <description>&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Not everyone might be aware, but in addition to living that dissertation life this past year, I have also been volunteering at &lt;a href=&#34;http://www.toynbeehall.org.uk/&#34;&gt;Toynbee Hall&lt;/a&gt; as part of their newly re-started Residential Volunteer Worker program.
The program started WAY back in the late 1890s.
The idea was that all the bougie people from Oxbridge would come and live on-site in East London in order to do charity work and help out a bit before they were to go on to do other things like be elected to Parliament and then pass laws and help those that were not as well off.
I probably will not be elected to Parliament anytime soon, but will hopefully go on to be a helpful member of society in other ways.
Toynbee Hall restarted this program this past year and I was lucky enough to have my application accepted for the position.
Since then, I have been helping out as part of the research team.&lt;/p&gt;
&lt;p&gt;I’ve helped out with a few smaller projects here or there (basically crunching the datasets they have and putting my other research skills to use doing interviews and what not), but before my year was up, I wanted to try and do my own mini-project which will be the focus of this post: a #data4good datathon at Toynbee Hall.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.eventbrite.co.uk/e/datathon-at-toynbee-hall-tickets-59760759039&#34;&gt;Check out the event and sign up here!!&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;history-of-helping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;History of Helping&lt;/h2&gt;
&lt;p&gt;You can read my glitzy description of the event in the link above, but if you’re reading my personal blog, as opposed to the event page, I assume this readership would want a bit more context on the Datathon and the ideas behind it.
To give a bit of background, Toynbee Hall is a very, very old charity based in East London.
It has a long history and was a touch point for a lot of important people in London back in the day who were interested in charity work (non-profit for all my North American readers).&lt;/p&gt;
&lt;p&gt;Ages ago, some of the people here at Toynbee Hall helped out &lt;a href=&#34;https://booth.lse.ac.uk/&#34;&gt;Charles Booth&lt;/a&gt; in his creation of the &lt;a href=&#34;https://booth.lse.ac.uk/map/14/-0.1174/51.5064/100/0&#34;&gt;London maps of poverty&lt;/a&gt; that essentially created the first data visualizations in what might be considered a #data4good cause.
Fast forward over 100 years and instead of going door to door and trying to use colors to highlight different levels of wealth on a map needing several panels, we can instead download publicly available data from something like the &lt;a href=&#34;https://theodi.org/&#34;&gt;Open Data Institute&lt;/a&gt; or &lt;a href=&#34;https://data.london.gov.uk/&#34;&gt;The London Datastore&lt;/a&gt; and make data visualizations with the same goals in mind.&lt;/p&gt;
&lt;p&gt;Given Toynbee Hall’s history and current research interests that engage the community by bringing different people together, it almost seemed like a no-brainer to try to put together something where we could try to combine all of these into one coherent event.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goals&lt;/h2&gt;
&lt;p&gt;Knowing that there are community issues in need of addressing in &lt;a href=&#34;https://en.wikipedia.org/wiki/London_Borough_of_Tower_Hamlets&#34;&gt;Tower Hamlets&lt;/a&gt;, as well as community members who know the area, we wanted to continue in the tradition of bringing people together to answer a question.
Because my background has a bit of #rstats and data science (because of my work in music science), what seemed like the logical move here was to combine the history that Toynbee Hall has on data visualization and the local community using the data community.&lt;/p&gt;
&lt;p&gt;So what were our goals in trying to do this event?&lt;/p&gt;
&lt;p&gt;Well the &lt;strong&gt;first&lt;/strong&gt; goal of this was to see what goes into organizing this kind of event.
I don’t really have too much of an event planning background and wanted to know what I would have to get in order to front load most of the data work for everyone.
This means gathering all the data to start with, figuring out how to best host it, and setting everyone up for success the afternoon of the event.
This data curation is most of what I do anyway, and I figured if this process did not prove too hard, then maybe I could help out remotely on events like this after I leave Toynbee in order to still be helpful.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;second&lt;/strong&gt; goal is the most important one:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;to have the event result in the creation of meaningful material that Toynbee hall can use in their upcoming advocacy campaigns.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The research team is planning on looking at issues of both Youth Homelessness and Safety in Tower Hamlets this summer.
Any data that we could find and could include in future reports could really help provide evidence that could eventually be used in a policy campaign.
In order to do this we are using data that &lt;a href=&#34;https://www.yhdatabank.com/&#34;&gt;Centrepoint&lt;/a&gt; has given us, publicly available data on crime in London, and maybe one more source that I am putting together today.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;third&lt;/strong&gt;, a by-product of having this event both locally and on Twitter, is to hopefully create some visibility to these issues.
Just having people tweeting their data visualizations and maybe having some be caught by the community at large would be great for making people aware of these issues.
It would also help us connect people who are interested in #data4good with charities or non-profits that they can help.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lastly&lt;/strong&gt;, in line with solving problems long term, I am also hoping to build up a network of people that Toynbee Hall can turn to after I leave.
The program that I am on is just one year long and I will go on to other work after this.
After my time here is up, I want to have linkd up the research team at Toynbee Hall with a network of people that could help in the future.
One way that we are hoping to accomplish this is take everyone that participates in our event and link them up with organizations like &lt;a href=&#34;https://www.datakind.org/chapters/datakind-uk&#34;&gt;DataKindUK&lt;/a&gt; and hopefully foster new relationships.&lt;/p&gt;
&lt;p&gt;I have a thousand other things I want to say about this event, but instead of editing more content that I originally drafted few weeks ago, I need to instead get all the content ready for us to go tomorrow.
I am sure I’ll be talking a lot more about this in the future when things calm down (still also on the come down from my dissertation submission last week…) but I invite you to join us tomorrow either at Toynbee (in the beautiful &lt;a href=&#34;http://www.toynbeehall.org.uk/events/&#34;&gt;Ashbee Hall&lt;/a&gt;!) or online on Twitter with #data4toynbee and download the data from our &lt;a href=&#34;https://github.com/research-at-toynbee-hall/datathons&#34;&gt;Github&lt;/a&gt; and help us out!
If you do download the data and something does not make sense, please @ me so I can fix it.&lt;/p&gt;
&lt;p&gt;Feel free to try it out now as I put on the finishing touches!&lt;/p&gt;
&lt;p&gt;Please get in touch if you want to chat more!
Either ping me on Twitter or send me an email!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sempre Grad Conference, 2019</title>
      <link>/post/sempre-grad-conference-2019/</link>
      <pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/sempre-grad-conference-2019/</guid>
      <description>&lt;p&gt;Since I am planning on spending the whole day today writing, I figured: What better way to warm up to some writing than with some writing?&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;
And what better way to get the juices flowing than to write about the wonderful experience that was the &lt;a href=&#34;https://www.sempre.org.uk/&#34;&gt;Sempre&lt;/a&gt; graduate student conference at Cambridge this past Monday.&lt;/p&gt;
&lt;p&gt;If you’re reading this in North America, you might not be familiar with Sempre, so for the uninitiated, Sempre or the Society for Education, Music and Psychology Research is a professional organization in the UK dedicated towards bringing together those in various fields of music, but specifically music education and music psychology.
I don’t think we really have something like this in the USA.
The music psychology people tend to hang out at the &lt;a href=&#34;http://www.musicperception.org/&#34;&gt;Society for Music Perception and Cognition&lt;/a&gt;, and I am kind of embarrassed that off the top of my head I don’t know where graduate students in music education might submit research they are working on to present to their peers?
According to &lt;a href=&#34;https://www.sempre.org.uk/about/1-background&#34;&gt;Sempre’s website&lt;/a&gt;, they’re very keen on helping out researchers at the start of their career and having this graduate conference is just one of the ways they do that.&lt;/p&gt;
&lt;p&gt;The conference this week was a short and sweet one-day affair held at the Faculty of Music at Cambridge.
Of course the weather was perfect that day, but luckily all the talks made it worth it to sit indoors all day.
Ian Cross welcomed everyone with a short the opening address and if my memory serves me correctly, he noted there were about 80 submissions to this conference.
He said most of those submissions did happen to come the day of the deadline, which is quite in line with any stereotypes one may have about graduate students.
But thinking about that, I feel like if all professional music organizations were to release the timestamp data of their abstract submissions, there wouldn’t be a single one that didn’t exhibit an exponential growth in submissions in the hours leading to the deadline.&lt;/p&gt;
&lt;p&gt;After his short introduction, the keynote speaker Martin Rohrmeier, head of the &lt;a href=&#34;https://dcml.epfl.ch/&#34;&gt;Digital and Computational Musicology Lab&lt;/a&gt; at &lt;a href=&#34;https://en.wikipedia.org/wiki/%C3%89cole_Polytechnique_F%C3%A9d%C3%A9rale_de_Lausanne&#34;&gt;École Polytechnique Fédérale de Lausanne&lt;/a&gt; was introduced.
Martin’s talk surveyed a lot of his previous work on using computational tools to model various aspects of tonality which showcased the work of his graduate students in attendance that day.
He also presented an overview of his lab’s work looking at &lt;a href=&#34;https://www.pnas.org/content/pnas/110/38/15443.full.pdf&#34;&gt;hierarchical, embedded structures within larger forms&lt;/a&gt;.
All of this empirical work then was tied together with what Martin put forward as a possible general theory of cognition and aesthetics that would be available to the listener, were they to choose, to engage deeply with a work of art.&lt;/p&gt;
&lt;p&gt;Knowing his audience as the next generation of music researchers, he made several pleas throughout his talk to encourage future work in music to work at the intersection between music theory, music psychology, and computer modeling.
This is an idea often gets poked at over here in Europe quite explicitly, but seems to live more as a tacit assumption in a lot of work in North America unless you’re at a talk by David Huron.&lt;/p&gt;
&lt;p&gt;The rest of the day took an ABA form with five minute flash talks surrounding a longer 12 minute talk session and the poster session scheduled for lunch.
As I tweeted earlier, I was skeptical of the five minute talks at first, thinking “How could people cram all they need to say into five minutes?!”&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; but seeing as this was only a one-day conference that was more about community building rather than getting in the nitty gritty about methods, the format worked well.
Because of it, in a one-day session we were able to hear about the research of 20 graduate students on a diversity of topics (not even including the 25 posters at lunch!).&lt;/p&gt;
&lt;p&gt;Almost all of the talks in the morning session addressed issues at the intersection of music and well-being with the exception of a paper by Daniel Harasim looking at unsupervised machine learning methods to examine jazz harmonies.
Listening to all these talks only reiterated assertions made earlier in the keynote of how many human resources are needed in order to push knowledge limits in the field of music.&lt;/p&gt;
&lt;p&gt;The start of the afternoon session was reminiscent of last summer’s multi-hub ICMPC conference as the winner of Sempre’s &lt;a href=&#34;https://www.sempre.org.uk/awards/hickman&#34;&gt;Hickman&lt;/a&gt; award, &lt;a href=&#34;https://twitter.com/LAWarrenburg&#34;&gt;Lindsay Warrenbug&lt;/a&gt;, skyped into the conference to deliver her paper on Melancholy and Grief in Music.
It’s nice to see the music science community putting their money where their mouth is on making remote presentations a possibility for presenters (something also heavily advocated for by the &lt;a href=&#34;https://sites.google.com/site/sysmusconference/&#34;&gt;SysMus Series&lt;/a&gt;).
Not to throw shade, but this is something that my home discipline of Music Theory needs to do a bit of catch-up on.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;
I look forward to the inevitable SMT Discuss or Twitter feud where we discuss if a person who can’t be to the national meeting in-person should instead be swapped out for someone who can.&lt;/p&gt;
&lt;p&gt;The rest of the talks in the afternoon were all extremely interesting.
It’s hard to go through and single out work I was loving since most of the work I liked was done by people who were good friends, but one paper that I feel like the theory and psychology community should be looking out for very soon was the work by &lt;a href=&#34;https://twitter.com/pmcharrison&#34;&gt;Peter Harrison&lt;/a&gt; and his advisor Marcus Pearce.
In 12 minutes, Peter was able to explain the current theories in the literature surrounding instantaneous consonance and dissonance (is it periodicity, is it spectral interference, is it cultural familiarity?), introduce an aggregated behavioral datasets from four prior studies where participants rated various sonorities, then introduce a new R package, &lt;a href=&#34;https://github.com/pmcharrison/incon&#34;&gt;“incon”&lt;/a&gt; that they used to computationally model 15 models of consonance to get to the bottom of the story.
If you’re interested in his findings, I know he also has a &lt;a href=&#34;https://psyarxiv.com/6jsug/&#34;&gt;pre-print&lt;/a&gt; of it out right now.&lt;br /&gt;
I only mention his paper in detail because I think what he is doing is very in line with the music theory world and will hopefully start an interesting conversation once it gets published.&lt;/p&gt;
&lt;p&gt;By the end of the day we’d have heard talks on musical chills, emotions and music, using music to help rehabilitate stroke survivors, music’s role maternal mental health in The Gambia, and posters on topics from more traditional music theory (thinking about hierarchical voice leading structure ala Schenker with Graph Transformations) to decentering the dominant discourse of the ‘Dead White Men’ cannon.
It was a great day.
Not only did the conference have a diversity of talks, but of the 20 talks, there appeared to be 50/50 split in the gender of the speakers.
I only note that so when the inevitable “But is it even possible to have a conference where we just have men talk?!” question arise, there’s documentaiton of this happening.&lt;/p&gt;
&lt;p&gt;Going to this conference and thinking about things like the &lt;a href=&#34;https://slate.com/technology/2019/03/google-doodle-bach-ai-music-generator.html&#34;&gt;general rage from last week regarding the Google Doodle&lt;/a&gt; made it very clear that there are many, many ways to do music research.
It made me wish that there were more conferences that follow this design beyond the graduate level.
I am only lamenting about this because if all goes well in May, this past Sempre conference will have been my last as a graduate student.&lt;/p&gt;
&lt;p&gt;Of course this brings up ideas about the “point” of the conference, but one important “point” is that I don’t think conferences should just be be about apex research at its final moment of metamorphosis before publication.
I feel like we as researchers need more of a chance to air out our ideas in a less, shall I say… performative setting.
I obviously acknowledge that the content and subject matter with a conference like this is very different to that of AMS or SMT, but honestly think that the culture of something like Sempre is much closer to what people want and need.&lt;/p&gt;
&lt;p&gt;This is especially true for early career researchers where point is to build relationships, explore new ideas, and not start to drink that academic Kool Aid right away since numerically speaking we will not all be academics, but will hopefully all engage in some kind of music research.
I’m also advocating for this because I’m currently in the peak of my PhD isolation phase now writing and going to this and being inspired by peers is what I think I’ll need to cross the finish line.&lt;/p&gt;
&lt;p&gt;So big thanks to Sempre for hosting it, allowing for a day of intellectual curiosity, and continuing to grow a healthy network of next generation of researchers.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;My thought yesterday morning, posting it a day later&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;In retrospect, an obvious personal projection&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Which I only mention as personal grivence since I was not able to present some of my own work this year at a conference that I was not able to attend because of travel…&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Digital Representations</title>
      <link>/post/digital-representations/</link>
      <pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/digital-representations/</guid>
      <description>&lt;p&gt;I obviously could not resist writing a blog post about yesterday’s &lt;a href=&#34;https://www.google.com/doodles/celebrating-johann-sebastian-bach&#34;&gt;Google Doodle&lt;/a&gt; twitter buzz.
I’m not going to try to talk about some of the smaller issues that came up.
And tons of people have already said &lt;a href=&#34;https://medium.com/@tompkinsguitar/googles-bach-ai-a-machine-learning-scientist-with-a-phd-in-music-theory-reacts-68d055f2461d?fbclid=IwAR1CuMvjwoAC1o0EPfBU5pa2mglRKp5BLcHpydh82HxHNktosxA3Ns7QgAg&#34;&gt;interesting things&lt;/a&gt;, but instead I’m hoping to ride the buzz so I can talk about something very relevant to the discussion: encoding and digital representation.&lt;/p&gt;
&lt;p&gt;Using &lt;a href=&#34;https://twitter.com/imanimosley&#34;&gt;Imani Mosley&lt;/a&gt;’s &lt;a href=&#34;http://www.imanimosley.com/humanistmachines/2019/3/21/breaking-the-doodle&#34;&gt;blog post&lt;/a&gt; as a jumping off point, she makes many great points about what Musicology twitter was interested in.
One thing that I was most struck by were claims the reason for the snark was the tired Bach-as-machine trope to explain why scientists gravitate to Bach.
In this post, I want to provide an alternative possibility of why researchers gravitate to Bach.
Researchers like Bach because it’s already encoded.&lt;/p&gt;
&lt;p&gt;As Imani points out in her post, the dataset is made up from 308 compositions that the model was trained on.
You can read more about the model &lt;a href=&#34;https://magenta.tensorflow.org/coconet&#34;&gt;here&lt;/a&gt;, but each Chorale has many more musical events/data points.
Navigating to where the Bach chorales live on my computer and running the &lt;code&gt;census -k *.krn&lt;/code&gt; humdrum command on the first 308 of the Bach Chorales (just to ballpark) that were edited by Craig Sapp results in 72,030 note heads that can be analyzed with tools that can interpret &lt;code&gt;kern&lt;/code&gt; data.
This is a subset of 120,528 humdrum tokens.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/digrep/kern1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I bring this up not to start debates about how much data is needed to train a model, but rather to point out that each little bit of data here had to be entered by hand for it to be used in any model.
And there is no way that you could get a model to even come close to output that would be interesting without very clean data.&lt;/p&gt;
&lt;p&gt;As a researcher looking to tackle problems in this domain, having something like a complete set of Bach’s chorales in a digital format is a fantastic resource.
But one of the problems in &lt;a href=&#34;https://en.wikipedia.org/wiki/Computational_musicology&#34;&gt;computational musicology&lt;/a&gt; is not the depth of data used to try to make a machine part-write in style of Bach, but rather the breadth of representation researchers can chose from when they pick which data to model.
How is it possible to make generalizable claims if you only look at a very small subset of music?&lt;/p&gt;
&lt;p&gt;In general, I work a lot with music and often use computers as a tool in that research.
Unlike the Bach problem here, I am interested in melodies, but the types of data and tools to analyze this data are similar.
One of the major problems I face in my work is that in order to make robust claims about music, you need to be able to show similar phenomena happen with new data.
But where does this new data come from?&lt;/p&gt;
&lt;p&gt;If you’re a data scientist, you might get this from scraping a website or a government database.
If you’re a music psychologist, you often get data from experiments.
If you’re a computational musicologist, new data is not nearly as accessible.
Most likely, you need to either find some or create a digitized version of that data yourself.&lt;/p&gt;
&lt;p&gt;If you take the former option, you first inclination might be to go to somewhere like &lt;a href=&#34;http://kern.ccarh.org/&#34;&gt;kern scores&lt;/a&gt; or see what something like the &lt;a href=&#34;https://simssa.ca/&#34;&gt;SIMSSA project&lt;/a&gt; curates and to see what is available.
If you pop on over there for a second, you see a sample of what you can select from.
For the most part, it’s like a worse version of the old dead white dude problem that is being faced in both curricula and the performance world right now.
That said, there is still tons of data there.
Based on what is current there, the kern scores page has 7,866,496 notes from 108,703 files.
In addition to a lot of dead men, we also get a lot of folk songs, primarily with Hurons’s digitizing of Schaffrath’s work with the Essen collection.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;
You’ll notice that there are not a lot of complete sets of lieder, full-on symphonies, or much new music (copyright and public domain are obviously also an issue).&lt;/p&gt;
&lt;p&gt;If you take the latter option, you have to do encode these melodies yourself and digitizing music is not fun at all.
Over the past year, I encoded 778 monophonic melodies using the &lt;a href=&#34;https://musescore.org/en&#34;&gt;MuseScore&lt;/a&gt; platform.
These 778 melodies resulted in 36,387 notes, a vast majority of which I did personally.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;
It sucked.
I hated having this giant encoding project living over my head throughout the dissertation writing process.
And since I am fresh out of it and still in peak bitter stage, thinking about this whole thing feels very much like &lt;a href=&#34;https://en.wikipedia.org/wiki/The_Little_Red_Hen&#34;&gt;a little red hen situation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the future, this kind of work could remain to be resigned to the individual.
Encoding a corpus could follow the lonely scholar in the ivory tower model where digitizing so much music is a sort of rite of passage.
But I don’t think that’s the best way forward for anyone involved.
I think encoding needs to be much more of a community effort and we can look to fields like psychology of how we as music-types might remedy this problem.&lt;/p&gt;
&lt;p&gt;For example, in psychology, undergrad students taking psych courses in the USA are generally required to take part in experiments which generate the data for the papers that the field of psychology generally reads.
Using such restricted samples leads to problems in that this data is basically &lt;a href=&#34;https://slate.com/technology/2013/05/weird-psychology-social-science-researchers-rely-too-much-on-western-college-students.html&#34;&gt;WEIRD&lt;/a&gt;, but I think you could argue it’s better than not collecting any data.
In music, we don’t really have this problem as much because our research is not this-kind-of-data driven, nor should it in many topics.
But in cases like the one I’m describing here, we have our own litte weird problem.&lt;/p&gt;
&lt;p&gt;To remedy this, I imagine a world where in teaching music theory– where often competence in a computer notation is a learning objective– we spend a bit of time talking about this problem of (digital) representation in music.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;
If every classroom were encodes some pop melodies (also a transcription exercise!), encode a Shostakovitch Fugue (a project I have been putting off for years now), or even encode one part of a symphony, all of this data would slowly start to accumulate for anyone to model.
Doing this would lead to a deluge of new data for researchers to use, allow for Meyerian comparisons between styles, and teach students about serious pressing problems in research while having them learn about notational software.
This also will expose them to the field of computational musicology, something that I didn’t even know existed until half-way through my Masters in &lt;em&gt;psychology&lt;/em&gt;.
Additionally, once compositions cross into the public domain, we as a research community can work towards having more music, more accessible to more people in the form of open scores.&lt;/p&gt;
&lt;p&gt;So it could be that there is this tired trope of Bach-as-machine (which certainly plays into it), but I’d be interested to know how much of what we do is just by-products of the means we have to answer those questions.
Either way, as noted by &lt;a href=&#34;https://twitter.com/Komaniecki_R/status/1108825660653473792&#34;&gt;Robert Komaniecki&lt;/a&gt;, hats off to the Google team for getting people to get so fired up about part-writing.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Schaffrath, H. (1995). The Essen Folksong Collection in Kern Format. [computer database]. D. Huron (ed.). Menlo Park, CA: Center for Computer Assisted Research in the Humanities, 1995.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;A couple of my LSU colleagues helped out on some of them.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Also a great place to talk about human biases in algorithms, the need to be ethical in research…&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>#wikiwednesday</title>
      <link>/post/wikiwednesday/</link>
      <pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/wikiwednesday/</guid>
      <description>&lt;p&gt;One of the biggest issues that I lament about regarding Twitter is how there is so much great information just falling through my feed, never to be recovered again.
A perfect example of this appeared yesterday when &lt;a href=&#34;https://twitter.com/willmasonmusic&#34;&gt;Will Mason&lt;/a&gt; &lt;a href=&#34;https://twitter.com/willmasonmusic/status/1105462217728761857&#34;&gt;tweeted asking for people to help provide examples of idiomatic instrumental constraints leading to funny pitch choices&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/wiki_20190313_post/will_tweet.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Within one day, there were loads examples of this along with some discussion contextualizing the responses.
Examples like this happen all the time on Twitter ranging from people needing specific musical examples to &lt;a href=&#34;https://twitter.com/DougShadle/status/1066802040217759744&#34;&gt;killer Twitter rants like this one&lt;/a&gt; of &lt;a href=&#34;https://twitter.com/DougShadle&#34;&gt;Doug Shadle&lt;/a&gt; that took me way too long to find, which only proves my point.
After liking the tweets, that’s usually where my engagement with the information ends, then I get sad that this information will never be available again for anyone who was not on Twitter yesterday.
I know it lives on Twitter and can dig it out, but I honestly feel I should be spending less time on my computer when I find myself telling more senior music scholars things like&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Oh, actually, there was a really good Twitter thread about this the other day”.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So what can be done?&lt;/p&gt;
&lt;p&gt;Well, one thing that I think a lot of people forget about is that all this information can and should be put on Wikipedia.
After looking at Will’s thread, I put “Idiomatic Music” in the Wikipedia search bar, and was directed to this &lt;a href=&#34;https://en.wikipedia.org/wiki/Instrumental_idiom&#34;&gt;stub of a page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/wiki_20190313_post/pre_edit.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is where I would have hoped to find the information on Will’s tweet if I were looking for it on another day, but it’s not there.
So in true “knowledge should be for everyone” fashion, I added it to the page and thought this might be a good opportunity to procrastinate writing my dissertation by writing a blog about my favorite dissertation writing procrastination activity: editing Wikipedia.&lt;/p&gt;
&lt;div id=&#34;editing-wikipedia&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Editing Wikipedia&lt;/h2&gt;
&lt;p&gt;One thing I have been meaning to blog about, and this obviously has given me a good reason to, is how easy and important I think it is for academics to edit Wikipedia.
It’s so important that I have even turned my love of editing Wikipedia into something I personally refer to as #wikiwednesday.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For me, #wikiwednesday is just a weekly reminder that I should add something to Wikipedia every Wednesday.
For the past few weeks, I have been doing it on topics related to my dissertation, but in this post I honestly wanted to show how easy it is to edit Wikipedia for anyone that hasn’t yet.
Hopefully after seeing a few screenshots, you too will be inclined to help out as well!&lt;/p&gt;
&lt;p&gt;The first thing that you need to do is make an account.
I made a quick one just for this post.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/wiki_20190313_post/make_account.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok, now with an account made, the next thing that you need to do is click the Edit Source button found in the top right corner.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/wiki_20190313_post/editsource.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here (at least my my Firefox?) there’s the option to use the Visual Editor mode.
If you’re not familiar with Markdown and HTML and what not, this is the way to go in my opinion.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/wiki_20190313_post/viz_editor.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From here, this opens up the hood and you can start editing.
I am personally in the process of doing the last 50 days of my dissertation, so I seriously do not have time to restructure this page, but I do have time to add in a bit of text (and do this quick blog post).&lt;/p&gt;
&lt;p&gt;Next, all I wanted to do was create a new heading that would provide some examples of this, then write some very OK text with some concrete examples from Will’s thread.
The text is not poetic by any means, but it’s better for it to be there than not.
And if you think what I wrote sucks, you can always change it!&lt;/p&gt;
&lt;p&gt;After adding all this in, I then tried to link in some of the composers and examples with the link tool.
It might take a second to get a handle of how this works, but it’s time well spent in my opinion.
After a few minutes of editing….&lt;a href=&#34;https://en.wikipedia.org/wiki/Instrumental_idiom&#34;&gt;Tah Dah!&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wiki-ing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;wiki-ing&lt;/h2&gt;
&lt;p&gt;Part of me wanted to just keep going and adding to this page, but my hope is that people will read this post and do a bit of their own #wikiwednesday-ing and just try to do one small edit on this page or another.
I find that once you start, I think it’s pretty fun and actually kind of hard to stop.&lt;/p&gt;
&lt;p&gt;Not only is it fun (my opinion), but I also think that if you are an expert in something (cougheveryonewithaphdcough), you have a responsibility to edit Wikipedia in your own subject area.
Not only is it important to give people up-to-date and accurate information, but Wikipedia is also a place where most people probably start most of their inquiries.
And this isn’t just undergrads, the amount of times I have seen people crack open Wikipedia on their phones at an SMT or AMS talk on a subject outside of their realm of expertise is pretty funny.&lt;/p&gt;
&lt;p&gt;The reason I think this is so important is because this writing will be more read than any article I write in any journal (especially if it’s written behind a pay wall!) and probably have more of a cumulative impact than most of my research.
Also, knowing that Wikipedia is probably the best empirical evidence of what we as a community canonize, it becomes so important to make sure that Wikipedia is representative of the world we want to exist in.
I’m obviously not the first by any means to point this out.
There has recently been coverage of Wikipedia’s diversity issues by the &lt;a href=&#34;https://www.washingtonpost.com/lifestyle/style/history-has-a-massive-gender-bias-well-settle-for-fixing-wikipedia/2019/02/15/b2537640-3163-11e9-86ab-5d02109aeb01_story.html?utm_term=.72aa4b49348f&#34;&gt;Washington Post&lt;/a&gt; and anyone that is not living under a rock (aka not on Twitter) knows that the music community is actively working towards &lt;a href=&#34;https://www.annakijas.com/rebalancing-the-music-canon/&#34;&gt;Rebalancing the Music Canon&lt;/a&gt; as blogged about by &lt;a href=&#34;https://twitter.com/anna_kijas&#34;&gt;Anna Kijas&lt;/a&gt; and working towards more accessibility of music from historically under-represented composers by initiatives like &lt;a href=&#34;https://twitter.com/MTEW_com&#34;&gt;Music Theory Examples by Women&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/ComposerProject&#34;&gt;The Institute for Composer Diversity&lt;/a&gt;.
All I am trying to get across with this post is how important I see it for anyone with skin in the game to take a bit of time out of their day each week and make a habit out of putting information out where most people read it.&lt;/p&gt;
&lt;p&gt;Of course this is being done too, I guess I just am hoping for more of it.
I’ve seen that some people like &lt;a href=&#34;https://twitter.com/K_Leonard_PhD&#34;&gt;Kendra Leonard&lt;/a&gt; starting to organize Wikipedia sessions (couldn’t find this exact tweet to link in?) to create and edit pages on Women composers.
In my opinion, if both &lt;a href=&#34;https://societymusictheory.org/&#34;&gt;SMT&lt;/a&gt; and &lt;a href=&#34;https://www.amsmusicology.org/&#34;&gt;AMS&lt;/a&gt; put aside 2 hours each national conference to edit the music pages of Wikipedia, we as a society could take most of the year off of #publicmusicology (just kidding, but maybe not because that level of human resources editing Wikipedia would be unheard of).&lt;/p&gt;
&lt;p&gt;So on that note, I think it’d be nice if anyone interested in &lt;a href=&#34;https://twitter.com/search?q=%23publicmusicology&amp;amp;src=typd&#34;&gt;#publicmusicology&lt;/a&gt; would join me in my weekly small edits of Wikipedia and share small portions of what they did with #wikiwednesday to remind others to also help out.
Or you could get back to devoting all your time to that article that most people probably will not read (#shotsfired).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;My love for editing Wikipedia pages grew out of taking extensive notes on a server that my advsior, &lt;a href=&#34;https://twitter.com/danielshanahan&#34;&gt;Dan Shanahan&lt;/a&gt; set one up for our department to facilitate collective knowledge for our department.
Of course those posts live in on a “private” wiki, but they will one day see the light of day when I either blog about them or try to get all that info to Wikipedia.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Cataloging Memories with Music</title>
      <link>/post/cataloging-memories-with-music/</link>
      <pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/cataloging-memories-with-music/</guid>
      <description>&lt;p&gt;The past few times that I have sat down to work on my dissertation I have run into a writer’s block.
Though it is not the kind of writer’s block where nothing comes out.
The past few times I have set aside time to write I have had &lt;em&gt;so many&lt;/em&gt; thoughts that I have not been able to just stick my nose to the ground and write.
My brain feel like it’s on fire.
It’s the end of the year and it has been an intense one and it’s important to acknowledge that.
Too often in grad school we work and work and work and work, and never reflect on how we even ended up in this situation in the first place.
There is too much doing and not enough reflecting.&lt;/p&gt;
&lt;p&gt;So to attend to that problem, as opposed to just tweet about how we need a change of culture (but really be frantically work on yet another manuscript) I set aside some time yesterday to write about something I have been wanting to share for a while: my auto-biographical musical playlists.&lt;/p&gt;
&lt;div id=&#34;darling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Darling&lt;/h1&gt;
&lt;p&gt;The general background and inspiration from this post and idea comes from some music science literature investigating how music has the power to evoke strong emotional memories that are not related to the structure of the music itself.
There has been some cool work on it that you can read about &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/09658210701734593&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://static1.squarespace.com/static/55ac0a53e4b05f2993b25780/t/55c9ffdde4b0e9393a61707c/1439301597483/Belfi_etal_2015_Memory.pdf&#34;&gt;here&lt;/a&gt; that digs into it properly, which also runs in tandem with research suggesting why &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/24006129&#34;&gt;really like the music we listened to in our formative years.&lt;/a&gt;
But for all the intents and purposes of this post, all you really have to know is that there is good evidence to suggest that a strong link exists between music and memories of specific moments in our lives.
Ages ago, when I first started getting into music science I heard this referred to as the “Darling, they are playing our song!” effect and &lt;strong&gt;I wanted to see if I could use some of this research in my own life&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So as a result of reading about this area of research in music science, a few years ago I decided to start keeping a playlist diary on Spotify.
The idea of it was that I would add a song to the playlist whenever I had a strong experience associated with music.
The experiences did not have to be exact mappings such as this exact song must be playing during a certain moment for it to be added to my playlist; the idea was that any memory that might be bottled up in a song could get added as not to make the inclusion criteria that strict.
This of course is naturally much easier for a musicophile like myself who basically always has music playing or is around music.
All that needed to happen was for me to be a bit more mindful as I went about my life.
If a song comes on that seems like it is painting a nice soundscape to the backdrop of my life, it gets added.&lt;/p&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;So I have been running this experiment on myself for the past four years.
It’s the end of 2018 now, so what got added to it this year?
Here is a link to the playlist itself.&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/user/1220215602/playlist/3rfnbRTo1WVReR4exGvNjE&#34; width=&#34;300&#34; height=&#34;380&#34; align=&#34;center&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;As I am writing this post I have been listening to the playlist and, like the past few years, when listening to it, I get immediately transported back to the memories I actively chose to catalog.
The first couple of songs take me right back to celebrating the New Year with my friends in Baton Rouge.
For example, I remember the second tune, Lambada, coming on the radio as my friends drove to buy food to make breakfast after a long weekend in New Orleans.
The moment felt right, so I whipped out my phone to Shazam the tune, and then later added it to my list.&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/track/3F7HdGevVojxdsARD4RHad&#34; width=&#34;300&#34; height=&#34;380&#34; align=&#34;center&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;It’s not just exact moments that I catalog, but I also record re-occurring songs.
For example, later on the list is some Debussy.&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/track/00IZ3ndf3TNXt3hMum4WAV&#34; width=&#34;300&#34; height=&#34;380&#34; align=&#34;center&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;I like the music, but more importantly it’s used in &lt;a href=&#34;https://en.wikipedia.org/wiki/Westworld_(TV_series)&#34;&gt;Westworld&lt;/a&gt; and I used to have weekly Sunday screenings of the show at my apartment in Baton Rouge.
I wanted to remember sitting quietly in a room with my friends having a bit of a break before Monday would come up and grad school would restart.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;patterns&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Patterns&lt;/h2&gt;
&lt;p&gt;One day I’d like to look at the list and maybe see some patterns over a lifetime.
For example, I’d imagine that more songs/memories would cluster around highly anticipated emotional events.
I saw this play out last year with two salient songs from my PhD General Exams.
The first example is this great tune my friend Crystal put on in my living room after a day of drinking celebrating being done with three days of writing for the first part of our General Exams and learning the &lt;a href=&#34;https://www.youtube.com/watch?v=uwJ66XxGBFk&#34;&gt;line dance&lt;/a&gt; to this.&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/track/42excP3MyVua6modv3v9Pz&#34; width=&#34;300&#34; height=&#34;380&#34; align=&#34;center&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;Fast forward a few weeks and the night before my oral defense, I remember sitting in a Sonic parking lot eating ice cream with my friends Sasha and Jacob, listening to Eye of the Tiger at full volume.
It was a moment where it felt like God had momentarily DJ’d for us.&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/track/2KH16WveTQWT6KOG9Rg6e2&#34; width=&#34;300&#34; height=&#34;380&#34; align=&#34;center&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;Cruising through this playlist I also realize how much traveling I did this year and all the great people I got to hang out with.
Just weeks after Eye of the Tiger, I got to hang out at a public musicology conference in South Carolina and ended up hanging out with both &lt;a href=&#34;https://twitter.com/darkmusictheory?lang=en&#34;&gt;darkmusictheory&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/12tonevideos?lang=en&#34;&gt;12tone&lt;/a&gt; late at night in an AirBnB and remember really liking the piano intro to this tune.&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/track/4L1eqdFY10yO7tMzvRp0sx&#34; width=&#34;300&#34; height=&#34;380&#34; align=&#34;center&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;Much of my playlist comes from memories traveling.
In Florida I remember sitting with my friend Rory while talking about how we are spending our lives and this recording of &lt;em&gt;A Mi Manera&lt;/em&gt; came on the radio.&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/track/0Nx5c4i2z90wi8Uj6LbUbH&#34; width=&#34;300&#34; height=&#34;380&#34; align=&#34;center&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;This was the first vacation/holiday I had ever taken just for the sake of a vacation (not academic work related) in years and I distinctly remember sitting on the patio, asking the server to start happy hour early, mis-ordering our 2 for 1 margaritas, then both being served 2 drinks at 2PM and boozing it up in full Jimmy Buffet regalia.
Of course I won’t go on a play by play&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; of my whole list, but the meaningful thing is that as a whole this playlist only ‘makes sense’ to me.&lt;/p&gt;
&lt;p&gt;Someone looking at the playlist might be able to figure out some of the tunes given that they knew where I was at what time such as this recording added at the start of July.&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/track/1b7kLkJkr6XRYgWqVmDm05&#34; width=&#34;300&#34; height=&#34;380&#34; align=&#34;center&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;But other songs that I have known for a while get emotionally recycled and obtain new meaning just for me.&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/track/6pGEheBKHMS6cvYqqKcU6O&#34; width=&#34;300&#34; height=&#34;380&#34; align=&#34;center&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;div id=&#34;zooming-out&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Zooming Out&lt;/h2&gt;
&lt;p&gt;And the cool thing is, as I mentioned above, I have been doing this for the past four years.
Going all the way back to 2015 I can still vividly transport myself back to eating donuts in an NYC apartment listening to Laura Mvula.&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/track/6VKhTOUK905auFiINtfLZZ&#34; width=&#34;300&#34; height=&#34;380&#34; align=&#34;center&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;And even as the 2015 playlist plays, I still anticipate each song within this playlist’s context with each memory being anticipated as well.
So what I guess I am basically suggesting is that on a personal level, if you have an assortment of external musical meaning your life, maybe it is time to consider starting an autobiographical musical memories playlist yourself in 2019?
I have found knowing that this playlist is there helps me stay more present when music is playing (which is a lot of the time) since you never know if a moment will meet your inner arbitrary threshold for adding it to the list.&lt;/p&gt;
&lt;p&gt;I would also be interested to know if anyone after reading this does go ahead and do it.
I’d be very keen to have an intense music psychology conversation about similarities in partaking in this activity.
I’ve also linked here the past four years just for proof, but also want to show off how diverse this kind of playlist can get since it’s not bound by anything like genre, style, mood, or some sort of lyrical similarity.&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/user/1220215602/playlist/2Ndu7gNgVs1KFMUB4rjoA9&#34; width=&#34;300&#34; height=&#34;380&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/user/1220215602/playlist/7kA7U2jeQIMzJjvhU8knhs&#34; width=&#34;300&#34; height=&#34;380&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/user/1220215602/playlist/5Gb4jiSS6ikyEJOgpemxn6&#34; width=&#34;300&#34; height=&#34;380&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/user/1220215602/playlist/3rfnbRTo1WVReR4exGvNjE&#34; width=&#34;300&#34; height=&#34;380&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Pun intended&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Examining Musical Sophistication</title>
      <link>/post/examining-musical-sophistication/</link>
      <pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/examining-musical-sophistication/</guid>
      <description>&lt;div id=&#34;replication&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Replication&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://davidjohnbaker.rbind.io/post/examining-musical-sophistication/&#34;&gt;Replication&lt;/a&gt;.
Is it popular and important for research in psychology?
Yes.
Will reading a replication paper get you all fired up about research?
Probably not.
That’s OK though, research really shouldn’t be all about new and flashy findings.
Every so often we should stop and think about what we are doing.&lt;/p&gt;
&lt;p&gt;Today our lab has a new publication “Examining Musical Sophistication: A replication and theoretical commentary on the Goldsmiths Musical Sophistication Index” that’s &lt;a href=&#34;https://journals.sagepub.com/eprint/qwVM6dhD6ipcWM4XAacv/full&#34;&gt;accessible online&lt;/a&gt; that replicates the &lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0089642&#34;&gt;Goldsmiths Musical Sophistication Index&lt;/a&gt;.
The “just-tell-me-if-I-can-keep-using-it” verdict of the paper is: “Yes, knock yourself out”.
Scroll to the bottom of the page, grab the APA citation and cite us as Baker, Ventura, Calamia, Shanahan, and Elliott (2018).&lt;/p&gt;
&lt;p&gt;If you’re keen on psychometrics and are interested in a few of the points we make in the paper, then read on.&lt;/p&gt;
&lt;p&gt;To give the very short version of the paper in plain English (not abstract-ese) the paper generally goes like this.
Over the past century the world of music science has struggled to quantify what it means to be musical.
We clearly need some measure of it if we want to make claims about how much engaging with music relates to other constructs, but every time you try to pin down what you think being musical is, you realize you forgot something.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measuring-musicality&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Measuring Musicality&lt;/h2&gt;
&lt;p&gt;A lot of people have tried to get around this problem a few different ways, but in this paper we decided to focus on the &lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0089642&#34;&gt;Goldsmiths Musical Sophistication Index&lt;/a&gt; since it’s a tool that has getting very popular, very quickly within the music science world and had previously been shown to be valid and reliable in a large UK sample.&lt;/p&gt;
&lt;p&gt;The inspiration for the paper came from a lab meeting one day at LSU where we were discussing open science and our own research practices, and how we wanted to participate in this important movement.
Given the resources we had at the time, we decided it would be a good idea to investigate the replicability of the Gold-MSI.&lt;/p&gt;
&lt;p&gt;We had a couple of specific reasons for this as well:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Most researchers are not going to be using samples like that in the original paper (N= 147,000+) and will probably just use it on their &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/20550733&#34;&gt;WEIRD&lt;/a&gt; subject pool, therefore we should investigate if there are any “weird” things that happen when you use it like this.&lt;/li&gt;
&lt;li&gt;Some researchers break off a part of the Gold-MSI and use just one of the sub scales; is that a good idea?&lt;/li&gt;
&lt;li&gt;An independent replication of the Gold-MSI would be a valuable contribution and it’d be good to write a paper that might be helpful for others to read and cite.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We had also just read this great paper on &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/1047840X.2016.1153946&#34;&gt;Process Overlap Theory&lt;/a&gt; and saw a lot of parallels between what Kovacs and Conway had to say when arguing against the idea of &lt;em&gt;g&lt;/em&gt; and the idea of measuring anything musical with a latent variable.&lt;/p&gt;
&lt;p&gt;So we went ahead, grabbed us some data, and spent a lot of time reading Alex Beaujean’s &lt;a href=&#34;https://www.amazon.co.uk/Latent-Variable-Modeling-Using-R/dp/1848726996&#34;&gt;Latent Variable Analysis with R&lt;/a&gt; book to dive deep into the world of latent variable modeling.
So what did we find?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;practice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Practice&lt;/h2&gt;
&lt;p&gt;In general, the Gold-MSI pretty much behaved as you would expect it to.
Using a WEIRD sample, we had a bit of a skewed distribution that people should look out for, but the sub-scale scores pretty much lined up with the means and standard deviations that were originally reported.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/Figure1_GMSIREP.png&#34; alt=&#34;Overall Skew&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Overall Skew&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/Figure2_GMSIREP.png&#34; alt=&#34;Similar Descriptive Statistics&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Similar Descriptive Statistics&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;That said, we did look at the item level data and found anything but normal distributions.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/Figure3_GMSIREP.png&#34; alt=&#34;Item Level Distributions&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Item Level Distributions&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This is not the biggest deal in the world due to the estimators you can use with the &lt;a href=&#34;http://lavaan.ugent.be/&#34;&gt;lavaan package&lt;/a&gt; that were used in the original paper, but if you are going to break off a scale to use just a part of the Gold-MSI, it’s worth keeping in mind.
We noted the biggest problems with this using the Emotion sub-scale.&lt;/p&gt;
&lt;p&gt;We also show have &lt;strong&gt;a lot&lt;/strong&gt; of tables in the paper that go over everything from descriptive statistics to model fits.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Theory&lt;/h2&gt;
&lt;p&gt;Given our generally successful replication, is there anything else researchers should know?
We we zoomed out a bit (from painfully up close to the Gold-MSI) and expanded outwards in our discussion.
In the paper we highlighted that the Gold-MSI uses latent variables, which basically relies on the same math as calculating &lt;a href=&#34;https://en.wikipedia.org/wiki/G_factor_(psychometrics)&#34;&gt;&lt;em&gt;g&lt;/em&gt;&lt;/a&gt;.
The construct &lt;em&gt;g&lt;/em&gt; is calculated using a bunch of objective tests that correlate with one another, but you still use factor analytic techniques to derive the larger constructs, in this case the General Sophistication score and its sub scales (actually it is even fancier than that since they use &lt;a href=&#34;https://link.springer.com/article/10.1007/BF02289209&#34;&gt;a bifactor solution&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;After pointing this out we draw upon a few arguments made by Kovacs and Conway and argued that even though someone might score highly on some of the Gold-MSI scales, it doesn’t mean that they are using their “musical sophistication” to actually carry out musical tasks.
Just like you wouldn’t say you used your general intelligence to solve a mental rotation puzzle, you wouldn’t say you used your musical sophistication to do a melodic memory task.
We note that this is great for descriptive theories, but we just want to remind people that they shouldn’t confuse a statistical abstraction for any sort of process that is actually happening.&lt;/p&gt;
&lt;p&gt;We get a lot more fanciful with the language in the article, but hopefully you’ll give it a read if you’ve gotten this far in the blog post.
The paper ends saying that people should of course keep using the Gold-MSI as we desperately need more consistency in measurements, but as a community we need to think about what our models are actually telling us.
This is especially true in that we are now coming up on the 100 year anniversary of Seashore’s Measures of Musical Talents and it’s a perfect time to stop and reflect on the tools we are using and the questions we are trying to ask.&lt;/p&gt;
&lt;p&gt;If this sounds up your alley, give the paper a read.
You can access it &lt;a href=&#34;https://journals.sagepub.com/eprint/qwVM6dhD6ipcWM4XAacv/full&#34;&gt;here&lt;/a&gt;, and if you’d like to cite us, just copy and paste below!&lt;/p&gt;
&lt;p&gt;We’re big proponents of collaborating, sharing data, and being transparent, so if anyone wants to get their hands on our data and analysis, please get in touch and check out our lab’s &lt;a href=&#34;https://osf.io/cdqm5/&#34;&gt;OSF page&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Also a big thanks to our reviewers!
The paper was of much higher quality after the suggestions!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Baker, D. J., Ventura, J., Calamia, M., Shanahan, D., &amp;amp; Elliott, E. M. (2018). Examining musical sophistication: A replication and theoretical commentary on the Goldsmiths Musical Sophistication Index. Musicae Scientiae. &lt;a href=&#34;https://doi.org/10.1177/1029864918811879&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/1029864918811879&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Project Management with git and R</title>
      <link>/post/project-management-with-git-and-r/</link>
      <pubDate>Tue, 08 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/project-management-with-git-and-r/</guid>
      <description>&lt;div id=&#34;staying-organized&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Staying Organized&lt;/h2&gt;
&lt;p&gt;The semester is finally over meaning it’s time to put some serious dents into my research projects. I’ve got a couple floating around at this point, but the one that I need to think about the most is my PhD dissertation. From what I’ve heard it’s quite an intensive ordeal and given that over the next year I need to write a multi-chapter document with a dissertation committee spread across thousands of miles, I think it might be important to try to &lt;strong&gt;stay organized&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A couple of different people I follow on Twitter have suggested that I use &lt;a href=&#34;https://www.literatureandlatte.com/scrivener/overview&#34;&gt;Scrivner&lt;/a&gt; to keep track of writing and what not, but for the past year or two I have really been trying to push myself to commit exclusively to using &lt;a href=&#34;https://en.wikipedia.org/wiki/Free_and_open-source_software&#34;&gt;Free and Open Source Software&lt;/a&gt; for everything I do. For some time now I have been thinking about trying to write my entire dissertation using all things &lt;a href=&#34;https://cran.r-project.org/&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;RStudio&lt;/a&gt; and I think I’m ready to try it. I don’t want to get on a rant here about FOSS and how great R is, but let me just give a few reasons why I want to head down this path as opposed to using proprietary, paid software.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It’s free&lt;/li&gt;
&lt;li&gt;Learning the ins and outs of all of this is going to make me a better programmer/researcher over the course of the next year (more skills = more employment options!)&lt;/li&gt;
&lt;li&gt;Working with this will help make my work more accessible and reproducible&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That said, I’m not only going to try to write my thesis using only free software (I’m sure there will be exceptions), but one thing I also want to do is to document as much of the process as I can so that anyone else (especially those without computer backgrounds!) can do the same if they would ever want to learn these sets of tools. I’ve always thought it’s a shame that a lot of this software is not common place in the Humanities and hope that through blogging about how to do it, it will help others who want to use it find it easier.&lt;/p&gt;
&lt;p&gt;So where do you even start?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-projects-and-github&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R Projects and Git(hub)&lt;/h2&gt;
&lt;p&gt;When starting a new project, the first thing that I normally do is to create a place on my computer where the entirety of that project can live and then link that up to &lt;a href=&#34;http://github.com&#34;&gt;github&lt;/a&gt; so I can share what I do with the people I work with.&lt;/p&gt;
&lt;p&gt;In this post I will just walk through how to just set up a version controlled project with R via Github.&lt;/p&gt;
&lt;div id=&#34;step-one-create-a-repository-on-github&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step One: Create a repository on Github&lt;/h3&gt;
&lt;p&gt;I find it’s easiest when starting a project to start on github first, then copy that onto your computer.&lt;/p&gt;
&lt;p&gt;The first thing to do is to log into your github account (assuming you have one, it’s easy to sign up!) and create a new repository (repo) using the little plus at the top right of your home page as you can see below.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/pmwr1.png&#34; alt=&#34;Navigate to New Repository Page&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Navigate to New Repository Page&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From here you want to give your repo a name, a small description, and tick the box that asks you initialize it with a README file. The screenshot below shows the page right before I click the green “Create Repository” button.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/pmwr2.png&#34; alt=&#34;Setting up your repo&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Setting up your repo&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;When choosing a name, it’s important to pick something easy to recognize and type. Once you click ‘Create Repository’, it will take you to your repo’s home page and should look something like this:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/pmwr3.png&#34; alt=&#34;The Blank Page&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;The Blank Page&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From here you are going to want to click the green ‘Clone or download’ and then copy that link to your clipboard. The next screen shot shows what that will look like.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/pmwr4.png&#34; alt=&#34;Clone the Git HTTPS&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Clone the Git HTTPS&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;With the link copied, you then want to open up your terminal (if you’re a Windows user, make sure to get &lt;a href=&#34;https://git-scm.com/&#34;&gt;Gitbash&lt;/a&gt;!!) and then navigate to where you want your project to live. I keep all my projects in a directory called &lt;code&gt;projects&lt;/code&gt; on my desktop for easy access. From here, you then want to use git to clone your project. If you’re totally new to using this kind of software to do work, you’ll also have to install git. If you don’t have git, &lt;a href=&#34;https://www.linode.com/docs/development/version-control/how-to-install-git-mac/&#34;&gt;this&lt;/a&gt; page will show you how to get it via &lt;code&gt;homebrew&lt;/code&gt; at the command line, which, SURPRISE! you also have to download separately!! You can get that &lt;a href=&#34;https://brew.sh/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Side note: when you are first starting out with all things computer-program-tech-whatever, you are going to need to get a lot of software before you can even start to use all the fun stuff. I remember this being particularly discouraging and no fun when I started. If you can, it’s best to try to get someone to slowly walk you through this via some &lt;a href=&#34;https://en.wikipedia.org/wiki/Pair_programming&#34;&gt;pair programming&lt;/a&gt;. If not, don’t be afraid to ask questions when you start!&lt;/p&gt;
&lt;p&gt;So getting back to what we were doing, you need to navigate to where you want your project to live (the first line) and then clone your directory by typing &lt;code&gt;git clone yourproject.git&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/pmwr5.png&#34; alt=&#34;Cloning Your Repo&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Cloning Your Repo&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now if you look in your projects folder (or just press &lt;code&gt;ls&lt;/code&gt; as in “&lt;code&gt;l&lt;/code&gt;i&lt;code&gt;s&lt;/code&gt;t” all the files that are in your directory), you’ll notice you have a new directory sent directly from github heaven!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-two-making-it-an-r-project&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step Two: Making It an R Project&lt;/h3&gt;
&lt;p&gt;Since I’m going to primarily using R to manage this project, I then need to turn this directory into an R project so that every time I open this project, it can be governed with R.&lt;/p&gt;
&lt;p&gt;In order to do that, I need to get out of terminal, and then open up RStudio. From here you need to go to &lt;strong&gt;File &amp;gt; New Project &amp;gt; Create From Existing Directory &amp;gt; &lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/pmwr6.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Then navigate and select to open the folder you just cloned from github.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/pmwr7.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;If you then tick the box that says to open the box in a new project, you’ll then see something like this:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/pmwr8.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Tah dah! We’ve now made our directory an R project and can save our changes on github.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-three-pushing-it-the-cloud&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step Three: Pushing it the Cloud&lt;/h2&gt;
&lt;p&gt;Now all that’s left to do is sync up the changes on our local machine with that on github. We can bring terminal back and assuming that we are in our project’s directory (we can check that by typing &lt;code&gt;pwd&lt;/code&gt; in Terminal), we then need to enter the work life cycle of updating a github account with the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git add .
git commit -m &amp;quot;Whatever you did&amp;quot;
git push &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or how I did it with my project here:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/pmwr9.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;If we then go back to our project page on Github.com, we can see that our changes should have been updated!&lt;/p&gt;
&lt;p&gt;I should note that each time you start your project you want to &lt;code&gt;git pull&lt;/code&gt; to make sure that you have the most updated version of the project (for when you are eventually collaborating with other people).&lt;/p&gt;
&lt;p&gt;In writing this I actually realized there is SO MUCH that underlies this seemingly basic process of starting a project, so if anyone has any questions on this whole process, please let me know and I can try to make this post (and future ones) clearer.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>To-Dos and Boundaries</title>
      <link>/post/to-dos-and-boundaries/</link>
      <pubDate>Mon, 09 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/to-dos-and-boundaries/</guid>
      <description>&lt;p&gt;As I write this sentence I know exactly how much time until my next break (45 minutes and 39 seconds if you’re interested). When my &lt;a href=&#34;https://itunes.apple.com/us/app/activity-timer/id808647808?mt=12&#34;&gt;Activity Timer&lt;/a&gt; dings I’ll stop what I am doing, even if I am mid-thought and do something else. Maybe I’ll make a cup of tea, or maybe attend to that unread Facebook message, but most likely get on Twitter. I really have no idea what I will do, but I do know that as soon as my clock is not counting down I am technically ‘off the clock’.&lt;/p&gt;
&lt;p&gt;This might seem absolutely insane, but when it comes to productivity and being a graduate student, if you don’t find something that works for you, you’ll find yourself feeling unbelievably stressed at all times. I’m weirdly proud of my own productivity system and after a brief exchange with &lt;a href=&#34;https://twitter.com/thomas_mock&#34;&gt;Thomas Mock&lt;/a&gt; on Twitter yesterday I figured it might be worth sharing. By documenting exactly how I do what I do, it’d also remind me to practice what I preach as I enter the last four weeks of the semester here at LSU.&lt;/p&gt;
&lt;p&gt;Most of this falls into two categories:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;To-do lists and&lt;/li&gt;
&lt;li&gt;Boundary setting at the local and global level.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I’ll explain each in turn.&lt;/p&gt;
&lt;div id=&#34;to-do-lists&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;To Do Lists&lt;/h2&gt;
&lt;p&gt;Some of the best advice on to-do lists I ever received was from &lt;a href=&#34;https://twitter.com/lewisrichard&#34;&gt;Richard Lewis&lt;/a&gt; when I was an RA on the &lt;a href=&#34;https://tm.web.ox.ac.uk/&#34;&gt;Transforming Musicology&lt;/a&gt; project. He was managing a huge amount of the project, as well as his own work at the time and I asked him how he did it. He showed me his to-do list. It was a simple text file (come to think of it, I think it was &lt;a href=&#34;https://orgmode.org/&#34;&gt;emacs org-mode&lt;/a&gt;) and gave me the sage advice of&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“only write down something on a to-do list that you can do.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As dumb as it sounds, think of all the times you have written down something like ‘Write Final Paper for American Music Class’ as an item on a to-do list. You can’t actually DO that in one sitting (and if you DO do things like that, you might have a perfectionism/procrastination problem and HIGHLY recommend &lt;a href=&#34;https://www.amazon.com/Too-Perfect-When-Being-Control/dp/0449908003&#34;&gt;this book&lt;/a&gt; to read), but you can DO all the component parts of said project. What you have do is find a way to organize all the ‘things’ you have to do and break them down into manageable steps. I structure my to-do list hierarchically into all of the projects or classes that I am currently working on. My to-do list is a text file that I edit with VIM that takes advantage of &lt;code&gt;vimoutliner&lt;/code&gt; where you can find &lt;a href=&#34;https://github.com/vimoutliner/vimoutliner&#34;&gt;here&lt;/a&gt;. I access it by opening up my terminal (I also work as much as I can in terminal on Mac because I am easily distracted and like to feel like I am hacking in The Matrix) via an alias command. I just type ‘workflow’ into my command line and it opens up the to-do list. To learn how to set this up, check out &lt;a href=&#34;https://coolestguidesontheplanet.com/make-an-alias-in-bash-shell-in-os-x-terminal/&#34;&gt;this link here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The file is organized hierarchically and &lt;code&gt;vimoutliner&lt;/code&gt; color codes the indents. At the top of the file I have major deadlines that I shouldn’t forget about. As you look down the list there are high level headings like industry, lsu, and personal. Under each larger level umbrella is the actual project which has its own folder/directory (often linked to a github repo because I love those green commit squares). Then under the project are the actual to-dos. Each line starts with an action verb in all caps followed by something I could do in one sitting. Important to-dos get a &lt;code&gt;;&lt;/code&gt; which my text editor colors as red, which I associate with as important.&lt;/p&gt;
&lt;p&gt;Here is what my current one looks like:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/todo.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;So when I am ready to work, all I do is type ‘workflow’ into terminal, then I have a whole list of things I can pick based on priority and what I have the mental energy for. By having a few things to choose and having everything written down my work is out of sight, out of mind for when I am not “working”. I often tell people that if I don’t write it down, I won’t do it. It’s a double edged sword because on the positive side, you don’t feel the mental heat &lt;a href=&#34;https://twitter.com/DavidJohnBaker/status/983456868214468609&#34;&gt;I was tweeting&lt;/a&gt; about yesterday as things pile up. They are just all on your to-do list and you’ll get to them when you sit down to work. The negative side is that if you use this system and forget to write it down immediately, you will forget to do it.&lt;/p&gt;
&lt;p&gt;So with all this stuff to &lt;strong&gt;do&lt;/strong&gt;, how do I actually then &lt;strong&gt;do&lt;/strong&gt; it? In order to answer that question, I need to take a step back and talk about boundaries.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;boundary-setting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Boundary Setting&lt;/h2&gt;
&lt;p&gt;The best and worst thing about graduate school is the lack of structure. It’s fantastic in that I can work when I want, where I want, and as long as I meet my deadlines, I wont be hanged, drawn, and quartered by my advisers. It’s terrible in that with no set boundaries, it creates this constant ‘I should be working right now’ culture (a world I am very familiar with coming from an undergraduate degree in music where you just replace ‘working’ with ‘practicing’). To be fair, my advisers would never do that, they might get a bit irritated but they are the nicest, most supportive people ever. I have found that the best way to work around this structure problem is to set very firm boundaries both at a local and global level.&lt;/p&gt;
&lt;div id=&#34;local-boundaries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Local Boundaries&lt;/h3&gt;
&lt;p&gt;One of the hardest things to do with work is to just start. Often I don’t start to work because I feel like the problems that I have to tackle are just too big. Add in a bit of self-doubt, fear of your work not being good enough, and looming thoughts of a terrible academic job market that your peers constantly remind you of and it’s almost like why even bother? The best way I get around that is to just start working by picking a certain amount of time you think you can commit your full attention to something. It might be five minutes, ten minutes, the holy &lt;a href=&#34;https://en.wikipedia.org/wiki/Pomodoro_Technique&#34;&gt;twenty five minute pomodoro&lt;/a&gt;, or you might have the juice to sit down for 50 minutes (this session). With that in mind, or knowing what I kind of work I feel like doing (writing, coding, emails) I press ‘start’ on my timer, open up my &lt;code&gt;workflow&lt;/code&gt; and pick one of the things I already wrote down to do that could be done in one session. While the clock is ticking down &lt;strong&gt;ONLY WORK ON THAT ONE PROJECT&lt;/strong&gt;. No Facebook, no phone, no Twitter. Don’t even start a conversation with your lab mate that walks in. If they start talking to you, practice saying ‘No’ to them and tell them you are doing a pomodoro session and will be done in 9 minutes and 19 seconds (current clock). When the timer goes off, stop. Even if you are almost done! The reason for this is that so you can get a quick breath so when you press ‘Start’ the next time, you will know exactly how to start your work!&lt;/p&gt;
&lt;p&gt;I find that doing this just a few times will get me in a much healthier place in terms of work life boundaries. And the best thing about it is as soon as the clock stops, you’re done and can make a cup of tea or coffee.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;global-boundaries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Global Boundaries&lt;/h3&gt;
&lt;p&gt;The other important catch to this system is setting global boundaries. What I mean by this is that you need to start working at about the same time every day and end at a reasonable time. I try to get into the office everyday at about 9AM and leave at about 6PM. It’s a long-ish work day (I take an hour lunch), but the best part is that having this regularity makes me happy. It might seem like I am just doing a 9-5 thing, but since I’m a grad student (and ABD right now) I can just stop whenever I want! The thing is I often do not because I weirdly really like what I do. The whole point of setting boundaries it to avoid burnout!&lt;/p&gt;
&lt;p&gt;In addition to keeping semi-regular work hours I also do not have email or slack on my phone. My first music theory professor on my first day of class of undergraduate told us ‘there is no such thing as a music theory emergency’. He is correct. Most problems are not emergencies and the rest you get at night is much better than being on call 24/7. If you are staying on top of your work and working healthily to a goal, this system is &lt;em&gt;supposed&lt;/em&gt; to prevent you from running into a place where you are submitting something at the last possible second.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-thoughts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Closing Thoughts&lt;/h3&gt;
&lt;p&gt;If after reading that you think “Dave, sounds like you just have a cushy grad position, you probably don’t work that hard. On the other hand I work 60-70 hours a week as a grad student!” I would have two things to say to you. The first is that I don’t believe you are as productive as you could be. I’d personally pay for the Go-Pro for you to wear all week and would be interested to know much time people accidentally waste each day looking at social media and doing other unproductive things. The other is that you might consider your ability to say ‘No’ to people. One of the hardest things I have hard to learn is to say ‘No’ and not let productivty creep happen (become more productive, get more time, fill free time with work). Saying ‘No’ is very hard but I actually got a lot of help learning about it when I read &lt;a href=&#34;https://twitter.com/DrHenryCloud&#34;&gt;Henry Cloud’s&lt;/a&gt; book on &lt;a href=&#34;https://www.amazon.com/Boundaries-Updated-Expanded-When-Control/dp/0310351804/ref=sr_1_1?ie=UTF8&amp;amp;qid=1523372885&amp;amp;sr=8-1&amp;amp;keywords=boundaries+book&#34;&gt;Boundaries&lt;/a&gt; about six years ago. I could do a whole blog post on that book.&lt;/p&gt;
&lt;p&gt;While there is something to be said about just falling deep into your work, especially at the start of grad school, I always try to do a bit of self reflection and ask myself if what I am doing is sustainable. The first two years of my PhD were not and it lead to me being very depressed and irritable at times. I really credit some of these ideas I just detailed with moving away from a lot of those very toxic mindsets that permeate many academic’s social media feeds (WORK WORK WORK!). I’d really like to have my cake and eat it too with a healthy work/life balance and this is just one idea that moves me closer to that goal.&lt;/p&gt;
&lt;p&gt;Also for those keeping score at home this post took 1 50 minute session, one 20 minute session, then I went to bed, and did three more 20 minute sessions to get it done and posted.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hire Me (as a Data Scientist!), Part IV</title>
      <link>/post/hire-me-as-a-data-scientist-part-iv/</link>
      <pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/hire-me-as-a-data-scientist-part-iv/</guid>
      <description>&lt;p&gt;Since this is my last post in the beer review series, I’ll keep this short and sweet in terms of analysis. Having done all of this, I do have a few reflections I would like to share after doing the &lt;a href=&#34;https://www.linkedin.com/pulse/how-hire-test-data-skills-one-size-fits-all-interview-tanya-cashorali/&#34;&gt;One-Size-Fits-All-Data-Science Interview&lt;/a&gt; that I have included at the end.&lt;/p&gt;
&lt;p&gt;Our final question to answer is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lastly, if I typically enjoy a beer due to its aroma and appearance, which beer style should I try?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a pretty broad question and should be able to be answered without many pitfalls, so let’s get started.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;data.table&amp;#39; was built under R version 3.4.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer &amp;lt;- fread(&amp;quot;data/beer_reviews.csv&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
Read 56.7% of 1586614 rows
Read 1586614 rows and 13 (of 13) columns from 0.168 GB file in 00:00:03&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From a bird’s eye view, it seems like the most sensible thing to do would be to look at our data from the highest level, then just zoom in until we have the level of granularity we feel answers the question well. Let’s first average all the beer styles to get a rough estimate of how a beer style fairs on the variables we are interested in, and additionally see how much variability is associated with that measurement.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sem &amp;lt;-function(x) {sd(x)/sqrt(length(x))}

question4.means &amp;lt;- beer[, .(mean_aroma = mean(review_aroma), mean_appearance = mean(review_appearance), mean_overall = mean(review_overall),
                            sem_aroma = sem(review_aroma), sem_appearance = sem(review_appearance), sem_overall = sem(review_overall),
                            sd_aroma = sd(review_aroma), sd_appeareance = sd(review_appearance),sd_overall = sd(review_overall)), 
                        by = beer_style]
question4.means&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                           beer_style mean_aroma mean_appearance
##   1:                      Hefeweizen   3.761735        3.828293
##   2:              English Strong Ale   3.749427        3.852469
##   3:          Foreign / Export Stout   3.828366        4.039015
##   4:                 German Pilsener   3.387159        3.572444
##   5:  American Double / Imperial IPA   4.097782        4.078916
##  ---                                                           
## 100:                          Gueuze   4.117574        4.034864
## 101:                            Gose   3.783528        3.908163
## 102:                        Happoshu   2.595436        2.925311
## 103:                           Sahti   3.827992        3.655985
## 104: Bière de Champagne / Bière Brut   3.734704        4.045889
##      mean_overall   sem_aroma sem_appearance sem_overall  sd_aroma
##   1:     3.929626 0.003668940    0.003595729 0.004051038 0.6129217
##   2:     3.783288 0.008118012    0.007674182 0.009323636 0.5623738
##   3:     3.877679 0.007222404    0.006830955 0.008163490 0.5581381
##   4:     3.731573 0.004611304    0.004323963 0.005097580 0.6863721
##   5:     3.998017 0.001937927    0.001600133 0.002171618 0.5682357
##  ---                                                              
## 100:     4.086287 0.007225256    0.006450020 0.008273156 0.5600855
## 101:     3.965015 0.019413627    0.015860893 0.023754558 0.5084740
## 102:     2.914938 0.048722437    0.051373864 0.063538226 0.7563756
## 103:     3.700283 0.019516104    0.017677122 0.021691778 0.6356980
## 104:     3.648184 0.021782360    0.018920362 0.026781986 0.7044834
##      sd_appeareance sd_overall
##   1:      0.6006912  0.6767538
##   2:      0.5316275  0.6458931
##   3:      0.5278874  0.6308640
##   4:      0.6436027  0.7587521
##   5:      0.4691883  0.6367582
##  ---                          
## 100:      0.4999910  0.6413163
## 101:      0.4154222  0.6221699
## 102:      0.7975368  0.9863785
## 103:      0.5757968  0.7065662
## 104:      0.6119209  0.8661809&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Knowing how each beer style fluctuates on our variables of interest (with our overall score thrown in for good measure!), let’s plot our results. Note that I have included standard error of the mean error bars as a sanity check to make sure that each beer’s ratings is not going to bleed into the others’ dimensions. By doing this, we can have a bit more confidence that we are looking at actually has some meaning. This plot shows the standard error on each of the two variables we are interested in, and for the most part it looks as if they are relatively well contained.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(question4.means, aes(x = mean_aroma, y = mean_appearance, color = beer_style)) + 
  geom_point() +
  geom_errorbar(aes(ymin=mean_appearance-sem_appearance, ymax=mean_appearance+sem_appearance), width=.1) +
  geom_errorbarh(aes(xmin=mean_aroma-sem_aroma, xmax=mean_aroma+sem_aroma)) + theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(title = &amp;quot;Mean Appearance and Aroma&amp;quot;, y = &amp;quot;Mean Aroma&amp;quot;, x = &amp;quot;Mean Appearance&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-29-hire-me-as-a-data-scientist-part-iv_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This graph has a lot of beers, but what we are interested in is that top right quadrant where both the average appearance and aroma are maxed out. Let’s zoom in on it and throw in a sizing variable of the overall rating and inspect our graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(question4.means[mean_appearance &amp;gt; 4 &amp;amp; mean_aroma &amp;gt; 4], 
       aes(x = mean_aroma, y = mean_appearance, color = beer_style, size = mean_overall)) + 
  geom_point() + xlim(4,4.2) + ylim(4,4.25) +  theme(legend.position=&amp;quot;none&amp;quot;) +
 # geom_errorbar(aes(ymin=mean_appearance-sem_appearance, ymax=mean_appearance+sem_appearance), width=.1) +
#  geom_errorbarh(aes(xmin=mean_aroma-sem_aroma, xmax=mean_aroma+sem_aroma)) + theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(title = &amp;quot;Mean Appearance and Aroma&amp;quot;, y = &amp;quot;Mean Aroma&amp;quot;, x = &amp;quot;Mean Appearance&amp;quot;) + 
  geom_text(aes(label=beer_style, hjust = .5, vjust = -.75)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-29-hire-me-as-a-data-scientist-part-iv_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at this subsection, it appears we have a few choices for beers that score highly on both Appearance and Aroma. The American Double / Imperial Stout looks like a good option as it scores higher on how it looks, but our Russian Imperial Stout has a higher Aroma score. We could start crunching more numbers here to find “the best” option, or at this point we could take off our data science hats and put our psychology ones back on (assuming that’s what we were wearing…) and run some double-blind experiments on ourselves to make sure that our data is actually lining up with something in the real world.&lt;/p&gt;
&lt;div id=&#34;reflections&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reflections&lt;/h2&gt;
&lt;p&gt;What started out as a fun weekend project actually turned into a really great learning experience. I’ve definitely put a few solid hours into this and have gotten a lot out of it. I learned that my friends actually know a TON about beer and data science, that &lt;a href=&#34;https://git-lfs.github.com/&#34;&gt;git-lfs&lt;/a&gt; is something I wish I would have known about earlier, and that I’m actually looking forward to doing more blogging in the future.&lt;/p&gt;
&lt;p&gt;I will probably have to go MIA for the next two weeks since my General Exams are coming up in early February, but I’m sure I will be back at it come late March. Until then, all I can hope for is that someone who is looking to hire a junior data scientist over the summer will come across these posts and think I might be a good temporary addition to their team.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hire Me (as a Data Scientist!), Part III</title>
      <link>/post/hire-me-as-a-datascientist-part-iii/</link>
      <pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/hire-me-as-a-datascientist-part-iii/</guid>
      <description>&lt;p&gt;Two questions down, two to go! For the third post I’ll explore the question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Which of the factors (aroma, taste, appearance, palate) are most important in determining the overall quality of a beer?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Whereas the posts before were questions on sorting data, this is our first attempt to make some statistical models. In this case, we’re going to be doing a bit of regression modeling.&lt;/p&gt;
&lt;p&gt;There are a couple of ways to tackle this problem. We could run some basic linear regression models and spend the whole post talking beta coefficients and what assumptions we violated. We could do a linear mixed effects model, then realize that doing so would be a terrible choice (I tried it for the fun of it, bad idea). Or we do a fun non-parametric, machine learning model that is on the easier-to-interpret side. Since machine learning is so hot right now, let’s stick with that.&lt;/p&gt;
&lt;div id=&#34;machine-learning-and-non-parametric-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Machine Learning and Non-Parametric Models&lt;/h2&gt;
&lt;p&gt;Non-parametric models make no assumptions about the data. The models do not assume that our points come from beautiful, normally distributed ideal populations; they just seek to find some way to give us a good rule of thumb about what is happening under the hood.&lt;/p&gt;
&lt;p&gt;In this case, we need to make a model to predict the quality of beer (our dependent variable, &lt;code&gt;review_overall&lt;/code&gt;) based on four different independent variables (&lt;code&gt;review_aroma&lt;/code&gt;, &lt;code&gt;review_taste&lt;/code&gt;, &lt;code&gt;review_appearance&lt;/code&gt;, &lt;code&gt;review_palate&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;After discussing the pros and cons of certain methods for tackling this problem, my friend &lt;a href=&#34;https://www.linkedin.com/in/tabitha-trahan-172471b4/&#34;&gt;Tabi&lt;/a&gt;, the data scientist over at &lt;a href=&#34;https://www.soundout.com/&#34;&gt;Soundout&lt;/a&gt;, suggested that an easy way to get the answer I was looking for was to use a random forest model. Some great explanations about how these models work can be found &lt;a href=&#34;http://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://www-bcf.usc.edu/~gareth/ISL/&#34;&gt;here&lt;/a&gt;, and &lt;a href=&#34;https://gormanalysis.com/random-forest-from-top-to-bottom/&#34;&gt;here&lt;/a&gt; and since this isn’t a post about how random forest models work, I’ll just note that basically the idea is that it is an algorithm that partitions your dataset into dimensions that help us either classify or predict observations in our dataset based on the variables you feed it. Relevant to our question: the ways in which the algorithm splits our dataset is going to help us figure out what are the important variables.&lt;/p&gt;
&lt;p&gt;Before running this model though, we need to talk about a small dependence problem in our dataset. In my earlier post, I talked about how we probably should not just run a model on the data &lt;em&gt;as is&lt;/em&gt;. Last time we found there were tons of NAs in our dataset, that not all beers were equally represented, and that not all reviewers were making even amounts of reviews. In addition to these problems, there was also the problem that some reviewers might rate generally higher or lower &lt;em&gt;all the time&lt;/em&gt;. Since we know things like this exist, we wanted to account for them.&lt;/p&gt;
&lt;p&gt;The simplest plan of action would be to try and make each of the points we want to model independent by averaging ratings across all beers so we don’t have cases where one person’s influence is spread across multiple beers. We also make sure to only use beers that have over 30 ratings as a quality check. Let’s index out the data we need!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;data.table&amp;#39; was built under R version 3.4.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer &amp;lt;- fread(&amp;quot;data/beer_reviews.csv&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
Read 69.3% of 1586614 rows
Read 1586614 rows and 13 (of 13) columns from 0.168 GB file in 00:00:03&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make READABLE unique beer ID
beer[, beer_name_unique := paste(brewery_name,beer_name, beer_style)]

# Only beers with over 30 reviews 
reliable.beers &amp;lt;- beer[, .(ReviewsBeerHas = .N), by = beer_name_unique][order(-ReviewsBeerHas)][ReviewsBeerHas &amp;gt; 30]

# Merge with Inner Join 
beer.reliable &amp;lt;- reliable.beers[beer, on = &amp;quot;beer_name_unique&amp;quot;, nomatch=0]

# Make independent ratings 
prediction.data &amp;lt;- beer.reliable[, .(AvgOverall = mean(review_overall),
         AvgAroma = mean(review_aroma),
         AvgTaste = mean(review_taste),
         AvgApp = mean(review_appearance),
         AvgPal = mean(review_palate)), by = beer_name_unique]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have some better data, let’s fit a random forest model. We are now attempting to predict the average overall rating from our four others measures. We’ll use the &lt;code&gt;randomForest&lt;/code&gt; package and make sure to ask it to include a measure of variable importance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(randomForest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;randomForest&amp;#39; was built under R version 3.4.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## randomForest 4.6-14&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Type rfNews() to see new features/changes/bug fixes.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;randomForest&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     margin&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(666) 
random.forrest.fit &amp;lt;- randomForest(AvgOverall ~ AvgAroma + AvgTaste + AvgApp + AvgPal, data = prediction.data, importance = TRUE)
random.forrest.fit$importance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             %IncMSE IncNodePurity
## AvgAroma 0.02441128      210.5021
## AvgTaste 0.09918310      316.1375
## AvgApp   0.01538637      194.8586
## AvgPal   0.05592918      278.5960&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The randomForest object gives us two different measures when it comes to variable importance. The first one, &lt;code&gt;%IncMSE&lt;/code&gt;, is a measure that tells us how much our model’s Mean Square Error (MSE) would change if we were to take that variable out of our model. Average taste here is trouncing the other variables in terms of importance. The second variable, &lt;code&gt;IncNodePurity&lt;/code&gt; gives us a measure of node purity from all of the trees that were used in creating our model. Here Taste again leads in terms of importance, but our Palate variable seems to be close behind. We see this visually below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;varImpPlot(random.forrest.fit, main = &amp;quot;Variable Importance Metrics&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-28-hire-me-as-a-datascientist-part-iii_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The result that Taste seems to be taking the cake here is to be expected. Using some of the more basic tools of statistics and data science, we can look at the correlation between our taste variable and our overall, and it’s quite high. Actually looking at our average ratings, all of the correlations are really high!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(prediction.data[,-1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            AvgOverall  AvgAroma  AvgTaste    AvgApp    AvgPal
## AvgOverall  1.0000000 0.8811997 0.9518908 0.8432381 0.9360091
## AvgAroma    0.8811997 1.0000000 0.9591004 0.8954413 0.9365741
## AvgTaste    0.9518908 0.9591004 1.0000000 0.8949545 0.9742687
## AvgApp      0.8432381 0.8954413 0.8949545 1.0000000 0.9120654
## AvgPal      0.9360091 0.9365741 0.9742687 0.9120654 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Doing the analysis with this data presents some strange issues of collinearity. Having an &lt;em&gt;r&lt;/em&gt; = .951 for our Overall and Taste variables is obnoxiously high. An &lt;em&gt;r&lt;/em&gt; = .974 for Palette and Taste is also strangely large. If you come from more of a psychology background, you would almost never see correlations this high in the wild; it would be an immediate sign for concern.&lt;/p&gt;
&lt;p&gt;The correlations go down a bit if you end up looking at the non-aggregated sets too (see below), but again remember that these values have those dependence and outlier issues with them. Still, &lt;code&gt;review_taste&lt;/code&gt; and &lt;code&gt;review_overall&lt;/code&gt; are the highest correlated variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(beer[, .(review_overall,review_aroma,review_appearance,review_palate,review_taste)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   review_overall review_aroma review_appearance
## review_overall         1.0000000    0.6160131         0.5017324
## review_aroma           0.6160131    1.0000000         0.5610290
## review_appearance      0.5017324    0.5610290         1.0000000
## review_palate          0.7019139    0.6169469         0.5666339
## review_taste           0.7898156    0.7167761         0.5469804
##                   review_palate review_taste
## review_overall        0.7019139    0.7898156
## review_aroma          0.6169469    0.7167761
## review_appearance     0.5666339    0.5469804
## review_palate         1.0000000    0.7341351
## review_taste          0.7341351    1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This effect is probably due to the fact that higher quality beers tend to score higher on everything. It would be pretty strange in practice to give a beer a 5/5 overall, but then think it’s deserving of a 2/5 in Taste. If I were on a team at Beer Advocate, I might suggest incorporating either a larger range of ratings (maybe a seven point scale) or maybe thinking about different dimensions to ask people to rate like ‘hoppiness’ or bang-for-your-buck. Variables like these would allow us to learn more about the beers since they would have less collinearity issues.&lt;/p&gt;
&lt;p&gt;So the take-home here is that the taste variable is the most important variable for determining the overall rating and again we’re reminded about how messy this data is!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hire Me (as a Data Scientist!), Part II</title>
      <link>/post/hire-me-as-a-data-scientist-part-ii/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/hire-me-as-a-data-scientist-part-ii/</guid>
      <description>&lt;p&gt;Continuing on from my earlier post, I’m now looking to tackle the question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you had to pick 3 beers to recommend using only this data, which would you pick?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a pretty open ended question, which is kind of fun.
I also don’t really have a ton of experience (yet!) in recommendation systems, though I have done a little reading here or there on it.&lt;/p&gt;
&lt;p&gt;My goals in coming up with three beers to recommend were to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Try to find the most popular beer among super users of the website&lt;/li&gt;
&lt;li&gt;Find a &lt;a href=&#34;https://youtu.be/IcjSDZNbOs0?t=31s&#34;&gt;bizzaro&lt;/a&gt; beer that matched the profile of my first beer, but lives in the &lt;a href=&#34;https://books.google.com/books?hl=en&amp;amp;lr=&amp;amp;id=DTeZAAAAQBAJ&amp;amp;oi=fnd&amp;amp;pg=PT6&amp;amp;dq=anderson+2006+long+tail&amp;amp;ots=MpaGpMbdfD&amp;amp;sig=25QPk_RCCNU2yFoo9nsU0hrt0sc#v=onepage&amp;amp;q=anderson%202006%20long%20tail&amp;amp;f=false&#34;&gt;long tail&lt;/a&gt; of the ratings distribution&lt;/li&gt;
&lt;li&gt;Find the best Beer sans Booze (Highest Rating with lowest ABV)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So let’s begin!
Here’s how I went about tackling this question.&lt;/p&gt;
&lt;div id=&#34;popular-with-super-users&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Popular with Super Users&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#=====================================================================================#
# Following suit of the last post... 
#=====================================================================================#
# Library
library(ggplot2)
library(data.table)
library(stringr)
#=====================================================================================#
beer &amp;lt;- fread(&amp;quot;data/beer_reviews.csv&amp;quot;)
beer.complete &amp;lt;- beer[complete.cases(beer)]
#=====================================================================================#&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having more experience in experimental settings, one of the first things I needed to get used to when I started working with non-psychology datasets was the lack of complete sets in what felt like almost everything.
Whereas in the &lt;a href=&#34;https://musiccog.lsu.edu/&#34;&gt;lab&lt;/a&gt; we spend lots of time trying to design balanced studies that hopefully don’t violate the litany of assumptions that classic null hypothesis significance testing demands, my first few attempts at analyzing large amounts of data made me realize it’s almost risible to think that you’re going to have even, independent data, ever.
This dataset is no different.&lt;/p&gt;
&lt;p&gt;Of all of the unique users on the site, most of them have done only a couple of reviews, but some have essentially made a job out of this.
Looking at the distribution of reviews, this is quite clear.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;review.counts &amp;lt;- beer[, .(.N), by = review_profilename][order(-N)]
review.counts &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        review_profilename    N
##     1:     northyorksammy 5817
##     2:      BuckeyeNation 4661
##     3:        mikesgroove 4617
##     4:          Thorpe429 3518
##     5:      womencantsail 3497
##    ---                        
## 33384:          beilfussd    1
## 33385:         MPHSours11    1
## 33386:         jennaizzel    1
## 33387:           hogshead    1
## 33388:            joeebbs    1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(beer[, .(.N), by = review_profilename][order(-N)]$N, 
     breaks = 200,
     xlab = &amp;quot;Number of Reviews&amp;quot;,
     main = &amp;quot;Distribution of Reviews Per User&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-ii_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is pretty important when it comes to modeling the data (discussed in Part III), and not being fully aware of where your ratings are coming from could put the quality of your models at serious risk.&lt;/p&gt;
&lt;p&gt;So looking at this dataset, I wondered if there were any sort of implicit assumptions I could make about this data that might be able to help me find a good beer.
One assumption that I didn’t think was too wild was that a sample of this population that had gone out of its way to rate over 500 beers was probably more of a beer expert than those who have only done a couple of reviews on the site.&lt;/p&gt;
&lt;p&gt;One thing I wanted to check was: of all the 1.5 million reviews, where were they coming from? Were there enough reviews among the super users that I could use?
And what made someone a super user?
I could have been a bit more scientific, setting an &lt;em&gt;a priori&lt;/em&gt; threshold, but for this I kind of just looked at that chart above, spit balled thinking 500 might be a good number to check, and then went to see how much of the data would be accounted if I put my threshold there.
I lucked out and got about half of it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(review.counts$N) # Number of Total Reviews &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1586614&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(review.counts[ N &amp;gt; 500]$N) # Number of Reviews from Super Users&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 731066&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;731066/1586614 # Percent of Total Reviews from 500+ Super Users&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4607712&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.users &amp;lt;- review.counts[ N &amp;gt;  500] # I can settle for .75 Million Reviews&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now I had a list of the users who had completed over 500 reviews and made up 46% of our entire data.
I could use this new table I had made to index through our dataset of all the reviews that I had (that have their ABV ratings!) so I was then only dealing with these higher quality reviewers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews &amp;lt;- super.users[beer.complete, on = &amp;quot;review_profilename&amp;quot;, nomatch=0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As I continued to chop down the dataset (since this was a very exploratory process compared with cleaning up an experiment), it was important to do &lt;strong&gt;quality assessment&lt;/strong&gt; steps.
One thing worth checking here was to see if I was actually dealing with beer omnivores in our super users and make sure that all different types of beers were being represented in our smaller subset.
This was done by just looking at the number of rows between the original dataset and our super user table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews[, .(beer_styles = unique(beer_style))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                          beer_styles
##   1:                      Hefeweizen
##   2:              English Strong Ale
##   3:          Foreign / Export Stout
##   4:                 German Pilsener
##   5:  American Double / Imperial IPA
##  ---                                
## 100:             Japanese Rice Lager
## 101:                      Roggenbier
## 102:                        Happoshu
## 103:                           Sahti
## 104: Bière de Champagne / Bière Brut&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer[, .(beer_styles = unique(beer_style))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                          beer_styles
##   1:                      Hefeweizen
##   2:              English Strong Ale
##   3:          Foreign / Export Stout
##   4:                 German Pilsener
##   5:  American Double / Imperial IPA
##  ---                                
## 100:                          Gueuze
## 101:                            Gose
## 102:                        Happoshu
## 103:                           Sahti
## 104: Bière de Champagne / Bière Brut&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Luckily they were the same.
If I were to really do some more work on this dataset, I would also want to check things such as how many of the beers had each super user tried?
Were there IPA experts in the group?
If yes, should their opinions be taken more seriously if I had questions about IPA recommendations in the future?
But for now, I just set out to see what the highest rated beer among all the super users of this dataset was.&lt;/p&gt;
&lt;p&gt;In order to answer that question, I had to find out which beer in specific had the highest mean rating.
The dataset ‘as is’ comes with a &lt;code&gt;beer_id&lt;/code&gt; unique ID, but the data downloaded as is does not give us a key to this, so I had to make it myself.
This was accomplished by just pasting together the brewery’s name, along with the beer name, and style into a new variable.&lt;/p&gt;
&lt;p&gt;As another &lt;strong&gt;quality assurance&lt;/strong&gt; step, it was worth checking to see if this recreated the unique ID variable, which it didn’t do exactly… but it was pretty close.
I would chalk that up to some sort of encoding error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews[, beer_name_unique := paste(brewery_name,beer_name, beer_style)]

length(unique(super.reviews$beer_beerid))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 42825&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(unique(super.reviews$beer_name_unique))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 42703&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;42703/42805 # Pretty close&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9976171&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews.popular &amp;lt;- super.reviews[, .(most_reviewed_beers = .N), 
                                       by = beer_name_unique][order(-most_reviewed_beers)]

hist(super.reviews.popular$most_reviewed_beers,
     main = &amp;quot;Distribution of Number of Ratings by Super Users&amp;quot;,
     xlab = &amp;quot;Number of Reviews each Beer Recieves&amp;quot;,
     breaks = 200)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-ii_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again I saw this was clearly &lt;strong&gt;not&lt;/strong&gt; anything resembling a repeated measures experiment and not all beers were rated equally.&lt;/p&gt;
&lt;p&gt;Continuing in the same fashion above, I just grabbed the top 100 beers of our super users and merged that on to our table from earlier that had all of the ratings from our super users.
Then from that table, I took the average of the overall rating and looked at our top ten beers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews.popular.100 &amp;lt;- super.reviews[, .(most_reviewed_beers = .N), by = beer_name_unique][order(-most_reviewed_beers)][1:100]

super.reviews.cream.of.crop &amp;lt;- super.reviews.popular.100[super.reviews, 
                                                         on = &amp;quot;beer_name_unique&amp;quot;, 
                                                         nomatch=0]

super.reviews.cream.of.crop[, .(mean_review_overall = mean(review_overall)), by = beer_name_unique][order(-mean_review_overall)][1:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                                                      beer_name_unique
##  1:                      Russian River Brewing Company Pliny The Elder American Double / Imperial IPA
##  2:                  Bayerische Staatsbrauerei Weihenstephan Weihenstephaner Hefeweissbier Hefeweizen
##  3:                              Tröegs Brewing Company Tröegs Nugget Nectar American Amber / Red Ale
##  4:                                 Ballast Point Brewing Company Sculpin India Pale Ale American IPA
##  5:                 Three Floyds Brewing Co. &amp;amp; Brewpub Dreadnaught IPA American Double / Imperial IPA
##  6: Founders Brewing Company Founders KBS (Kentucky Breakfast Stout) American Double / Imperial Stout
##  7:                                                 Bell&amp;#39;s Brewery, Inc. Two Hearted Ale American IPA
##  8:                            Bell&amp;#39;s Brewery, Inc. Bell&amp;#39;s Hopslam Ale American Double / Imperial IPA
##  9:                    Three Floyds Brewing Co. &amp;amp; Brewpub Alpha King Pale Ale American Pale Ale (APA)
## 10:                Founders Brewing Company Founders Breakfast Stout American Double / Imperial Stout
##     mean_review_overall
##  1:            4.536630
##  2:            4.535072
##  3:            4.449084
##  4:            4.443287
##  5:            4.367580
##  6:            4.366876
##  7:            4.353270
##  8:            4.349810
##  9:            4.346652
## 10:            4.334526&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we have our winner!
It’s &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/863/7971/&#34;&gt;Pliny The Elder&lt;/a&gt; from Russian River Brewing Company as my first beer recommendation!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bizzaro-beer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bizzaro Beer&lt;/h2&gt;
&lt;p&gt;Now Pliny The Elder seemed to be a pretty popular beer.
But what if I was trying to sketch out some ideas about what other beers I could recommend to beer lovers who like Pliny The Elder?
It needed to somewhat “look like” the target beer, but have way less reviews.&lt;/p&gt;
&lt;p&gt;Playing with some of the fringe data here, I wanted to be careful not to again pick a beer with only one or two ratings on it.
My rationale was coming from assuming there is some sort of true “population mean” for this beer and having a beer with too little reviews will not approximate the mean correctly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Make Unique Beer Label for Larger Dataset
beer[, beer_name_unique := paste(brewery_name,beer_name, beer_style)]

## Count Number of Reviews Each Beer Has  
number.of.reviews &amp;lt;- beer[, .(NumberOfReviews = .N), by = beer_name_unique][order(-NumberOfReviews)]
 
## Only get beers with over 30 reviews
reliable.beers.list &amp;lt;- number.of.reviews[ NumberOfReviews &amp;gt;= 30 ]

## Join that to our big &amp;#39;beer&amp;#39; dataset only matching beers with over 30 reviews
beer.reliable &amp;lt;- reliable.beers.list[beer, on = &amp;quot;beer_name_unique&amp;quot;, nomatch=0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With our dataset chiseled down to only ‘reliable’ beers, I needed to find a way to get some sort of profile of each of the beers.
While my first instinct was to do some sort of data reductive type thing like a PCA on our continuous variables and use distances from certain scores as metrics of similarity (&lt;a href=&#34;https://musiccog.lsu.edu/davidjohnbaker/papers/Baker_Trahan_Mullensiefen_ProceedingsPaper.pdf&#34;&gt;which I have done before&lt;/a&gt; and it ended up actually being the inspiration for a tool currently used by &lt;a href=&#34;https://www.soundout.com/brandmatch&#34;&gt;Soundout&lt;/a&gt;!), doing that on so few predictors seemed &lt;a href=&#34;https://www.urbandictionary.com/define.php?term=extra&#34;&gt;extra&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So instead, I figured why not just assume that there is some sort of wiggle room in my hastily made recommendation system and just match first on the overall review, then if there are some close contenders, look for matches on other metrics?&lt;/p&gt;
&lt;p&gt;The next bit of code creates a table of the metrics I am interested in, gets beers that have over 30 reviews, but less than 100, and also creates a vector so I can pull out all of the IPAs on my less reviewed beers table.
I then joined the tables for my candidates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # Get metrics used for distance calculations
beer.metrics &amp;lt;- beer.reliable[, .(mean_review_overall = mean(review_overall),
                                  mean_review_aroma = mean(review_overall),
                                   mean_review_appearance = mean(review_appearance),
                                   mean_review_palate = mean(review_palate),
                                   mean_reviw_taste = mean(review_taste),
                                   sd_review_overall = sd(review_overall),
                                   sd_review_aroma = sd(review_overall),
                                   sd_review_appearance = sd(review_appearance),
                                   sd_review_palate = sd(review_palate),
                                   sd_review_taste = sd(review_taste)),
                               by = beer_name_unique]

## Get only IPAs with less than 100 reviews
less.reviewed.beers &amp;lt;- number.of.reviews[NumberOfReviews &amp;lt;= 100 &amp;amp; NumberOfReviews &amp;gt;= 30]
## Make vector to help find IPAs
find.IPA &amp;lt;- str_detect(string = less.reviewed.beers$beer_name_unique, pattern = &amp;quot;Imperial IPA&amp;quot;)
bizzaro.candidates &amp;lt;- less.reviewed.beers[find.IPA]

#Create Table
bizzaro.candidates.metrics &amp;lt;- bizzaro.candidates[beer.metrics, on = &amp;quot;beer_name_unique&amp;quot;, nomatch=0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of these less reviewed beers, I now needed to find the one that was “closest” on the few dimensions I had to work with.
The simplest way to do this would be to just subtract our target beer (Pliny The Elder), from every other beer in our interested list, then check out the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get metrics for our target beer
rrbcpteadii.metrics &amp;lt;- beer[beer_name_unique == &amp;quot;Russian River Brewing Company Pliny The Elder American Double / Imperial IPA&amp;quot;,
                           .(mean_review_overall = mean(review_overall),
                             mean_review_aroma = mean(review_overall),
                             mean_review_appearance = mean(review_appearance),
                             mean_review_palate = mean(review_palate),
                             mean_reviw_taste = mean(review_taste),
                             sd_review_overall = sd(review_overall),
                             sd_review_aroma = sd(review_overall),
                             sd_review_appearance = sd(review_appearance),
                             sd_review_palate = sd(review_palate),
                             sd_review_taste = sd(review_taste))]
## Create vector for looping over
key.vector &amp;lt;- as.vector(rrbcpteadii.metrics)
## Pull off the tags of our search
search.vector &amp;lt;- bizzaro.candidates.metrics[, -c(1,2)]

## Sanity check that what we are going to subtract has same names
names(key.vector)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;mean_review_overall&amp;quot;    &amp;quot;mean_review_aroma&amp;quot;     
##  [3] &amp;quot;mean_review_appearance&amp;quot; &amp;quot;mean_review_palate&amp;quot;    
##  [5] &amp;quot;mean_reviw_taste&amp;quot;       &amp;quot;sd_review_overall&amp;quot;     
##  [7] &amp;quot;sd_review_aroma&amp;quot;        &amp;quot;sd_review_appearance&amp;quot;  
##  [9] &amp;quot;sd_review_palate&amp;quot;       &amp;quot;sd_review_taste&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(search.vector)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;mean_review_overall&amp;quot;    &amp;quot;mean_review_aroma&amp;quot;     
##  [3] &amp;quot;mean_review_appearance&amp;quot; &amp;quot;mean_review_palate&amp;quot;    
##  [5] &amp;quot;mean_reviw_taste&amp;quot;       &amp;quot;sd_review_overall&amp;quot;     
##  [7] &amp;quot;sd_review_aroma&amp;quot;        &amp;quot;sd_review_appearance&amp;quot;  
##  [9] &amp;quot;sd_review_palate&amp;quot;       &amp;quot;sd_review_taste&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## And that the apply function I am going to run is doing what I think it will
search.vector[1]- key.vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    mean_review_overall mean_review_aroma mean_review_appearance
## 1:          -0.7900277        -0.7900277             -0.6386031
##    mean_review_palate mean_reviw_taste sd_review_overall sd_review_aroma
## 1:         -0.6263257       -0.8393187        0.09103239      0.09103239
##    sd_review_appearance sd_review_palate sd_review_taste
## 1:            0.1273011          0.11882       0.1073594&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Run apply function
ipa.distances &amp;lt;- apply(search.vector, 1, function(x) x - key.vector)
ipa.distances.dt &amp;lt;- data.table(do.call(rbind.data.frame,ipa.distances))
## Combine this back with vector with names
bizzaro.candidates.distances &amp;lt;- cbind(bizzaro.candidates.metrics, ipa.distances.dt)
## Sort our data by overall and see if we have a good match!
bizzaro.candidates.distances[order(-mean_review_overall)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                                          beer_name_unique
##   1: Hill Farmstead Brewery Galaxy Imperial Single Hop IPA American Double / Imperial IPA
##   2:           Lawson&amp;#39;s Finest Liquids Double Sunshine IPA American Double / Imperial IPA
##   3:        Kern River Brewing Company 5th Anniversary Ale American Double / Imperial IPA
##   4:                           Alpine Beer Company Bad Boy American Double / Imperial IPA
##   5:             Iron Hill Brewery &amp;amp; Restaurant Kryptonite American Double / Imperial IPA
##  ---                                                                                     
## 132:                            BrewDog Sink The Bismarck! American Double / Imperial IPA
## 133:                   Blue Frog Grog &amp;amp; Grill The Big DIPA American Double / Imperial IPA
## 134:                            Hermitage Brewing Hoptopia American Double / Imperial IPA
## 135:                    Florida Beer Company Swamp Ape IPA American Double / Imperial IPA
## 136:            BrewDog Storm (Islay Whisky Cask Aged IPA) American Double / Imperial IPA
##      NumberOfReviews mean_review_overall mean_review_aroma
##   1:              76            4.592105          4.592105
##   2:              85            4.588235          4.588235
##   3:              41            4.475610          4.475610
##   4:              79            4.468354          4.468354
##   5:              44            4.443182          4.443182
##  ---                                                      
## 132:              76            3.197368          3.197368
## 133:              53            2.867925          2.867925
## 134:              32            2.687500          2.687500
## 135:              52            2.615385          2.615385
## 136:              92            2.440217          2.440217
##      mean_review_appearance mean_review_palate mean_reviw_taste
##   1:               4.368421           4.440789         4.559211
##   2:               4.317647           4.311765         4.552941
##   3:               4.329268           4.219512         4.500000
##   4:               4.234177           4.335443         4.474684
##   5:               4.284091           4.318182         4.420455
##  ---                                                           
## 132:               3.835526           3.401316         3.539474
## 133:               3.433962           3.047170         2.801887
## 134:               3.500000           2.890625         2.500000
## 135:               3.442308           3.009615         2.586538
## 136:               2.902174           2.836957         2.706522
##      sd_review_overall sd_review_aroma sd_review_appearance
##   1:         0.3337716       0.3337716            0.3861642
##   2:         0.3289275       0.3289275            0.3845766
##   3:         0.3865103       0.3865103            0.3641730
##   4:         0.3698733       0.3698733            0.3741267
##   5:         0.3768892       0.3768892            0.3796836
##  ---                                                       
## 132:         1.2438621       1.2438621            0.7136206
## 133:         0.8995241       0.8995241            0.6866707
## 134:         1.2427207       1.2427207            0.4918694
## 135:         0.9108033       0.9108033            0.5994593
## 136:         0.9829379       0.9829379            0.6843587
##      sd_review_palate sd_review_taste mean_review_overall
##   1:        0.3997258       0.3826844         0.002077562
##   2:        0.4293993       0.3620669        -0.001792407
##   3:        0.4749840       0.4873397        -0.114417945
##   4:        0.4060561       0.3660140        -0.121673270
##   5:        0.4586495       0.4026537        -0.146845883
##  ---                                                     
## 132:        1.1431528       1.1277209        -1.392659280
## 133:        0.7355280       0.8165336        -1.722103173
## 134:        0.8005479       0.9418581        -1.902527701
## 135:        0.7637009       1.0035288        -1.974643085
## 136:        0.7883377       1.0086226        -2.149810310
##      mean_review_aroma mean_review_appearance mean_review_palate
##   1:       0.002077562            -0.02018203        -0.01053621
##   2:      -0.001792407            -0.07095603        -0.13956098
##   3:      -0.114417945            -0.05933479        -0.23181349
##   4:      -0.121673270            -0.15442587        -0.11588264
##   5:      -0.146845883            -0.10451218        -0.13314386
##  ---                                                            
## 132:      -1.392659280            -0.55307677        -1.05000989
## 133:      -1.722103173            -0.95464082        -1.40415587
## 134:      -1.902527701            -0.88860309        -1.56070068
## 135:      -1.974643085            -0.94629539        -1.44171030
## 136:      -2.149810310            -1.48642917        -1.61436916
##      mean_reviw_taste sd_review_overall sd_review_aroma
##   1:      -0.07177483       -0.12136909     -0.12136909
##   2:      -0.07804418       -0.12621327     -0.12621327
##   3:      -0.13098536       -0.06863039     -0.06863039
##   4:      -0.15630181       -0.08526747     -0.08526747
##   5:      -0.21053081       -0.07825155     -0.07825155
##  ---                                                   
## 132:      -1.09151167        0.78872139      0.78872139
## 133:      -1.82909857        0.44438341      0.44438341
## 134:      -2.13098536        0.78758001      0.78758001
## 135:      -2.04444690        0.45566254      0.45566254
## 136:      -1.92446362        0.52779720      0.52779720
##      sd_review_appearance sd_review_palate sd_review_taste
##   1:          -0.01935577      -0.04762635     -0.03307969
##   2:          -0.02094339      -0.01795284     -0.05369722
##   3:          -0.04134702       0.02763182      0.07157561
##   4:          -0.03139329      -0.04129607     -0.04975012
##   5:          -0.02583641       0.01129741     -0.01311039
##  ---                                                      
## 132:           0.30810063       0.69580063      0.71195677
## 133:           0.28115074       0.28817587      0.40076950
## 134:           0.08634938       0.35319581      0.52609404
## 135:           0.19393929       0.31634876      0.58776473
## 136:           0.27883874       0.34098558      0.59285853&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course if I were building a real recommendation machine I could start talking about what factors are more important for what users and what factors are more predictive than others, but this seems like an OK enough solution to at least have completed my &lt;em&gt;a priori&lt;/em&gt; goal.&lt;/p&gt;
&lt;p&gt;Based on this solution, it looks like I will have to find myself a bottle of &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/863/7971/&#34;&gt;Pliny The Elder&lt;/a&gt; and the &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/22511/67760/&#34;&gt;Hill Farmstead Brewery Galaxy Imperial Single Hop IPA American Double / Imperial IPA&lt;/a&gt; and do some of my own empirical work to see if this was a good idea.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;beer-sans-booze&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beer sans Booze&lt;/h2&gt;
&lt;p&gt;The last beer that I think I wanted to recommend would be one that tastes great, but does not have a lot of alcohol in it.
The reason this question kind of interests me is because if we are &lt;em&gt;really&lt;/em&gt; going to talk about how tasty a beer is, it would be nice to be able to factor out of the equation how drunk we are actually getting from it.&lt;/p&gt;
&lt;p&gt;I can see first of all IF this relationship exists if we look at the mean overall rating of a beer as a function of its ABV content.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer.complete[, beer_name_unique := paste(brewery_name,beer_name, beer_style) ]

# ABVs and Mean Scores
abv.vs.mean &amp;lt;- beer.complete[, .(Abv = mean(beer_abv), MeanOverall = mean(review_overall)), by = beer_name_unique]

ggplot(abv.vs.mean[Abv &amp;lt; 20], aes(x = Abv, y = MeanOverall)) + 
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;, color = &amp;quot;blue&amp;quot;) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, formula = y ~ poly(x,2), color = &amp;quot;orange&amp;quot;) +
  labs(title = &amp;quot;Rating as Function of ABV (Beers with than 20% ABV)&amp;quot;,
       x = &amp;quot;ABV Content&amp;quot;,
       y = &amp;quot;Mean Overall Rating&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-ii_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Surprisingly, when I ran some quick and dirty regression models (that yes, I know violate tons of assumptions) I saw that only a very small amount of variance was being explained by its ABV.
Note that although the models were significant, the R squared values hovered around 3-5%!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# &amp;quot;The More Booze The Better&amp;quot; Model
abv.linear &amp;lt;- lm(MeanOverall ~ Abv, data = abv.vs.mean[Abv &amp;lt; 20]) 
# The &amp;quot;Diminishing Returns Model &amp;quot;
abv.poly &amp;lt;- lm(MeanOverall ~ poly(Abv,2), data = abv.vs.mean[Abv &amp;lt; 20])
# The &amp;quot;Dissapointing Amount of Variance Explained Summaries&amp;quot;
summary(abv.linear)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = MeanOverall ~ Abv, data = abv.vs.mean[Abv &amp;lt; 20])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.06793 -0.27278  0.09684  0.38850  1.72081 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 3.276138   0.008995  364.20   &amp;lt;2e-16 ***
## Abv         0.060975   0.001369   44.55   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.6013 on 48822 degrees of freedom
## Multiple R-squared:  0.03906,    Adjusted R-squared:  0.03904 
## F-statistic:  1984 on 1 and 48822 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(abv.poly)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = MeanOverall ~ poly(Abv, 2), data = abv.vs.mean[Abv &amp;lt; 
##     20])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.88738 -0.28351  0.09695  0.37907  2.17585 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)     3.658087   0.002706 1351.73   &amp;lt;2e-16 ***
## poly(Abv, 2)1  26.785156   0.597970   44.79   &amp;lt;2e-16 ***
## poly(Abv, 2)2 -13.922969   0.597970  -23.28   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.598 on 48821 degrees of freedom
## Multiple R-squared:  0.04961,    Adjusted R-squared:  0.04957 
## F-statistic:  1274 on 2 and 48821 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This actually surprised me and might be worth looking into at a deeper level another time, but for now I want to keep going on and find a beer knowing that how much booze is in it doesn’t really affect how good it is.&lt;/p&gt;
&lt;p&gt;So let’s take one final dive into the dataset, grab only our quality reviews then plot a subset of our data so I can see beers that have a very high overall rating with a very small amount of booze in them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Quality Assurance Step
reliable.and.abv.beers &amp;lt;- reliable.beers.list[beer.complete, on = &amp;quot;beer_name_unique&amp;quot;, nomatch=0]

## Get mean ratings and keep ABV (which won&amp;#39;t change if I average it)
dd.beers &amp;lt;- reliable.and.abv.beers[, .(mean_overall = mean(review_overall), abv = mean(beer_abv)), by = &amp;quot;beer_name_unique&amp;quot;]

# Only Beers that Fit Our Criterion
dd.beers.2 &amp;lt;- dd.beers[mean_overall &amp;gt; 4.6 &amp;amp; abv &amp;lt; 10]

# Plot It!
ggplot(dd.beers.2, aes(x = abv, y = mean_overall, label = beer_name_unique, color = beer_name_unique)) +
  geom_point() +
  geom_text(aes(label=beer_name_unique),hjust=-.01, vjust=0) +
  labs(title = &amp;quot;High Quality Beers with Low ABV&amp;quot;,
       x = &amp;quot;ABV&amp;quot;,
       y = &amp;quot;Mean Overall Rating&amp;quot;) + theme(legend.position = &amp;quot;none&amp;quot;) +
  xlim(0, 20) +
  scale_y_continuous(breaks = c(seq(4.6,5,.1)), limits = c(4.6,4.85))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-ii_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These are all OK choices (most of the beers are still above 5% ABV), but we do have one beer clocking in at 2.0% ABV giving us our final beer recommendation – the &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/1628/8626/&#34;&gt;Southampton Publick House Southampton Berliner Weisse Berliner Weissbier&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;After all of this, I know have three beers to check out.
&lt;a href=&#34;https://www.beeradvocate.com/beer/profile/863/7971/&#34;&gt;Pliny The Elder&lt;/a&gt; is our winner for the top rated beer among our Super Users of the site, the &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/22511/67760/&#34;&gt;Hill Farmstead Brewery Galaxy Imperial Single Hop IPA American Double / Imperial IPA&lt;/a&gt; is a beer to maybe follow up on from our first choice, and then lastly we have the the &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/1628/8626/&#34;&gt;Southampton Publick House Southampton Berliner Weisse Berliner Weissbier&lt;/a&gt; which supposedly tastes great, despite its lack of alcohol content.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hire Me (as a Data Scientist!), Part I</title>
      <link>/post/hire-me-as-a-data-scientist-part-i/</link>
      <pubDate>Sat, 13 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/hire-me-as-a-data-scientist-part-i/</guid>
      <description>&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;I read &lt;a href=&#34;https://medium.com/&#34;&gt;Medium&lt;/a&gt; blog posts on “How to Become a Data Scientist” more often than I care to admit.
Much of this comes from a fear that after doing all this work on the PhD and then hitting the Music Theory job market, I won’t fit the mold of the kind of theorist most schools want to hire.
Not coming from &lt;a href=&#34;https://pushpullfork.com/music-theory-job-market/&#34;&gt;one of five schools that seem to have a monopoly on the tenure track jobs&lt;/a&gt; can be a bit discouraging, but I also won’t deny that having a non-academic job with a regular 9-5 schedule and a &lt;a href=&#34;https://www.glassdoor.com/Salaries/data-scientist-salary-SRCH_KO0,14.htm&#34;&gt;decent salary&lt;/a&gt; is pretty tempting after spending the vast majority of my twenties in school.
And even if I don’t go on over to industry after the PhD, I’ll probably always be looking for a bit more work in summer.&lt;/p&gt;
&lt;p&gt;On top of all of that, I believe that skills that are acquired in a PhD (especially if you do computational musicology and music cognition!) are very transferable to most jobs, and it’s just a matter of being a bit more pro-active in promoting myself that might help me one day land a stable, non-academic job.&lt;/p&gt;
&lt;p&gt;That said, one tweet I saw last week by &lt;a href=&#34;https://twitter.com/kierisi&#34;&gt;Jesse Meagan&lt;/a&gt; linked to this really interesting Linked-In
post by &lt;a href=&#34;https://twitter.com/tanyacash21&#34;&gt;Tanya Cashorali&lt;/a&gt; that purported to have a &lt;a href=&#34;https://www.linkedin.com/pulse/how-hire-test-data-skills-one-size-fits-all-interview-tanya-cashorali/&#34;&gt;one size fits all data-science interview process&lt;/a&gt; which has candidates take home a big dataset with a bunch of beer reviews and answer four very broad questions.
Considering myself an aficionado of How-To-Become-a-Data Scientist articles, this of course caught my eye.&lt;/p&gt;
&lt;p&gt;After reading the article, I figured why not give it a go?
It’s the start of the semester, I’m basically ABD, need more of a portfolio beyond &lt;a href=&#34;www.github.com/davidjohnbaker1&#34;&gt;my github&lt;/a&gt;, and I have nothing to do with my Saturday morning.
Why not see what I can produce in 4 or 5 hours?
At the very least I’ll hopefully just have something to point to if a future employer wants to see how I think through data-science problems.&lt;/p&gt;
&lt;p&gt;And if anyone is reading this that does have comments on my code or thought process… please let me know what you think on &lt;a href=&#34;www.twitter.com/DavidJohnBaker&#34;&gt;Twitter&lt;/a&gt;!
I’d love some feedback!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-the-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring the Dataset&lt;/h2&gt;
&lt;p&gt;The first thing I did was to grab this dataset which you can get &lt;a href=&#34;https://s3.amazonaws.com/demo-datasets/beer_reviews.tar.gz?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3BDqDjqRycTlq5seB4xN3ocA%3D%3D&#34;&gt;here&lt;/a&gt; and then I set up my R script with a few of my favorite packages (again, big love to Ben at &lt;a href=&#34;https://gormanalysis.com/&#34;&gt;GormAnalysis&lt;/a&gt; for helping me learn &lt;a href=&#34;https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html&#34;&gt;data.table&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#=====================================================================================#
# Beer Script
#=====================================================================================#
# Library
library(ggplot2)
library(data.table)
library(stringr)
#=====================================================================================#
beer &amp;lt;- fread(&amp;quot;data/beer_reviews.csv&amp;quot;)
#=====================================================================================#&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset has about 1.5 million observations across 14 different observations, so don’t try to open it in LibreOffice.
The reviews come from a variety of different users that have rated the beers based on five different attributes (Appearance, Palate, Aroma, Taste, Overall) and then each beer has a few other variables listed such as its ABV, the brewery it comes from, the beer’s name (duh), and what kind of beer it is.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(beer)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;brewery_id&amp;quot;         &amp;quot;brewery_name&amp;quot;       &amp;quot;review_time&amp;quot;       
##  [4] &amp;quot;review_overall&amp;quot;     &amp;quot;review_aroma&amp;quot;       &amp;quot;review_appearance&amp;quot; 
##  [7] &amp;quot;review_profilename&amp;quot; &amp;quot;beer_style&amp;quot;         &amp;quot;review_palate&amp;quot;     
## [10] &amp;quot;review_taste&amp;quot;       &amp;quot;beer_name&amp;quot;          &amp;quot;beer_abv&amp;quot;          
## [13] &amp;quot;beer_beerid&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer[, .(Number = unique(beer$brewery_name))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                             Number
##    1:              Vecchio Birraio
##    2:      Caldera Brewing Company
##    3:       Amstel Brouwerij B. V.
##    4:        Broad Ripple Brew Pub
##    5:   Moon River Brewing Company
##   ---                             
## 5739:        Gattopardo Cervejaria
## 5740:         Brauerei Lasser GmbH
## 5741:        Wissey Valley Brewery
## 5742:      Outback Brewery Pty Ltd
## 5743: Georg Meinel Bierbrauerei KG&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer[, .(Number = unique(beer$review_profilename))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Number
##     1:        stcules
##     2: johnmichaelsen
##     3:        oline73
##     4:      Reidrover
##     5:   alpinebryant
##    ---               
## 33384:     jennaizzel
## 33385:    mine2design
## 33386:       hogshead
## 33387:     NyackNicky
## 33388:        joeebbs&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer[, .(Number = unique(beer$beer_name))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                        Number
##     1:           Sausa Weizen
##     2:               Red Moon
##     3: Black Horse Black Beer
##     4:             Sausa Pils
##     5:          Cauldron DIPA
##    ---                       
## 56853:      Bear Mountain Ale
## 56854:        Highland Porter
## 56855:       Baron Von Weizen
## 56856:          Resolution #2
## 56857:     The Horseman&amp;#39;s Ale&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer[, .(Number = unique(beer$beer_style))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                               Number
##   1:                      Hefeweizen
##   2:              English Strong Ale
##   3:          Foreign / Export Stout
##   4:                 German Pilsener
##   5:  American Double / Imperial IPA
##  ---                                
## 100:                          Gueuze
## 101:                            Gose
## 102:                        Happoshu
## 103:                           Sahti
## 104: Bière de Champagne / Bière Brut&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From a bird’s eye view we have 56,857 unique beers in 104 different categories from 5,743 different breweries and 33,388 unique beer aficionados who have gone out of their way to tell this website what they think about the beers they drink.&lt;/p&gt;
&lt;p&gt;Before diving in further, it’s worth doing a preliminary check of the quality of the data (aka we should know if this is BAD (Best Available Data) or has undergone a fair deal of cleaning).
As someone who comes from more of a psychology background, I’ve noticed what certain people consider “clean” when it comes to data varies a lot.&lt;/p&gt;
&lt;p&gt;The first thing I check for is if there is any kind of data missing and if there is, is it due to chance?
Or is it due to some sort of systematic variation?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(complete.cases(beer))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   FALSE    TRUE 
##   67785 1518829&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;67785/1518829&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04462978&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So about 4% of our rows don’t have every entry, so probably not too much cause for concern unless we start getting into specific questions about specific beers.
Looking into this a bit further it seems like it’s just beers missing the ABV of the beer.
Anyone who has made some beer ratings has made ratings on all five variables.
And although it’s only 4% of our entire ratings that don’t have their ABV, comparing that to every beer we have, we see we are actually missing ~25% of the ABV ratings of all of our beers.
That could be a problem later, but it’s good to know about it sooner rather than later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer.complete &amp;lt;- beer[complete.cases(beer)]

beer[!complete.cases(beer)][, .(.N = unique(beer_name))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                             .N
##     1: Cauldron Espresso Stout
##     2:    The Highland Stagger
##     3:              Alpha Beta
##     4:          Imperial Stout
##     5:               Megalodon
##    ---                        
## 14106:       English Nut Brown
## 14107:              Hop Common
## 14108:     Very Hoppy Pale Ale
## 14109:       Prohibition Lager
## 14110:           Resolution #2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;14110/56857&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2481665&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer[969]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    brewery_id             brewery_name review_time review_overall
## 1:      12770 City Grille and Brewhaus  1145738954              4
##    review_aroma review_appearance review_profilename
## 1:            3                 4         UncleJimbo
##                 beer_style review_palate review_taste     beer_name
## 1: American Pale Ale (APA)           3.5            4 City Pale Ale
##    beer_abv beer_beerid
## 1:       NA       30088&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More problems might come up here or there, but let’s move on the first question.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;which-brewery-produces-the-strongest-beers-by-abv&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Which brewery produces the strongest beers by ABV%?&lt;/h2&gt;
&lt;p&gt;Answering the first question on the list is pretty straight forward.
Essentially all you need to do is grab all of the observations that have an ABV associated with their rating, and then
get the average ABV of all the beers that that brewery produces.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Use object before that has only ratings with ABVs 
beer.complete[, .(AvgABV = mean(beer_abv)), by = brewery_name]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       brewery_name   AvgABV
##    1:              Vecchio Birraio 5.675000
##    2:      Caldera Brewing Company 6.168849
##    3:       Amstel Brouwerij B. V. 3.816373
##    4:        Broad Ripple Brew Pub 6.006202
##    5:   Moon River Brewing Company 5.724103
##   ---                                      
## 5152:        Gattopardo Cervejaria 6.033333
## 5153:         Brauerei Lasser GmbH 5.200000
## 5154:        Wissey Valley Brewery 5.133333
## 5155:      Outback Brewery Pty Ltd 4.787879
## 5156: Georg Meinel Bierbrauerei KG 5.850000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create table that has means and standard deviations of beers by brewery 
# Order them from most to least
abv.counter &amp;lt;- beer.complete[, .(AvgABV = mean(beer_abv), 
                                 SdABV = sd(beer_abv)) , 
                             by = brewery_name][order(-AvgABV)]
abv.counter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                              brewery_name     AvgABV      SdABV
##    1:                        Schorschbräu 19.2288235 12.3273042
##    2:                       Shoes Brewery 15.2000000  0.0000000
##    3:                Rome Brewing Company 13.8400000  1.9718012
##    4:                   Hurlimann Brewery 13.7500000  0.5752237
##    5:            Alt-Oberurseler Brauhaus 13.2000000         NA
##   ---                                                          
## 5152:             Cerveceria Vegana, S.A.  2.2608696  2.2455490
## 5153: Moskovskaya Pivovarennaya Kompaniya  2.1500000  1.6881943
## 5154:                      Fentimans Ltd.  1.3750000  1.6201852
## 5155:                        Borodino ZAO  0.9666667  0.4041452
## 5156:                    All Stars Bakery  0.5000000  0.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having this table could be good enough for government work, but looking at the output there are clearly problems, and one thing to consider in this table (and pretty much this whole dataset) is “Is this data point a good representation of what I am trying to measure?”.
Note for example the huge variability as measured by the standard deviation in our top answer as well as the fact that some of the SDs have NAs and there is a value of 0.
Given that, I think it’d be good to put on some sort of threshold that would up the quality of our answers.
One way to do this would be to see exactly how many beers each brewery makes and use that as a proxy for how big the brewery is.&lt;/p&gt;
&lt;p&gt;The code below does just that and reveals the variability in terms of size of breweries within this dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create table that counts number of beers 
NoOfBeers &amp;lt;- beer.complete[, .(NameOfBeer = unique(beer_name)), 
                           by = brewery_name][, .(.N), by = brewery_name]
NoOfBeers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       brewery_name  N
##    1:              Vecchio Birraio  4
##    2:      Caldera Brewing Company 25
##    3:       Amstel Brouwerij B. V.  9
##    4:        Broad Ripple Brew Pub 40
##    5:   Moon River Brewing Company 34
##   ---                                
## 5152:        Gattopardo Cervejaria  3
## 5153:         Brauerei Lasser GmbH  1
## 5154:        Wissey Valley Brewery  3
## 5155:      Outback Brewery Pty Ltd  6
## 5156: Georg Meinel Bierbrauerei KG  2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make table that lists each beer with it&amp;#39;s ABV and the name of the brewery
abv.table &amp;lt;- NoOfBeers[abv.counter, on = &amp;quot;brewery_name&amp;quot;]
abv.table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                              brewery_name  N     AvgABV      SdABV
##    1:                        Schorschbräu 10 19.2288235 12.3273042
##    2:                       Shoes Brewery  1 15.2000000  0.0000000
##    3:                Rome Brewing Company  2 13.8400000  1.9718012
##    4:                   Hurlimann Brewery  3 13.7500000  0.5752237
##    5:            Alt-Oberurseler Brauhaus  1 13.2000000         NA
##   ---                                                             
## 5152:             Cerveceria Vegana, S.A.  2  2.2608696  2.2455490
## 5153: Moskovskaya Pivovarennaya Kompaniya  2  2.1500000  1.6881943
## 5154:                      Fentimans Ltd.  3  1.3750000  1.6201852
## 5155:                        Borodino ZAO  2  0.9666667  0.4041452
## 5156:                    All Stars Bakery  1  0.5000000  0.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create z scores 
abv.table[, zAvgABV := scale(AvgABV)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After visually inspecting the graph on the size of breweries (below), I figured I could just look at breweries that make over five beers (which hopefully wipes out your hipster friend’s “micro brewery” in his basement where he is just trying to make the most potent IPA ever) and then only look at beers that score 4 standard deviations above the mean of all beers in terms of ABV content to narrow down possible candidates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# How many beers to count for a big brewery? 
hist(NoOfBeers$N, breaks= 200, 
     main = &amp;quot;Distribution of Size of Breweries&amp;quot;, 
     xlab = &amp;quot;Number of Beers Produced by a Brewery&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-i_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NoOfBeers[N &amp;gt; 200] # Clearly some big breweries here! &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                        brewery_name   N
## 1:    Minneapolis Town Hall Brewery 243
## 2:            Goose Island Beer Co. 304
## 3:   Iron Hill Brewery &amp;amp; Restaurant 269
## 4: Rock Bottom Restaurant &amp;amp; Brewery 522&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abv.table[N &amp;gt;= 5, ][order(-AvgABV)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                              brewery_name   N    AvgABV
##    1:                                        Schorschbräu  10 19.228824
##    2: Brasserie Grain d&amp;#39; Orge (Brasserie Jeanne d&amp;#39;Arc SA)  10 12.445860
##    3:                          Brauerei Schloss Eggenberg  14 11.779681
##    4:                     Brasserie Dubuisson Frères sprl  14 11.432746
##    5:                            Kuhnhenn Brewing Company 142 11.345839
##   ---                                                                  
## 2539:                             Berliner Kindl Brauerei  12  3.532627
## 2540:  Yanjing Pijiu (Guilin Liquan) Gufen Youxian Gongsi   5  3.440000
## 2541:                                            Ochakovo  16  3.203150
## 2542:                        Grogg&amp;#39;s Pinnacle Brewing Co.   6  3.200000
## 2543:                                        Deka Brewery   7  2.620000
##            SdABV   zAvgABV
##    1: 12.3273042 10.235332
##    2:  1.7054879  5.048898
##    3:  3.0759353  4.539520
##    4:  1.6583471  4.274244
##    5:  3.5788003  4.207793
##   ---                     
## 2539:  1.0372607 -1.766396
## 2540:  0.6426508 -1.837221
## 2541:  1.2735986 -2.018323
## 2542:  0.0000000 -2.020731
## 2543:  1.6356553 -2.464215&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(abv.table[N &amp;gt;= 5, ][order(-AvgABV)]$zAvgABV,
     xlab = &amp;quot;z Score of ABV&amp;quot;, 
     main = &amp;quot;Distribution of ABV in Breweries that make more than 5 Beers&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-i_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abv.table[N &amp;gt;= 5 &amp;amp; zAvgABV &amp;gt; 4, ][order(-AvgABV)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                           brewery_name   N   AvgABV
## 1:                                        Schorschbräu  10 19.22882
## 2: Brasserie Grain d&amp;#39; Orge (Brasserie Jeanne d&amp;#39;Arc SA)  10 12.44586
## 3:                          Brauerei Schloss Eggenberg  14 11.77968
## 4:                     Brasserie Dubuisson Frères sprl  14 11.43275
## 5:                            Kuhnhenn Brewing Company 142 11.34584
##        SdABV   zAvgABV
## 1: 12.327304 10.235332
## 2:  1.705488  5.048898
## 3:  3.075935  4.539520
## 4:  1.658347  4.274244
## 5:  3.578800  4.207793&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Doing it this way puts Schorschbräu as the highest ABV brewery, which makes sense because &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/6513/51466/?ba=wordemupg&#34;&gt;they claim to make the world’s strongest beer&lt;/a&gt;.
Making a quick plot of the data for our winner and the second place finisher, we see how strong Schorschbräu really is.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;schor.abv &amp;lt;- beer.complete[brewery_name == &amp;quot;Schorschbräu&amp;quot;, 
                           .(beer_name = unique(beer_name)), by = beer_abv]

ggplot(schor.abv, aes(x = beer_name, y = beer_abv)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;)  + 
  labs( title = &amp;quot;Schorschbräu Beer ABV&amp;quot;, x = &amp;quot;Beer Name&amp;quot;, y = &amp;quot;ABV&amp;quot;) +
   theme(axis.text.x=element_text(angle = -90, hjust = 0)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-i_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;brassOrg.abv &amp;lt;- beer.complete[brewery_name == &amp;quot;Brasserie Grain d&amp;#39; Orge (Brasserie Jeanne d&amp;#39;Arc SA)&amp;quot;, 
                              .(beer_name = unique(beer_name)), by = beer_abv]

ggplot(brassOrg.abv, aes(x = beer_name, y = beer_abv)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;)  + 
  labs( title = &amp;quot;Brasserie Grain d&amp;#39; Orge Beer ABV&amp;quot;, 
        x = &amp;quot;Beer Name&amp;quot;, 
        y = &amp;quot;ABV&amp;quot;) +
   theme(axis.text.x=element_text(angle = -90, hjust = 0)) + ylim(0, 60) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-i_files/figure-html/unnamed-chunk-8-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I think that saying Schorschbräu is technically correct here, but after sharing my findings with my one beer drinking friend
he pointed out that one thing that this analysis did not take into account was that beers
that are traditionally brewed to have a higher ABV (like IPAs and Belgiums) might skew my results.
So if you are a big IPA brewery, you are going to have higher average ABV because of the beers you decide to brew!&lt;/p&gt;
&lt;p&gt;So in the true spirit of that &lt;a href=&#34;http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram&#34;&gt;data science Venn diagram&lt;/a&gt; noting that data scientists need to be flexible in
incorporating others’ domain knowledge, I did another analysis to just show how much an answer can change depending on how you change your operationalization of the question!&lt;/p&gt;
&lt;p&gt;Let’s do another one!&lt;/p&gt;
&lt;p&gt;First up for this one is making a plot of the data to see how much beers actually vary from type to type.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get mean and SD of each beer type 
abv.by.type &amp;lt;- beer.complete[ , .(MeanAbvType = mean(beer_abv), 
                                  SdAbvType = sd(beer_abv)), 
                              by = beer_style]

# For Graphing, order, set style as factor 
ordered.abv.by.type &amp;lt;- abv.by.type[order(-MeanAbvType)]
ordered.abv.by.type$beer_style &amp;lt;- factor(ordered.abv.by.type$beer_style, levels = ordered.abv.by.type$beer_style)

# Code for plot, blogdown crunches the images 
# Average ABV by beer type
# ggplot(ordered.abv.by.type, aes(x = beer_style, y = MeanAbvType)) + 
#  geom_bar(stat=&amp;quot;identity&amp;quot;) +   coord_flip() +
#  labs(title = &amp;quot;Average ABV by Type of Beer&amp;quot;,
#       x = &amp;quot;Beer Style&amp;quot;,
#       y = &amp;quot;Mean ABV, bars represent SD&amp;quot;) +
#  geom_errorbar(aes(ymin=MeanAbvType-SdAbvType, ymax=MeanAbvType+SdAbvType)) +
#  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/abvtemp.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So you can see here that if you wanted to have a higher ABV on average for your brewery, you’d benefit from having more IPAs, Barley Wines, and Belgian Stouts.&lt;/p&gt;
&lt;p&gt;Now with average ABV for each beer, let’s then match that to our big list, find how each beer fairs against its own category, sort them, and then combine them with our information from before on how big the brewery is.
For the purposes of this example, let’s only look at breweries that have over 100 beers in the database and look the top 20.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Combine ABV per type data with complete data
beer.complete.avg.abv &amp;lt;- abv.by.type[beer.complete, on = &amp;quot;beer_style&amp;quot;]
# Make new z score variable based on other beers in group
beer.complete.avg.abv[, zABV := (beer_abv-MeanAbvType)/SdAbvType]  
# Get averages per brewery on z variable
zAvgBeers &amp;lt;- beer.complete.avg.abv[, .(AvgAbvZ = mean(zABV)), by = brewery_name][order(-AvgAbvZ)]
# Combine back with our data on proxy of size of brewery
BreweryAndAvgAbv &amp;lt;- zAvgBeers[NoOfBeers, on = &amp;quot;brewery_name&amp;quot;]
# And the winner is...
BreweryAndAvgAbv[N &amp;gt; 100][order(-AvgAbvZ)][1:25]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                               brewery_name     AvgAbvZ   N
##  1:               Kuhnhenn Brewing Company  1.26003627 142
##  2:                     Cigar City Brewing  0.87721989 171
##  3:                             The Bruery  0.74965237 143
##  4:     Three Floyds Brewing Co. &amp;amp; Brewpub  0.71875251 128
##  5: Flossmoor Station Restaurant &amp;amp; Brewery  0.53182569 114
##  6:                     Brouwerij De Molen  0.51695738 119
##  7:               Jackie O&amp;#39;s Pub &amp;amp; Brewery  0.40179915 105
##  8:                          Mikkeller ApS  0.37012567 184
##  9:               Founders Brewing Company  0.24770929 130
## 10:                      Deschutes Brewery  0.19390121 127
## 11:      Port Brewing Company / Pizza Port  0.19357914 194
## 12:                     Fitger&amp;#39;s Brewhouse  0.18931699 111
## 13:                       Bullfrog Brewery  0.10590796 121
## 14:                Victory Brewing Company  0.09970522 107
## 15:         Iron Hill Brewery &amp;amp; Restaurant  0.03563186 269
## 16:                  Goose Island Beer Co.  0.01475446 304
## 17:                Sly Fox Brewing Company -0.01733499 140
## 18:              Sierra Nevada Brewing Co. -0.07417651 121
## 19:                      Stone Brewing Co. -0.08646265 119
## 20:              Cambridge Brewing Company -0.12326285 124
## 21:       Rock Bottom Restaurant &amp;amp; Brewery -0.26029542 522
## 22:                Willimantic Brewing Co. -0.28003004 134
## 23:     John Harvard&amp;#39;s Brewery &amp;amp; Ale House -0.29598861 151
## 24:          Minneapolis Town Hall Brewery -0.41963873 243
## 25:                                   &amp;lt;NA&amp;gt;          NA  NA
##                               brewery_name     AvgAbvZ   N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at this list, we get a totally different answer.
It appears that on average &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/2097/&#34;&gt;Kuhnhenn Brewing Company&lt;/a&gt; brews their beers 1.26 standard deviations above the mean of all other beers in that category!&lt;/p&gt;
&lt;p&gt;Both answers could be technically correct, but more importantly demonstrate how important it is to come up with how you frame your question first, and then try to answer it so you don’t end up going on a fishing expedition!&lt;/p&gt;
&lt;p&gt;Moving on to question #2!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Looking For Musicologists on Twitter</title>
      <link>/post/looking-for-musicologists-on-twitter/</link>
      <pubDate>Mon, 20 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/looking-for-musicologists-on-twitter/</guid>
      <description>&lt;p&gt;For the most part, Twitter is full of garbage.
But I’m an optimist and a firm believer in &lt;a href=&#34;https://en.wikipedia.org/wiki/Sturgeon%27s_law&#34;&gt;Sturgeon’s Law&lt;/a&gt; so by that logic there must be some good on it.
That good is academic twitter.&lt;/p&gt;
&lt;p&gt;While this isn’t a post advocating for academic Twitter, I did want to&lt;br /&gt;
1. see if I could figure out how to write a post with some R code in it and
2. share how I scraped Twitter to find active users in the Musicology and Music Theory community&lt;/p&gt;
&lt;p&gt;So here it goes…&lt;/p&gt;
&lt;p&gt;The first thing that you have to do is get some tweets.
Luckily some packages exist in the #rstats world that can help with this.
For this project I used the &lt;a href=&#34;https://cran.r-project.org/web/packages/twitteR/twitteR.pdf&#34;&gt;twitteR&lt;/a&gt; package which lets you log into Twitter’s API via R and and search it.
There are already some instructions on how to get started with it that you can find &lt;a href=&#34;https://davetang.org/muse/2013/04/06/using-the-r_twitter-package/&#34;&gt;here&lt;/a&gt;, so I won’t go into tons of detail about setting it up.
(Also please note you can’t just copy and paste my code verbatim since it requires credentials from &lt;em&gt;your&lt;/em&gt; own Twitter account)&lt;/p&gt;
&lt;p&gt;Let’s first load the two packages we’ll need.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)
library(twitteR)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next up, we need to access Twitter’s API by entering in the details from the link above.
I find it’s easiest to copy and paste each of my keys and tokens into a nice little character string, assign those to an object, then call those objects in the last command in this block.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;consumer_key &amp;lt;- &amp;#39;YOUR CONSUMER KEY HERE&amp;#39;
consumer_secret &amp;lt;- &amp;#39;COPY AND PASTE YOUR CONSUMER SECRET HERE&amp;#39;
access_token &amp;lt;- &amp;#39;THEN PUT YOUR ACCESS TOKEN HERE&amp;#39;
access_secret &amp;lt;- &amp;#39;AND YOUR ACCESS SECRET HERE&amp;#39;
setup_twitter_oauth(consumer_key, consumer_secret, access_token=NULL, access_secret=NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running that last line in the chunk should then direct you to your default browser.
This will log you into your Twitter account and R will ask for Twitter’s permission to enter through the back door.&lt;/p&gt;
&lt;p&gt;The next bit of code won’t run the way I have it set up because Twitter doesn’t let you download tweets older than a week old.
So if you want to play with tweets from a conference’s hashtag or some event, make sure to think ahead to download them!!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;amsTwitter &amp;lt;- searchTwitter(&amp;quot;#smt2017&amp;quot;, n = 700)
amsTwitter &amp;lt;- searchTwitter(&amp;quot;#amsroc17&amp;quot;, n = 1600)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This line above searches Twitter for anything matching the conference hashtags and saves the output of it in a list.
You can also include an argument asking for a certain number of tweets, which I’ve also done.
Luckily the twitteR package has a function that will take this list and convert it to a data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;amsTwitter.df &amp;lt;- twListToDF(amsTwitter)
smtTwitter.df &amp;lt;- twListToDF(smtTwitter)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these nice data frames, we’ll soon be able to join them together and count up some tweets!
In order to do this we can take advantage of the
&lt;a href=&#34;https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html&#34;&gt;data.table&lt;/a&gt; package to join our two tables together.
Of course there are other ways, but Ben over at &lt;a href=&#34;https://gormanalysis.com/&#34;&gt;Gorm Analytics&lt;/a&gt; sold me on data.table this past summer and since then I have really been loving its easy syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;amsTwitter.dt &amp;lt;- data.table(amsTwitter.df)
smtTwitter.dt &amp;lt;- data.table(smtTwitter.df)
amstweets &amp;lt;- amsTwitter.dt[, .(amsTweets = .N), by=screenName][order(-amsTweets)]
smttweets &amp;lt;- smtTwitter.dt[, .(smtTweets = .N), by=screenName][order(-smtTweets)]
totalTweets &amp;lt;- merge(smttweets,amstweets, on =&amp;quot;screenName&amp;quot;, all = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first thing the above code does is swap our data frames over to data.tables.
Once they are in the data.table format, we can count up the tweets by screen name, then list them from biggest to smallest all in the same line.
From there we merge the two together via the shared column, making sure to grab every instance in each table since not every Tweeter tweeted with both hashtags.&lt;/p&gt;
&lt;p&gt;We then need to clean up some of the NAs (which as a data.table are characters!) in our bigger dataset with R’s ifelse() function that basically works exactly like an ifelse statement would in Microsoft Excel.
It looks over a column in your dataset, checks if a value is an NA, if it is then it gives it a 0, if not, it puts in the value that was there in the first place.
After replacing NAs, I then make a new variable that adds together both columns then run our final line that prints out our final dataset from top to bottom.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;totalTweets$smtTweets &amp;lt;- ifelse(test = is.na(totalTweets$smtTweets),
                                yes = 0,
                                no = totalTweets$smtTweets) 
totalTweets$amsTweets &amp;lt;- ifelse(test = is.na(totalTweets$amsTweets),
                                yes = 0,
                                no = totalTweets$amsTweets) 

totalTweets[, TotalTweets := smtTweets + amsTweets]
totalTweets[order(-TotalTweets)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here it was simply a matter of using an &lt;a href=&#34;http://www.convertcsv.com/csv-to-html.htm&#34;&gt;online converter&lt;/a&gt; to turn it our final table an html file and then ssh it up to our &lt;a href=&#34;http://musiccog.lsu.edu/&#34;&gt;Music Cognition at LSU&lt;/a&gt; server!
Since then I’ve also added both the 2017 &lt;a href=&#34;https://musiccog.lsu.edu/davidjohnbaker/data/amsmt17twitterdata/AmtTwitterData.csv&#34;&gt;AMS&lt;/a&gt; and &lt;a href=&#34;https://musiccog.lsu.edu/davidjohnbaker/data/amsmt17twitterdata/SmtTwitterData.csv&#34;&gt;SMT&lt;/a&gt; datasets that I used to generate the counts in case you want to try this for yourself.&lt;/p&gt;
&lt;p&gt;If anyone has any questions on this, please &lt;a href=&#34;https://twitter.com/DavidJohnBaker&#34;&gt;tweet me&lt;/a&gt;!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
