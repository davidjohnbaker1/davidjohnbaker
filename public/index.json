[{"authors":["admin"],"categories":null,"content":"David John Baker is a music researcher and educator passionate about questions at the intersection of music and science. His research looks to understand how computational musicology and cognitive psychology can explore how music be can used to learn about how people think.\nIn his current role, he combines his love of teaching and research as Lead Instructor of Data Science at Flatiron School in London, England. Previously he has worked both on music industry projects as a data scientist and has volunteered in the charity sector.\n This website is currently being migrated to a new version, so please excuse any mess you find while I update!\n ","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"David John Baker is a music researcher and educator passionate about questions at the intersection of music and science. His research looks to understand how computational musicology and cognitive psychology can explore how music be can used to learn about how people think.","tags":null,"title":"David John Baker","type":"authors"},{"authors":[],"categories":["data science","education"],"content":" The past few months have been pretty exhausting. On top of all things lockdown and the global pandemic, a little over a month ago I was told that my job was “at risk” of redundancy. After going through the full collective consultation period, today is my last official day working as an employee of Flatiron School. While this has been a bit of a shock, I’ve been thinking about my time at Flatiron these past few weeks and as my first prelude comes to its final PAC and brace for the subsequent fugue1, I always find that writing about what is on my mind really helps me see the bigger picture.\nLooking over everything I did mull over, it’s quite long so I’ve broken it up into three parts:\n What Did I Just Do?: A prosaic setting of my resume Vocationtional Vicissitudes: My thoughts on why vocationalism versus the liberal arts helps me understand higher education My Dream AD (Applicant’s Description): Things I Might Want to do Next with My Job  As always, thanks for caring about me and reading my blog posts.\nWhat Did I Just Do? For the past nine months I have been a Lead Instructor of Data Science at the London campus of the Flatiron School. During my time there, I’ve led four different cohorts of students through an intensive fifteen week program where students with not much experience in coding or statistics or machine learning immerse themselves in order to learn as much as they possibly can from their instructors, the written curricula, as well as their peers.\nThe bootcamp approach to learning is extremely different from that of traditional, liberal arts, higher-ed and while it can get a bad reputation for not being as exhaustive as spending four years doing something (duh), there are many things that bootcamps really excel at such as having so many learners concentrated into one space, all at about the same level of understanding, with near constant access to instructors to help guide a herd of overly eager learners.\nAt Flatiron School, I was mainly responsible for delivering material for the first two modules they took which were “Python for Data Science” and “Probability and Statistics” and was supported by a team of four wonderful people.\nMany of us are now also on the job market now and if you are reading this and want to pick up the scraps of a fantastic team, go add these people on LinkedIn. All you have to do is click the links below. They’re all fantastic educators and even better data scientists.\n Wachira Ndaiga Dan Sanz Becerril Javi Fernandez Suarez Ioana P  I also lectured a bit on other topics that I personally like a lot such as clustering, principal components analysis, and was always the one to give my obligatory Introduction to R and the Tidyverse workshop. We taught mostly Python, so I wouldn’t have been able to sleep at night without introducing them to R.\nAs of today, we’ve graduated a sizeable batch of students and have placed many of them in jobs.2 We’re not at 100% yet like our Software Engineering program was able to do, but I get messages from students when they eventually get jobs and it’s always the best part of my day. Our students have gone on to work from companies ranging from start-ups, to educational programs like Flatiron School, to more high visibility brands like Nielsen and the Bank of England.\nIn addition to all things teaching, I also got my first taste of being part of senior management, so I got to be on the hiring side of things for once. I now understand what people mean when they say people will only look at your resume less than a minute. Being on the other side of the interviewing power-dynamic was surprisingly one of the most valuable experiences I will remember going forward. I’ve also felt how the volatility of a business’ performance directly affects one’s work since our parent company was WeWork. There were definitely ups and downs.\nNear the end of my time, I also got to help out with a bit of the curriculum development (I got to write some of the R materials for our graduate’s continuing education) that you can check out on the newly minted Teaching section of my website.\nIt was a lot of work a lot of the time (especially at the start of the role and then again when we had to jump online) and it was 180 degrees in the direction of what I was doing before (writing my dissertation alone in my flat). And although it wasn’t what I thought I would be doing straight after my PhD, I learned a ton from the work and found many aspects of the job very enjoyable.\n The Good By far, the best part of the job was working with the bootcamp students. This group of students was by far the most engaged collective of learners I have ever worked with. To contextualise this, prior to teaching data science at Flatiron, the students from the two areas subjects in higher education I had experience with were both music theory and aural skills (a subset of music notorious for having undergraduates historically question the content area’s utility) as well as statistics for doctoral level psychologists (where a majority of the class was clinical psychologists who, for good reason, question both the quantification, reficication, and reductionism of quantitative methods).\nIn contrast to undergraduate music majors and clinical psychologists3, none of the students at Flatiron were in my classes because they were “required to for graduation” and instead most of them had quit their jobs hoping to change the entire trajectory of their career. You fork out a lot of cash to do programs like this and that’s not something they took lightly as students and thus it was not something we took lightly as instructors.\nAs a result of this large time and financial investment, the students would absolutely rinse myself and the other instructors for questions every day to the point that by 6:00PM I was totally exhausted from so much face-to-face work. This constant engaging with instructors and asking as many questions as possible is actually one of the big appeals of this kind of flavor of bootcamp; there are not many other educational settings where students have so much direct access to instructors and peers who are all on the same page.\nWhile we did our best to set boundaries with students it was hard not to engage with people who were so eager to learn. The real trick that I eventually learned was to try to get the students to essentially bootstrap each other’s learning and move away from the sage-on-the-stage approach and move over towards as much group and active learning as possible. I learned a lot about giving up control in the classroom and what you get when you start to acknowledge the fact that when you have adult learners, they bring with them far more life experience than those fresh out of school.\nAs a result of all this rapid learning, the last major thing that I loved about this job was getting to see so many students grow so quickly during their time as a bootcamp student. Of course it’s true that you’re never going to learn everything in a bootcamp, but these environments are designed to get someone from near zero to competence so they can get in their own self-directed learning feedback loops where they can start critically thinking rather than adopting a pumping them full of knowledge approach.\nNow teaching in a bootcamp setting was not like anything I had taught in before. The goal wasn’t to teach people the history of sciences or competing philosophies of science throughout the 20th century (though I did weave a lot of that in my lectures, not gonna lie); it was straight up about getting people the tools they need to get a job.\nFor this reason, I’ve now spent a lot of time thinking about the idea of vocationalism versus liberal arts education which I elaborate on in Part II.\n  In keeping with the metaphor I used when I wrote about joining Flatiron↩\n I don’t want to go into the numbers I keep here since that probably counts as sensative information to post publicly, but the students are doing well. We’ll have to wait for our next third part audit for the official numbers.↩\n Obviously there were some in each group that were really into each class, I don’t mean to portray this as apathy↩\n   ","date":1593475200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593420535,"objectID":"baea04850d055cb8776c4283d0b5536f","permalink":"/post/begin-the-fugue-part-i/","publishdate":"2020-06-30T00:00:00Z","relpermalink":"/post/begin-the-fugue-part-i/","section":"post","summary":"The past few months have been pretty exhausting. On top of all things lockdown and the global pandemic, a little over a month ago I was told that my job was “at risk” of redundancy.","tags":["data science","education"],"title":"Begin the Fugue, Part I","type":"post"},{"authors":[],"categories":["data science","education"],"content":" Vocationtional Vicissitudes After existing in this bootcamp space for the past few months, I feel like I am walking away with a very fresh take on what it means to teach in a vocational, rather than a liberal arts setting. When the goal of the teaching is unabashedly “be useful in a neoliberal vocation” context, there’s a lot less theory (of course something I personally wasn’t the biggest fan of for hopefully obvious reasons) and much more honesty about what this specific type of student wanted out of their education: new opportunities for employment.\nI want to be very clear that I do not think that this approach should be the default goal of education (even though that is the new constant pressure), but now having experienced living in this bizarro world where success is defined solely by employment I now have a lot more opinions about the “value/values” demarcation in education. In many ways, I very much agree with many goals of Flatiron in that there is a huge hole in the way things are set up now for vocational job training that doesn’t fit the traditional guild/apprentice model (electrician, plumber) and there should be more opportunities for people to accomplish this if they want.\nThis is especially true if it will lead to a marked difference in quality of life when it comes to their income. This absurdly large differences in income has been benefiting people (white dudes) in tech for a long time, and opening it up to others via education is imporant. It makes me very happy to have worked for a company that has made this a priority and has allocated scholarship money for those that are underrepresented in tech and to have been able to pass on those opportunities to people I know personally.\nOf course the idealist in me would go so far as to try to steer the next part of this conversation away from idea of social mobility (why don’t we just pay people all more than living wages and avoid classist ideas that value white collar above blue collar work) but the realist in me knows that this structure is entrenched in Western society right now so if there is a way to open the door to making more money and better quality of life for people through work, then people who can open that door need to open it as wide as humanly possible.\n Emulating Ideas from Vocational Education Reflecting on the vocational versus liberal arts really has me thinking about that and what the future of what higher education could be. Many of my academic colleagues know the pressure of being asked to demonstrate “transferable skills” in their curricula. I literally was praised today for including a line in a course I plan on teaching this Winter for explicitly saying I will talk about career paths within music psychology. Students rightly want to be able to make a living for themselves when they graduate.\nBut if we just follow the job market and placements as our only metric for success, I feel as though we really betray the entire goal of education. If you do think that, you need to get yourself a copy of David Graeber’s Bullshit Jobs immedietly. Now I don’t want to fall prey to Rule Number 2 of grad school (“Be on the lookout for false dichotomies”) thinking we can’t have both, but as more pressure is put on higher education to use employment as a KPI (a new term I learned this year) there is a lot higher ed can and should take from the vocational/bootcamp model. If so much can be accomplished in so little time that can lead to better chances at employment, I am sure there is time during a four year undergraduate degree to equip them with what they need for employment (if they want to do this) that is not just a few resume review sessions.\nI know this might not be as big of a deal for some of my STEM colleagues or students, but I do think about what this would look like if there was some sort of vocational option for music students (my home field). The irony is also not lost on me that when I talk about music students, I understand the historical links to a music conservatory, which again is all about going on to “work” as a professional musician (having lived that life myself). But the reality that every one of my conservatory friends found post-graduation is that it is really, really hard to have a “normal life” as a performing musician. You can gig a lot and do a fair bit of teaching, but you often find that many of your colleagues are bank rolled by their wealthy spouses or parents. And on top of that, the hours don’t align with the “working day” (which I know not everyone wants anyway). Or you can go on tour and basically give up your “normal life”.\nIf you do this, you can be in the position to make a living, but it’s not exactly normal. I know things are a bit different for Music Ed, but I’m more thinking about the many jobs that people with really fantastic music educations could do, but either don’t know exist or have zero job preparation to get themselves that role. For example, the graduate student in musicology who has their finger on the pulse of the fantastic ideas going around academic circles, but would never have anything in their curricula about putting theory into practice at something like an arts organisation, let alone the connection from a professor to put them in touch with the person to get them employed.\nI could easily imagine what a summer intensive might look like for music students (especially grad students) looking to pick up the skills they would need to go alt. I am sure there are tons of major orchestras or larger arts organizations that would benefit tremendously from music students who are like what I just described, but lack the mental model as to what power tools they need to be effective. Just learning basic data literacy, enough programming to wrangle data and make convincing visualizations for the board, and tools for thinking to not make rookie inferential mistakes could all be learned in a setting like this. Of course how I am thinking about it takes much more of a quant focus than what would traditionally be offered in a music school setting, but that’s kind of the point in that you’d be able to do both. I can’t emphasize enough how much computer and research/data literacy (not even mastery) can add value to your organization when you couple it with the pre-existing expertise you have of your domain.\nI am sure a small group of students who just wanted to learn these in-demand vocational skills over the summer or an alt-semester, they’d be ripe and ready by the end of it to go on and help arts organizations or work somewhere else. Maybe even set it up to lead to internships or something? The new skill set would give them comfort in knowing all their eggs are not in the one basket that is music performance or the “academic job market” while not losing the culture of total immersion in music. Just a little bizarro world like the one I have been living in for the past year for them to enter for a few months to give them something different.1\nOf course I don’t know who is going to pay for that or if students would be interested, but it’s got my mind ticking after seeing what can be done in three months when you just stare into vocationalism.\nThe whole experience teaching in this context thing really felt like if you’ve had the chance to learn a second language (non-natively) like being a white kid learning Spanish in secondary school and even though you’re learning Spanish, you’re also learning more about English because you had no idea what you took for granted about your own language. I will definitely be re-doing many parts of my teaching statement after the past nine months here.\nI won’t be able to implement these zany ideas anytime soon, but I did want to put them out there now since I think this push for vocational skills within the liberal arts will only become stronger as funding continues to drop for higher education and students withdraw from enrolling until they can go back on campus.\nI’ll just end this section again here with repeating that I do not think that this emphasis on jobs should be the new standard for higher ed, but rather stealing ideas from the bootcamp model, annexing it as something separate for students who are interested, and then actually pulling this idea that liberal arts degrees need to be about employability out of the curricula so we can focus on deeper issues than just getting jobs.\nNow knowing I won’t be doing this next, what might I be?\nThis takes me to Part III of this series of me having a lot of time to think the past month, but no ability whatsoever to concentrate on my professional research.\n  I can already hear the response of “There is no room for this in the curricula! We barely can let go of Set Theory!” Maybe one day it can be a fun Twitter conversation.↩\n   ","date":1593475200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593421224,"objectID":"8cb8da8f77906aff38953b20ac6cd2a0","permalink":"/post/begin-the-fugue-part-ii/","publishdate":"2020-06-30T00:00:00Z","relpermalink":"/post/begin-the-fugue-part-ii/","section":"post","summary":"Vocationtional Vicissitudes After existing in this bootcamp space for the past few months, I feel like I am walking away with a very fresh take on what it means to teach in a vocational, rather than a liberal arts setting.","tags":["data science","education"],"title":"Begin the Fugue, Part II","type":"post"},{"authors":[],"categories":["data science","education"],"content":" What’s Next Having coached many people to apply and get their first jobs in the world of data science, I have now seen A LOT of job postings and understand that a JD (job description) is more of a wish list of an ideal candidate rather than a required listing of skills. On top of all of that, the characteristics you want out of your future colleagues (like all of the patience and kindness required to sit with a student as you explain how git works, a skill that our instructors seemed to have infinite amounts of, again add ,Wachira Ndaiga, Dan Sanz Becerril, Javi Fernandez Suarez or Ioana P on LinkedIn if you want this, you don’t even have to send a message) do not show up on resumes.\nSo in that spirit, I want to end this post with posting my own description of what I’d like in my own work, maybe I can think of it as an AD (applicant’s dream!).\nI know I won’t hit everything that I’ll list here for my next line of work, but I have a way better idea of what I want out of my next employment having worked in my past role.\n Applicants Dream (AD) The first, and probably most important, aspect I want of my next job is one where I am challenged in a way that leads to daily personal growth. I found my work at Flatiron School to be challenging, but doing the same fifteen week loop over and over again so quickly made it feel more like I was getting better at some insane Shaun T HIIT work-out in terms of my teaching endurance rather than doing some sort of brain bulk (I hope my next employer likes colorful metaphors).\nThe second thing I really want is to be around people who are obsessed with what they do. As soon as I was told that my job was “at risk” of being redundant, my first thought was thinking about applying for some sort of academic jobs. Rationally, this is probably not the greatest idea considering the incoming COVID Higher Ed crisis, but I still felt this pull.\nWhy?\nWell, I spent a lot of time reflecting and thinking about this very irrational thought and after sitting with it for a couple of days I think that the main reason I feel this pull is because I know that academia selects for people who like and value the same weird stuff that I do.\nAt this point in life I feel OK telling people that I have extremely nerdy and niche interests compared to many people. In conversations with many of my friends, I know that I like to dive much deeper into my interests and am the kind of person who reads textbooks for fun. (#iamverysmart1) I want to be around other people who are like that.\nOf course this has its serious downsides, I find it really, really difficult to have hobbies that don’t turn into obsessions and projects, but at least I am aware of that and OK with that now. That is kind of what academia is and it’s nice to know that I am going to have traits like that in common with my future colleagues from Day One if I were to work in that setting, but I have learnt (in more of a feeling, rather than intellectual way) that academia is not the only place that this type of personality exists. Again, this is very much like the learning Spanish analogy from before having worked somewhere where I didn’t think I would have.\nNow I think it’s becoming a lot more common for many academics to work in industry after coming to this realisation. At this juncture in life (and considering the future of the academic “job market”), it seems a bit wiser to still keep an eye open to whatever opportunities come up that are close to what I find meaningful, rather than getting too parochial too soon.\nSo right now, what I really want to do next is keep investing in my technical skills, ideally in the area of where my domain expertise is which is music. At Flatiron School, every so often when students would ask me a question where I didn’t know the answer right away, I had to remind them that the knowledge that I have about the world of data science is really only a by-product of wanting to answer questions that I have found personally meaningful in my own work.\nAt the end of the day, I didn’t learn all this coding and stats stuff so I could just teach it (never in 100 years would I have thought I’d be doing that in my first year of my PhD); I learned it so I could ask really obscure questions about the the hierarchical structure of western tonal music in a subset of a WEIRD population known as Western conservatory musicians who are learning solfege. That’s what I currently really like.2\nIs there market value in knowing that? Can I make the “business case” for that? At this point, I’m sure I could spin it so there was something, but I’m OK with it standing on its own.\nBut I know that no one will pay me to do that (unless I can maybe secure some postdoc funding in that?) but I’m planning on using a lot of my time in the future to devote to my WEIRD interests.\nThen lastly, going forward to my next line of work it would be really nice to have a manager who was as kind as my old manager. I remember being totally dumbfounded at the idea of “onboarding” coming from an academic background (hello baptism by fire with hidden curricula abound!). The amount of support and questions and effort my manager went through to get to know me as I was starting was something I will take with me if I am ever in that position. Big thanks to Ben Miller for everything on that front.\nThen lastly, it would be nice, but obviously not necessary to have all the perks of working in industry like having a salary that allows you to be more generous with your money, no obligation to work off the clock, free coffee and beer (that will probably never happen again), and remote work options.\n Conclusions Now a lot of this sounds like maybe the next step here is to maybe do a postdoc or try to secure a job doing music industry, but I would not be so bold with the amount of uncertainty that everyone is facing right now to say I know exactly what is next. Even though I might know what I want, I’m fully aware that I need temper my expectations.\nAnd if I can’t land something right away, I have a lot of personal research projects I want to attend to in the near future and the fact that I want to spend my free time working on these probably is a big hint of what I should look at next. I also have three fat textbooks I want to read this summer and have plans to brush the dirt off a lot of my programming.\nLastly, I’d just like to end by saying that I had a great experience in something that I didn’t anticipate and just wanted to say that out loud for anyone else that reads this and might be in a similar position. It felt like such a commitment signing my contract here, like I’d given up on what I wanted to do. But here I am again, thinking about weird research in a much more stable position, using my free time to write papers and submit to conferences I find interesting.\nThough I guess the real end of this should be if anyone knows of any gigs or jobs, either in teaching or research, please get in touch. I’m cleared to work both in the United Kingdom and the United States.\n  This is ironic for people who are not on the internet as much as I am.↩\n I love teaching too, don’t get me wrong, but I’m hoping my pendulum swings a bit more near towards the research side next.↩\n   ","date":1593475200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593421263,"objectID":"aea525e96b4338f99db7d2234c9ce44b","permalink":"/post/begin-the-fugue-part-iii/","publishdate":"2020-06-30T00:00:00Z","relpermalink":"/post/begin-the-fugue-part-iii/","section":"post","summary":"What’s Next Having coached many people to apply and get their first jobs in the world of data science, I have now seen A LOT of job postings and understand that a JD (job description) is more of a wish list of an ideal candidate rather than a required listing of skills.","tags":["data science","education"],"title":"Begin the Fugue, Part III","type":"post"},{"authors":[],"categories":[],"content":"  This post originally appeared on the SysMus blog on June 9th, 2020. Find the original post here.\n Introduction You don’t get to be a grad student forever. At some point, you will take everything you’ve been working on in the past however many years, bundle it into a few hundred page document, defend it, and be immensely proud of the fact that you completed a doctoral degree on the topic of systematic musicology. In a utopian situation, you take a few weeks off after your defense, then come back to “reality” or the “real world” and consider what you want to do next.\nMaybe upon your return you get lucky and apply for a position at a university and are able to start teaching and researching right away. Or maybe you get a postdoc, pack up your bags and move to a new city to continue on your academic adventure for a few years, or maybe you end up taking an adjunct gig somewhere and have to teach part-time with no guarantee of future employment at something near a living wage. Or maybe you take a good look five, ten, or twenty years down the road and think to yourself: besides academia, what else is out there? What if you wanted to get a job in industry? Will I never get to do research again if I “leave academia”?\nHow does a graduate student in systematic musicology explore what else is out there? While there’s an entire culture built around how to continue in academia, there’s not nearly as much available for grad students to figure out how to land an industry job from within the academy.\nWhat benefits could preparing for being employable in industry have? And what steps could you take today in order to achieve these goals this week? In this post, I’d like to directly address these questions in order to provide advice for current graduate students in the SysMus community.\nBefore I get going, I assume that if you’re reading this post, you’re a graduate student (in systematic musicology specifically) and are interested in the idea of finding work after graduate school in industry. By industry, I mean literally any job that is not an academic job (aka what most of the world does). Specifically, I want to give four pieces of advice and the reasons behind them as to how you can best prepare yourself for getting a job in industry. It’s the how and why of what I wish someone would have told me a few years ago.\nIf you don’t want to read the rest of the post and just want the actionable items, here they are:\nMake a LinkedIn account but don’t position yourself as an academic. Ask people who do the job that you are interested in doing for help. Create an online presence. Get better at programming.  Though as academics we’re never satisfied with just the answer to what we should do; we want to know how and why. With that in mind, let’s get started investigating why I think you should do these four things if you’re interested in a job in industry.\nBut why take my advice? Since this isn’t my personal blog, I should probably introduce myself and establish a bit of credibility before taking some random advice from some dude on the internet. My name is Dave (though you’ll find me on the internet as @DavidJohnBaker since I have a pretty basic Western name) and as of last year, I completed my Ph.D. in Music Theory at Louisiana State University where I investigated how tools from computational musicology and cognitive psychology can help inform how we teach aural skills in music school.\nWhile a grad student, I went to a couple of SysMuses (is that the plural?) in 2014 (Goldsmiths) and 2017 (Queen Mary) currently am serving as the trainee representative for the Society of Music Perception and Cognition. I went directly from my PhD to working a non-academic job where I am currently Lead Instructor of Data Science at Flatiron School in London, England. In my current position I teach people skills they need in order to get jobs in industry, specifically jobs as (~junior) data scientists. I watch people make this transition into industry literally every day and as a former member of the SysMus community, am more than willing to try to help out any SysMus people I can if they’re interested in what I can offer in the future.\nOk, let’s get to it.\n Make a LinkedIn If you have spent any time in academia, you know that no one in academia uses LinkedIn. In fact, I have heard some senior academics brag about the fact that they literally do not get the point of the platform, don’t have an account, and pride themselves in not knowing about it at all. These are probably the same people who also boast about not knowing the difference between a CV and a resume.\nThis makes sense in a lot of ways.\nAcademics don’t need LinkedIn because the currency of what we value as academics is not an apparent part of the website’s infrastructure. Furthermore, what “counts” in academia does not “count” in most industry jobs. There’s no great way to display all your ever-increasing publication record, the pedigree of the institutions from which you have worked your way through, or any sort of teaching or service accolades you’ve accumulated over the years.\nAnd that’s OK. Your persona on LinkedIn shouldn’t be your academic persona.\nIf you’re staring at your yet-to-be-fleshed-out LinkedIn page and are having trouble inserting your square block of an academic persona into industy’s round hole, this trouble is just a symptom of the real reason I picked the first piece of advice as creating a LinkedIn account: you first need to realise that your value working in industry comes from your ability to add value to an organization; your online profile should reflect you understand that.\nSo how do you do that?\nThe first step is trying to figure out what value you can bring to an organization. If you’re working for a company (as opposed to a charity/not-for-profit), this value best translates as the literal monetary value that you will be able to bring to the company. You need to think about ways that you can position yourself to make it very clear via how you present yourself that you understand the role you are about to undertake is to create value for the company.\nCan you do analyses that will make their marketing more efficient? Can you write a small classifier that helps eliminate bots from their platform? Are you great at communicating the value of your work to non-technical stakeholders?\nYou should ask yourself: how can I best present myself on LinkedIn in order to communicate that the skills that I have are directly beneficial to the company that I will work for?\nJust like re-writing your #scicomm blog posts and tweets to make it easier for the general public to understand the multi-level mixed model you ran and wrote up in your most recently published peer-reviewed journal article, you need to rephrase what you can do in the language of the industry you want to enter.\nA lot of this comes from talking to people currently working in the area you want to break into (covered in my next point). For example, in the world of data science this can range from understanding that your background as a music psychologist in experimental design needs to be rephrased as proficiency in AB testing or maybe causal inference. You can use your latest paper you’ve published as a vehicle to talk about how you excel at taking ill-defined problems, turning them into something quantifiable, performing some sort of reproducible analysis on this, then sharing the importance and results of you work with both technical (fellow academics) and non-technical stakeholders.\nI’ll lastly note here that when you are advocating for yourself on LinkedIn, I’d say it’s better to NOT try to brand yourself as an academic, but rather the job title that you eventually want. In my case, my LinkedIn is me as a data scientist and educator. Am I really that? Who knows. You kind of are who you say who you are, you just need to be ready to back it up. For whatever job you want to do, you just need to be able to somehow to demonstrate you know what the requirements of the job are and that you can do that. In terms of industry, the closest job that my skills align with is data scientist and identifying as that (even though I don’t know if that’s how I’d describe how I think of myself) makes it easier for others to get what I do.\nI felt uncomfortable as a grad student at first calling myself a data scientist having never had a job where I had that title, but if you are graduate level scientist and understand basic inferential statistics, can spend some time and page your way through some introductory machine learning textbooks (for example, Introduction to Statistical Learning) , and can run a linear regression in R or Python and correctly interpret it, you’re more than qualified to at least put your title as a Junior Data Scientist who is looking for roles. If not, see point three below.\nAs for other jobs that you might want, a large part of this is going to be doing research (what we do best) in the area you want to enter then positioning yourself accordingly.\nRemember, you can always learn new models or frameworks, one of your biggest assets as a grad student in systematic musicology is the ability to learn and synthesize complex things quickly.\nDon’t worry that your academic side doesn’t show on LinkedIn, just add a note that if the person visiting your page on LinkedIn wants to see your academic side, check out your website or Google Scholar. Even though as a grad student your identity is probably 100% intertwined with your academic work, for your own mental health, it’s probably time to start to separate your identity from your work. And what better way than to have a LinkedIn persona that showcases how what you know can be applied elsewhere.\nOf course this will take a bit of research to find out how best to communicate this all, but research is what we academics do best!\n Ask for Help Just like academia, the world of industry is also hugely about who you know and how they can help you. Unlike academia, things in the world of industry move at a much faster rate and there’s a lot more off an ebb and flow in when and where you can get hired to do what.\nSo how do you navigate this new space?\nYou need to ask for help via some sort of network. Granted, when you are first looking into entering the world of industry, you probably don’t know too many people, but if you reach out to those you do know (especially those who can relate to your current situation, former academics) I’d bet you’ll be very surprised of how supportive people can be.\nSpecifically, how do you do this?\nOften it’s a matter of adding someone on LinkedIn (LinkedIn is not like Facebook, people add pretty much anyone looking to connect, especially if it comes with a meaningful note), tweeting to someone you think might be helpful, or asking around in your current academic network about people who you can best reach out to in order to start building those bridges. If they can help, most likely they will; If they can’t help, they probably will direct you to someone who is better suited for your specific situation. If this doesn’t happen, you can message me and I’d be more than happy to try to help you out.\nIf there’s some sort of professional space that you are looking to enter that doesn’t have an obvious person-you-know point of entry, one of the other highly suggested “things to do” is to go to some sort of Meet Up around the topic you’re interested in. People who go to meetups in general all tend to be interested in the same thing (obviously, or they wouldn’t spend their free time going to them) and are often looking to grow their community so attending an event is a great way to show your interest. Right now (2020), many physical events are currently cancelled, but this doesn’t mean there are not stil online events. People are adapting quickly.\nSo what do you even message them about?\nWell you don’t really have to know exactly what you want, but remember it’s always easier to give people specific requests when you approach them and say something like:\n“I want to be able to apply for jobs in music and advertising in 8 months time, what should I do to best prepare for that and work at a company like Shazam?”\nThough don’t feel like you need to know exactly what you want. You should also feel free to message people and ask for guidance in the form of an informational interview. There’s a lot of information on the web on this, so I won’t go too much into that here.1\n Have an Online Presence In addition to just having a LinkedIn account (your industry Google Scholar!), I think it also helps to have a bit of an online presence. This could mean having a Twitter feed, ideally a website and possibly even a blog. If you’re interested in reasons as why you might want to do this, check out this post here by David Robinson on why it’s a good idea for aspiring data scientists but many of the reason hold outside of data science. If you think starting a blog to just share information is a lame idea, you can read this post I did on my personal website after talking about this with one of my former students.\nThe executive summary of that (notice I didn’t say abstract) is having this presence allows you to communicate with the community you want to enter. It shows ahead of time how you think about problems. It can also catch the eye of people in the industry you want to work in.\nFor example, maybe you want to stay in the world of music and not do “data science” but try to get on the ever growing data train for arts management. If you can show what you can do by showing how you take some publicly available data and turn it into a meaningful narrative, you’re going to stick out much more in both processes of networking to show your value which will eventually get you hired.\n Get Better at Programming The last thing that I would be remiss not to mention, especially for those doing any sort of quantitative related research is to get better at coding.\nWhat blows my mind is the amount of people who learn all these super-out-there stats for getting their research published, but do not put in the time (especially given the freedom you have in grad school) to teach yourself some sort of open source programming language. Arguments for open and reproducible science aside, coding will open up more doors for you in terms of career and salary than anything else right now on the market coming from academia.\nIt’s a bit of a pain to learn in some ways, but one of the reasons it pays so well is because it is hard to learn. But if you are no longer limited by what software is in front of you (or what your company can afford…) then your value will increase not only in the near future, but I am sure it will carve out a great trajectory for you over the next twenty years.\nI feel confident in passing on this advice to the SysMus crowd in particular because in doing a graduate research degree, this process of just getting good at asking meaningful questions is the thing that is way more difficult to teach and get better at in a workplace setting. Now working with a lot of people who are aspiring data scientists, I have seen time and time again that just like practicing an instrument, you will get better at coding by just practicing it daily.\nIn your current capacity as a graduate student, the way to work on this is to slowly move your projects to something like R or Python, and to try to make those projects available for people to see on Github or the Open Science Framework. Just learning how to document them well and work in a code based environment is an extremely good investment for the next 30 years of your working career (assuming median age of most SysMus readers is a little less than 30). Doing this and posting it will show to future colleagues and employers that you are going to be a colleague who is useful and easy to collaborate with.\nIf you’re totally new to this, the way I would suggest going forward is to first make your way through R for Data Science , then read Hands on Programming with R , then slowly try your hand at easy coding problems on websites like r/daily programming challenge , daily coding problem or code wars.\nWhen going through daily challenges, try to think of each one as a little brain teaser. They’re just like nerdy crosswords or sudoku. It’s OK to look at the answers if you don’t get it after a couple of minutes, remember the point is to learn new ways of thinking and coding, not test the limits of your patience. Of course programming languages are not spoken languages and you also need to remember that when you are starting out, you need to build up a lot of vocabulary and see what a language can do before you can expect yourself to just look at a blank page and start typing code like some sort of hacker. Just look at a lot of code at first, the writing will come soon enough.\nIn terms of what languages to learn, it really depends what you want to do, but you can’t go wrong with R, SQL, and Python. If you work in anything quant related, you have to learn SQL. Everyone does SQL. Then maybe pick either R or Python. I suggest R here (and above) because it’s way more common where you currently are as grad student in systematic musicology where you can get help from peers. Don’t make things harder for yourself. You can learn other coding languages later. There are tons of articles on the internet differentiating why you might start with one or the other, but I’d suggest picking the one that you will find most useful right now for what you’re doing as a grad student.\nI would also suggest slowly making your way through Introduction to Statistical Learning if you are remotely interested in data science. There are also data science equivalents to the daily coding challenges. Note that in the data science world it’s wise to learn both Python and R, but if you’re getting to that point and want resources on all this, please just get in touch with me personally and hopefully by then I will have some more resources to point you towards. You don’t have to do it all at once.\nIf you need further incentive, I suggest checking out the most recent stack overflow calculator and just putting in one year of R or Python in combination with a doctoral degree.\n Why? Now you don’t have to take my advice here and the advice provided is not exhaustive at all. But if you are in the middle of your PhD and are reading this, I would argue that you have nothing to lose and everything to gain from doing a little bit of industry investment each week as a graduate student. If you say you don’t have time, you’re probably taking a very parochial view of what your career and life could be by putting all your eggs in the proverbial academic basket. This is not a smart move on either personal or professional note (you never know when life is just going to happen).\nIf you start to build this side of your professional development, you’ll get a couple of things. I think the most valuable thing you will gain is the peace of mind that when you are done with the monumental task that is doing a PhD, you will have some options waiting for you. By investing in these skills you will open up more avenues in your own research and will further aid your ability to pursue the questions that got you into academia in the first place. Lastly, by just trying to meet more people outside the Ivory Tower, you’ll meet more friends than you would have otherwise.\nWinding this post down I want to reflect on a talk that I also heard David Huron give at the Society for Music Theory in Vancouver the during the second year of my PhD where he predicted that a lot of the research that will capture the public’s attention in the realm of music (broadly defined) will come from the world of music industry where they have endless amounts of data and he predicted that the category divide between academia and industry will only become more important to be able to walk back and forth between the two. Now I don’t think the most important questions will be asked at this intersection, but it’s something to consider.\nWhy do I think this is important to note? Well it’s worth saying out loud that even if you don’t go on to an academic job, research still happens outside the Ivory Tower. There’s a big demand for the skills you are cultivating as a graduate student. You have a lot more value than you think at this point.\nOf course this might not be as idyllic as the past few paragraphs have made some parts of working an industry job sound. There’s a lot less time to work on things that are uniquely yours. How you spend your time is at the discretion of your boss and the needs of the company. If you want to work on your own stuff you have to do it on your own time. But the flip side of that argument is that you actually have your own time.\nRight now in my current role I am finding it a bit frustrating to work on a few of my pet projects, but again this comes in exchange for a job with a salary that reflects the years of training I have put in, a healthcare plan, the right to not have to “take my work home with me” after the end of the day, and peace of mind that now knowing how the industry works.\nThis has been quite the essay, but hopefully it’s explored a space that I know I wish I would have heard more about when I was a PhD student. Please feel free to reach out to me if you’d like to talk more about this!\n  https://hbr.org/2016/02/how-to-get-the-most-out-of-an-informational-interview↩\n   ","date":1591660800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591691391,"objectID":"d0d26e0d4d5e0a34068ccf71e690f4c8","permalink":"/post/life-after-a-phd-in-systematic-musicology-how-you-might-prepare-for-industry/","publishdate":"2020-06-09T00:00:00Z","relpermalink":"/post/life-after-a-phd-in-systematic-musicology-how-you-might-prepare-for-industry/","section":"post","summary":"This post originally appeared on the SysMus blog on June 9th, 2020. Find the original post here.\n Introduction You don’t get to be a grad student forever. At some point, you will take everything you’ve been working on in the past however many years, bundle it into a few hundred page document, defend it, and be immensely proud of the fact that you completed a doctoral degree on the topic of systematic musicology.","tags":[],"title":"Life after a PhD in Systematic Musicology: How You Might Prepare for Industry?","type":"post"},{"authors":[],"categories":["review"],"content":" A couple of weeks ago I attended Dr. Frank Harrell’s Regression Modeling Strategies course. I wanted to use the chance to write a short post on some of the key takeaways that I got from the course for my own sake and just keep on my practice of writing every day.\nExpectations and Reality The course met my expectations in that it was four days of A LOT of information from Dr. Harrell’s book on regression modeling pitched for people who know this stuff way better than I do.\nI don’t have a background in (bio)statistics, epidemiology, or live and breathe stats every week to the extent that I imagine most other people on the Zoom call did. At many times it felt like tons of the material was slipping right through my fingers and was struggling to keep up with it (first semester grad school throwback!). Maybe things would have been different if I was not on UK time and starting the course at 3PM and ending at 10PM (after doing some teaching in the morning), but there’s no alternative universe in which I would have been sitting on the other end and feeling comfortable with the content given what I’ve learned up until this point.\nThat said, I learned SO MUCH from this course and spending the six hours a day soaking in this information made me realise how much I miss having research be a part of my daily life. In the spirit of making writing a practice and not a performance, I wanted to summarize much of what I did take away from the course for my own (and other’s) benefit.\n Simulations My first big takeaway was that I now feel that statistical simulations were and continue to be the missing piece of my previous statistics education and teaching.\nIf you’re not exactly sure what I mean by this, a statistical simulation (or at least what I mean when I am referring to it in this context) is when you as the analyst sort of “play God” in order to create data that behaves exactly like you want it to then see how well your methods do in finding the “Truth”. Of course we don’t get this luxury when doing actual research (since if we knew the Truth, we wouldn’t have to do any empirical research!) so running a simulation before we actually get any messy data will give us a much better view of how our methods will perform under different conditions.\nFrom a research perspective, doing this seems almost totally obvious to have to do before investing so much time and energy into data collection, but speaking first hand as an applied researcher who is learning much of this as they go, this was just never a part of any class I ever took.\nOne of the reasons that this idea of running simulations resonated with me so much mostly comes from one of the hours on the second day of the class where Dr. Harrell used this technique to show how bad running automated stepwise regression can be. You can page through the notes here in the notes for Chapter 4 but the TL;DR is that if have some sort of regression model with 8 variables– four of which actually have a true relationship with what you’re trying to predict– and you were to just rely on Akaike’s Information Criterion to do the work for you, you need at least ~2,000 samples in order to have your little robot select the correct set of variables only half of the time…\nLike much of my statistical knowledge, I feel like so much of it started as a collection of rules of thumbs such as “never, ever, ever do stepwise/automated regression models”, but it was not until I saw something like this where it began to to make sense on how to get these many thumbs that I have been collecting.\nI think a lot of that has to do with the fact that a simulation gives the learner some sort of causal dial to play around with and not just be told that “well if you did it a different way then this would happen, just take my word on it”.\nThough getting back to the stepwise regression example, this little detour was depressing as hell to know how common this is in research and how bad of a tool it actually is and how long people have known about it. I want to play around with this a bit in the future to understand it better and will hopefully be blogging about it more soon.\nBut as depressing as this was to know how misguided so many regression models have been, one thing I really did like about the course was how Dr. Harrell reiterated many times that the reason that he does this course is so other people don’t have to make the mistakes that he did.\nI guess the bottom line of all of this is that I really think that becoming a bit more knowledgeable about simulation data is going to be helpful both in my own research as well as as a pedagogical tool going forward. I think coupling it with ideas about pre-registration, it will help a lot more with thinking beforehand about what the true state of the world rather than just taking a shotgun approach to knocking down null hypotheses and calling it science.\n Case Studies While many of the more abstract concepts went a bit over my head as someone without formal statistics training, the next thing that I found a lot of value in were the few case studies that were used to exemplify the topics being discussed. Nearer the end of the week, there were a few examples taken from Regression Modeling Strategies that Dr. Harrell worked though to demonstrate many of the concepts we had been talking about in action.\nFor example, there was an extended walk through of modeling the titanic3 dataset. Now I know that many data science people reading this might groan at the thought of slugging through another tutorial on that uses the Titanic dataset, but from a pedagogical standpoint using these canonical datasets really does reduce the germane cognitive demands of a learner to take in both new techniques and all things relevant to a new dataset.\nBecause there was so much new information being conveyed, if Dr. Harrell would have used anything else, I think my stats brain would have basically imploded. What I found particularly helpful about this walk through (and many others, which is why I think these screencasts by Julia Silge and David Robinson have become so popular) is the chance to see what an analysis looks like through the eyes of an expert.\nPersonally, I feel I learn best when trying to emulate in the presence of an expert with them gently guiding me when I go off course. This might come from my background as a musician, but I personally can’t think of anything more powerful from a pedagogical standpoint than one-on-one pedagogy (For more reading I remember seeing some refereneces to this in Teaching Tech Together both here and here)), but of course this doesn’t scale well.\nWhat’s the answer then?\nWell, I think a bit part of it is trying to give learners access to that stream-of-consciousness leading to final product thought-process so that the learner can build up whatever temporary scaffolding they need to figure out how and why an expert made the choice they did.\nReflecting on the course three weeks out, returning to these case studies is the first thing that I will want to do when I do have more time to look at this because everything was so tangible and thus much more memorable. I will be keeping that in mind for my future teaching.\n Random There were other many other gems that I also took away from the course that didn’t make it under the main umbrellas here like justification for using multiple imputation methods for missing data (answer: don’t throw away valuable data you already collected!!), not assuming that your model is always going to be a straight line and thinking about using splines (when does that ever happen in nature), and a whole lot of other stuff.\nSo hopefully going forward I will be able to start hacking away at all this, this coming summer with my own projects.\nThe last thought I wanted to end on regarding things I took away from the course (so I can also come back to and know that I actually did feel this) was more of a lateral learning moment in hearing many times that the goals with a lot of the statistical modeling that I do, there is way less on the line than what others have to deal with.\nThere arn’t millions of dollars of government money on experiments I design and I’m very glad that degree of ischemia is not the dependent variable I need to model in a clinical trial. Relative to this kind of work, there’s no reason for me to get as stressed about many of my modeling choices in my goofy reaction time studies where I am looking at recreating a WEIRD subject’s implicit understanding of Western tonal music via some sort of behavioral paradigm. Yes, I might get it wrong, but if you can’t be wrong, you’re really not doing science.\nSo wrapping up, I really took a lot from this course despite the fact that I felt very out of my depth most of the time. The course made me really realise how much I missed learning at this rate and made me long for being able to do more research rather than teach more. I have a feeling that will be more reality than I think in the future, but I guess only time will tell on that front!\n ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590756084,"objectID":"2aa28818ff9ce0eb7605a7e2a09807cd","permalink":"/post/regression-modeling-strategies-review/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/post/regression-modeling-strategies-review/","section":"post","summary":"A couple of weeks ago I attended Dr. Frank Harrell’s Regression Modeling Strategies course. I wanted to use the chance to write a short post on some of the key takeaways that I got from the course for my own sake and just keep on my practice of writing every day.","tags":[],"title":"Regression Modeling Strategies 2020 Review","type":"post"},{"authors":[],"categories":[],"content":" I have always tried to live by the adage that if you are going to do something, you should try to do it well. For the past eight or so months at Flatiron School, what I have been doing is teaching. In fact, like many trained academics, what actually “pays the bills” at the end of the day and what you spent a sizeable amount of your time doing is teaching, not research (especially in the United States).\nYou wouldn’t think that was the case if you looked at what is required of most of us with academic backgrounds. The emphasis with academics is often to gain subject matter expertise in something where you become the expert. You take classes, read really thick books, take obscure examinations, and synthesize them all in order to come up with new ideas. Eventually you know so much about something that it’s sort of assumed you’re saturated with all the information you’ve soaked up like a sponge. All it takes to share that information is one good squeeze and all the information comes gushing out.\nAnd we’ve all witnessed this. We’ve sat in a classrooms where someone who “really know their stuff” gets up in front of a group of people and the information that’s inside of them just comes out every which way and you as the student are spending most of your effort trying to put all the pieces together to make sense of what is happening rather than absorbing the material yourself.\nYou might think to yourself, “I know this teacher really knows their stuff, why do I not just get it? What’s wrong with me?” Well the answer is that the skills that get you in a position of power to teach something very complex are often not the skills that make you a good teacher.\n The skills that get you in a position of power to teach something very complex are often not the skills that make you a good teacher!\n This is true in academia and over the past few months I’ve also found it’s also true in the world of data science and software engineering (and literally any other discipline in my opinion, anyone that’s taken music lessons from a fantastic player knows that their playing ability doesn’t always translate to teaching ability).\nBut why is this the case? Well in my experience, a huge part of this is that academics have never really been taught how to be an effective teacher to the extent they have for research. Literally five years as a graduate student and only in one semester did I take a class on teaching which was mostly designed to teach professional musicians who had to teach a music theory class so they can explain what an augmented sixth chord is.\nThe rest of my education on teaching comes from being on the receiving end of teaching. It consisted of making a big list of things I like and don’t like that my teachers did with the latter list being much longer than the former.\nI know the above is a bit heavy on the autobiographical side, but I’d bet many people who are reading this can relate.1 Many people find themselves in a position of having (probably wanting!) to teach, but not having nearly as many resources to help you be a great teacher as there are to be a great researcher. And if you’re going to teach, you should do it well. So what can you do?\nWell one thing that you could try to do, like anything else you want to get better at, is to get external training on becoming a better teacher.\nR Studio Instructor Certification In this post I want to reflect on my experience of completing the RStudio Instructor Certification program and just think out loud regarding the process for anyone else who also finds themselves in the boat of wanting to become a better teacher (especially those working in the world of technology!), but not really knowing an efficient way to go about this. I’m also not the only one who has written about this, if you want to see what other’s have said check out what Ted, Ari, Omayma or this interview have to say.\nIt’s Me!\n  Certification I first heard about the RStudio instructor certification back in August from Josiah Perry who mentioned it to me since at the time since he knew I was doing a few workshops in around London for Minerva Statistical Consulting and trying to grow that side hustle as I looked for full time employment.\nHe Was Right\n My initial reason for signing up for this course was on the RStudio website there is a big list of teachers that RStudio points people to who are looking to hire a trainer. They note the demand for teaching R is growing fast, so why not try to get on that list to say I’m ready and willing!\nSo how do you get on that list? Well it’s a multi-step process that you can read about on the website here but the summary is that you need to…\n Sign up for the class (there’s often a bit of a waiting period) Attend an ~8 hour course via Zoom Take a 90 minute teaching exam Take a 90 minute subject area exam in the tidyverse (you can do Shiny too!)  The course costs $500, which is pretty pricy and unfortunatly I couldn’t get my employer to cover it for reason that might soon be clear, but they do waivers if needed!\nBut many people reading this probably knew that and instead want to read more about the experience so they know what it’s like and if it’s for them. So let me reflect on each step in turn. Some will get more attention than others.\n Step One As for step one, I want to mention that I first reached out to Greg Wilson about the course in August, tried to sign up for a course in December, but because life happens I was not able to attend the remote sessions that are offered until March. I mention this only to note that as of now, the course is quite popular and if you are interested in participating, I suggest you sign on sooner rather than later. This of course might be different depending on when you read this, but did want to mention it.\n Step Two The first part of the certification requires an about eight hour course on evidence based teaching methods. As can be found on the slides for the course that are open sourced, the pitch of the whole course is to treat the attendees as if they were educational front line workers who need to know best practices for when you are in front of students, rather than take a more academic review of all things education research.\nThe vast majority of the course content comes from Greg Wilson’s Teaching Tech Together. The book is free to read online, but as a compulsive book buyer (and back in the day when I had 45 minutes on my commute I liked to sit down and read the actual book) I bought a copy to read in preparation for the course.2 You don’t have to read the book to take the course, but I wanted to get most out of the course and it’s a long wait between sign-up and the course, so why not best prepare?\nThe course is not just a rehash of the book, most of the content comes from the slides I linked to earlier. Now instead of just rehashing the course, I want to talk about some ideas I found valuable from the course to give you a taste of what you might dive deeper into if you do sign up.\nLearning Personas The first meaningful thing from the course came in the first lecture which was the idea of a learner persona. The main idea here is to get you to realise first and foremost that you are not your students. Writing a learner personal reminds you that you are going to teach real people who want to know real things, not just listen to how much you know.\nA lot of the time this is sort of done for us as teachers. We are assigned to teach course ABC202 with this material XYZ and can assume that students have taken pre-reqs ABC-101, 102, and maybe 104, but in the Wild West of weekend workshops, this cannot always be assumed.3 By realising you are not your students, the idea is that you are able to better scope the content of your materials and meet your learners where they are in order to give them what they want in the limited time you have with them.\n Concept Maps The second thing I really liked about the course was the idea of the concept map to help talk about learner’s different mental models. The idea here is to write down your thoughts on paper in a non-linear way because the order that you think of things (for reasons of expertise discussed in the course) is not always the best way to deliver that material to people who don’t know about it. You subsequently learn how to take these concept maps and turn them into lesson plans.\nI really got a lot out of this idea and have been using them in my own work. In addition to using this for lesson plans, you can see the drafts below of what I sketched out for the Science of Netflix event I did back in March. I have also started to use it for organising some of the academic papers I am still writing!\nOne of My Drafts\n What I found particularly helpful about concept maps is how there is not one “true” concept map for any topic and that the configuration of the ideas in your map really is in the eye of the beholder. The examples in the slides that I have taken directly from the training shown below offer two views of the organisation of a library: one from the vantage point of a patron and another the director.\nThese are two different perspectives on the same thing and you’d also imagine that these maps would differ based on two levels of expertise of a subject. A kid might have different bubbles and arrows compared to an eldery person explaining how a library works.\nPatron View\n Director View\n Of all the things in the course, concept maps are what I have thought about most in the weeks following.\nI’ve really spent a lot of time thinking about the idea that the metal model that you use to think about an idea does not have to be “exactly” how something does work . It sort of is related to Greg’s Rule of teaching Number 6 that it’s ok to sacrifice truth for clarity.\nThis really got me thinking about all things statistics and data science education. For example, how does sacrificing truth for clarity come into play when I teach p values? Part of me feels like the students need to understand everything I know or else I am doing them a disservice and part of me knows that hitting them over the head with all that at once is not going to be an effective teaching strategy in the long run.\nMy original draft of this had a giant detour on this, so maybe better to save this for another time…\nRegardless, I really feel the idea of concept maps in tandem with considering the state of a student’s mental model has been very helpful in explaining concepts to students the past month or so.\n Reproducible Lesson Planning The third major item I want to note that I found valuable cropped up across many of the sessions and that was the idea of how to do efficient, reproducible lesson planning. This is something that myself and the data science team at Flatiron in London have been thinking a lot about as many of us are teaching the same materials, all with our own idiosyncrasies, and all having to reinvent the wheel in order to make a lesson.\nPrior to a few months ago, there were not stock lessons that any data science teacher could just grab, spend a few minutes reading, and be ready to deliver the materials. It was always a matter of piecing together someone else’s scraps of a lesson then gluing their pieces together with your own understanding before putting the material in front of students.\nIn the past few months, we’ve gotten a bit better at this. For example some of our lessons are getting carved out to have everything people would want to just get up and teach it which includes:\n Observable Learning Objectives A Lesson Outline with Time Points Slides An interactive notebook with questions for various ability levels for skill differentiation List of questions for instructors to ask built into the deck that are formative in order to check for understanding An objective exit ticket at the end to check for summative understanding Lesson notes for other teachers who are picking up the lesson cold  Though the amount of effort to make a lesson with all of this is above and beyond some of the other resources I currently use that are probably good lessons, like this one here on this thing, you wouldn’t be able to tell from the materials alone because most of what is important about this is in my head (where it’s only of use to me at this point). It’ll be a bit more work to get that all cleand up and ready for someone else to use, but once that is done, I’ll probably save someone else a ton of time.\nThe last bonus thing I wanted to mention was some lateral learning I picked up on while participating in Greg’s Zoom sessions. The sessions themselves were masterclasses in online teaching. Throughout the two days I felt engaged and that if I tuned out, my lack of presence would be noticed. I felt like I was part of the learning experience.\nI’ve really tried to emulate this in my own online classes recently. If you find yourself online teaching (or will in Autumn) and want suggestions that work well, check out this recording and follow up that were posted around the time we all moved online two months ago.\nSo all in all, I learned a ton from this course. Of course I’m not so naive to think that an eight hour course has totally transformed my teaching and made me a master teacher, but I have been able to use much of what I learned right away and it’s made both mine and my student’s lives easier.\n  Testing After taking the course, you are then asked to complete your two exams within three months of the course. Scheduling your exams was super easy and will next reflect on those experiences.\nThe first exam you need to take is the teaching exam. If you have taught prior to this course and were paying attention and actively participating in the class, all that is really required of you here is to make sure you put in the time to prepare your materials well (just like a regular class). If you don’t have lots of experience doing this, I’d say just make sure to not have the first time you deliver your materials be at the exam…\nYou need to prepare a 15 minute lesson that demonstrates you understand the material from the course and can apply it, then you spend the rest of your 90 minutes on unprepared material that reflects your understanding of other aspects of the course.\nIf you’d like, you can check out some of the materials I prepared here but don’t take that as any sort of benchmark for good or bad. I just feel proud of what I did create and wanted to share it with others. My only tip here is that whatever you decide to teach on, remember you only have 15 minutes so make sure to not be too ambitious in your materials and practice it in front of your friends a few times even if you are comfortable teaching.\nI know that for as much as I do teach (and perform!) I still get super nervous when I know there is a pass/fail competent to what I am about to do. Though if doing a degree in music taught me anything it’s that if you’re nervous it’s often because you actually care about what you’re doing.\nThe tidyverse exam is separate from the teaching exam, needs to be scheduled at a separate time, and also is capped at 90 minutes. Of course I can’t say what is on the exam without compromising the integrity of the current test, but if you have a bit of psychometrician in you, you know that one aspect of any good test is that multiple versions of the same test should yield similar results. With that in mind, my big piece of advice here would just be to try an old version of the test and see how you do in 90 minutes.\nThe test is also totally open internet, you just can’t ask any friends for help. If you’re nervous about this, what I’d also say is in your practice test and actual test, just practice googling even stuff that you know. Even as someone that does tidyverse stuff all the time, I found that just committing myself to opening up R for Data Science or googling any problem I didn’t know like the back of my hand alleviated a lot of the burden of choice I know I would have in a test taking environment. Remember, if you’re nervous it just means that you care about it.\nIf you didn’t do as well as you’d like to have on your homemade practice test, then just slowly start to make your way through R for Data Science and stay active with all things Tidy Tuesday (you know you’re gonna do some ggplotting).\n What’s Next So with both the course and the tests under my belt, what’s next? Well it’s been about a month since I finished and the first thing on my plate has been trying to take all of our London materials and get it to the point so that others within Flatiron can pick up my lesson plans. I’ve also started to try to share a lot of what I have been learning with my colleagues. On a meta-level post, which you’ll probably see a lot more of on my Twitter feed, I don’t know exactly what is next for me in terms of education in tech, but am really hoping to apply everything that I have learned in future teaching!\n  I am also just dying to write anything and everything being in lockdown… so sorry if this gets rambly↩\n Though if you do teach in the world of technology/coding/data science, this is an excellent read!!↩\n Or anyone trying to get into data science↩\n   ","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588764290,"objectID":"631e22af2b15a9b8133c1fad77a3ea13","permalink":"/post/rstudio-instructor-certification-thoughts/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/post/rstudio-instructor-certification-thoughts/","section":"post","summary":"I have always tried to live by the adage that if you are going to do something, you should try to do it well. For the past eight or so months at Flatiron School, what I have been doing is teaching.","tags":[],"title":"RStudio Instructor Certification Thoughts","type":"post"},{"authors":null,"categories":null,"content":"   ","date":1587686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587686400,"objectID":"ee8815fd75534c0e2c6d1dfccbebc762","permalink":"/talk/science-of-netflix-fis/","publishdate":"2020-04-24T00:00:00Z","relpermalink":"/talk/science-of-netflix-fis/","section":"talk","summary":"A Public Outreach Event","tags":null,"title":"The Science of Netflix","type":"talk"},{"authors":["Dominique T. Vuvan","Ethan Simon","David John Baker","Elizabeth Monzingo","Emily M. Elliott"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"cb3a291b238103d66a3c542c1697f7ee","permalink":"/publication/memory-cognition-auditory-2020/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/memory-cognition-auditory-2020/","section":"publication","summary":"Previous research has examined the relationships among cognitive variables and musical training, but relatively less attention has addressed downstream effects of musical training on other psychological domains, such as aesthetic preference, and the potential impact of domain-general constructs, such as working memory. Accordingly, the present study sought to draw links between musical training, working memory capacity, and preference for musical complexity. Participants were assessed for their experience with musical training, their working-memory capacity, and their preference for musical complexity. Diverging from predictions based on vision research, our analyses revealed that musical training significantly mediated the association between working memory capacity and preference for music complexity. This significant mediation held even after a variety of sociodemographic variables (gender, education, socioeconomic status) were taken into account. Furthermore, the role of working memory capacity was domain general, such that the mediation was significant regardless of which measure of working memory capacity was used (tone, operation, or symmetry span). The current results develop a model of aesthetic preference that illuminates differences between vision and audition in terms of the multifaceted effects of complex skills training on cognition and affect. Moreover, they drive new work aimed at better understanding how domain-general constructs such as working memory capacity might interact with domain-specific cognition.","tags":null,"title":"Musical training mediates the relation between working memory capacity and preference for musical complexity","type":"publication"},{"authors":null,"categories":["data science"],"content":" We’re nearing the point of saturation when it comes to data science blogs. The amount of reccomended Medium, Towards Data Science, Analytics Vidhya, or whatever website you can post to articles that come up on my suggested reading on my phone is unending. Why? Because aspiring data scientists have repeatedly been told that one of the best ways to break into data science is to:\n START. A. BLOG.\n You, an aspiring member of the workforce, not only should be refining all your technical skills as you apply for jobs, brush up your LinkedIn profile, refactor all that code from that one project you still have yet to set to public on Githhub, but in addition to all that (and your normal life) should devote time to writing data science blogs.1\nThis is a lot to ask of someone applying for jobs.\nAnd as many of our students at Flatiron School have pointed out, there is not only a sea, but an ocean of data science blog posts out there. There are so many out there (of varying quality) and there seems to be such a push for aspiring data scientists to blog that one of our students joked (maybe not?) that their final project should be a bot that writes data science blog posts just to keep up appearances. In many ways, I couldn’t argue with them, it seems that this advice of starting a blog and blogging is pushed on people wanting to break into data science and what results is a collection of very OK posts when you click around on Medium. This influx of blogs from people-yet-to-get-hired also means that there are a lot of people who are probably under-qualfied to write about the tutorial they set out to detail, especially if it’s about stats. Hello Dunning-Kruger. In some ways, because there are so many posts out there, writing a bad blog post at this point would probably do you more harm than good.2\nBut as I thought about this AI Blog Bot that one of our students came up with and the sea of bad blogs out there, deep down in my teacher bones I felt that starting a blog is still a good idea.3 Like all thoughts, I couldn’t put my finger on it right away, but this post here is sets out to address the real reasons why in 2020 I STILL think people should be starting a blog if they are interested in breaking into the world of data science (or really whatever you’re into).\nThe “Point” of a Data Science Blog In the world of data science, especially at Flatiron School, when we first introduce the idea of blogging to students they think that the “point” of writing data science blogs is to disseminate some sort of technical information. For a newcomer to the field, this feels like a gargantuan task that almost validates someone’s impostor syndrome as they start to even think about topics in which they can address. How can a student, someone who is just getting to grips with stats, programming, and machine learning have anything of substance to contribute to that sea of Medium posts? In many ways, I think anyone new to the social media (Twitter, blogs) has a valid point in this regard.\nWe’re at a point now where knowledge is becoming so democratized that so many good resources exist that even as someone with a PhD with lots of teaching experience, I feel like I can’t compete with the sea of free books, blog posts, documentation, and YouTube videos out there in terms of making new content.4\nBut to think that the “point” of writing a data science blog post on a technical topic is to compete with someone like 3blue1Brown explaining eigenvalues would be to make one of the classic blunders and assume the “point” of writing a blog is that they are contributing the a giant ocean of knowledge that is “data science”, hoping to have their dribble “counts” as a meaningful drop to that big ocean.\nIn my opinion, on a deeper level, it’s more about writing blog posts that are about yourself and how you relate to the community of people that you eventually would like to be a member of. This is important because “data science” is not some big monolith, but rather a community of people. It shouldn’t be reified like a “thing” or some established part of the academy (let’s not forget how old data science is). For me, it’s better thought of as a big network of people, all with various understandings of what data science is or even can be. It’s about the people and you as a person.\nIt might not feel like that at first, especially with all the gate keeping that seems to perpetually exist, but people who make up data science are literally just other people. This might seem painfully obvious, but if you think about this from a social media perspective, all things social media (blogs, Twitter, LinkedIn) provide direct access for you to join said community. In my opinion, blogging without some sort of humanistic intent to make a connection with another person is just an exercise in bureaucracy. It’s something a lizard person would do.\n Blogging and Social Media Let me expand a bit. Social media and the online presence thing really never “clicked” for me until I started just interacting with other people and started getting reactions from “famous” people. I still remember when I watched The Inturrupters when I was like 22 and I tweeted about how much I liked the documentary and one of the people central to the documentary– Gary Slutkin – not only liked the tweet, but commented back.\nIt blew my mind that for a fleeting moment I had some sort of interaction with someone who I had no connection to. And this really hasn’t gone away. A few years later I remembering feeling like a nerd when Hadley Wickhamn liked one of my stupid tweets. I even screenshotted it for my collection.\nOf course not every interaction is with someone that is “twitter famous”, the most useful connections I have had on Twitter were throughout my Ph.D. finding so many people that share a common interest (what lead to the #musicscience hash tag!!). These connections have lead to things that I value (relationships with other people with shared intersts) and have also led to things of value (getting offers for data science gigs based on some of my data science posts) because I was able to start some sort of relationship.\nThe point I am trying to make here is not that it feels nice to get people to like your memes,5 but that social media allows everyone the chance to start a relationship with someone before you actually have a relationship with them. By blogging and having a bit of a social media presence, you are able to kind of “front load” what that relationship could be.\nWhat do I mean by this? What does does this actually look like in practice?\n The Actual Point In my opinion the point of blogs or front-loading the relationship can take one of two different forms.\nThe first, which I tend to dub the “Twitter approach”, is to start building up your relationship before you actually meet someone face to face. Coming from an academic background, this is mostly how I’ve used social media. The name of the game is to just be you and rub shoulders with people who you know you will inevitably meet so that when you do meet someone face to face, you have some sort of idea of who they are as a person so you can start your real relationship sooner and use the finite time you have together (normally at a conference) to then form a meaningful connection.\nIf you adopt this mentality, then the act of “networking” (a term dreaded by most introverted academics who tell us how introverted they are) can become what it should be which is just making more friends.\nOf course it’s not so easy as to just start tweeting, build a huge following, and then profit. But I figure if you’re reading this blog, you’re already half way interested in this process. You have to start slowly, just ‘Like’ weird stuff that the chronic posters post, and then get to know what people are like, and learn what people value. A lot of the times it will be memes. Some of time there will be a dumpster fire. But there is a lot to be learned just watching it all pass by. Eventually you’ll find your niche and the whole “twitter for networking thing” will start to make more sense.\nThe second way, which sort of seems more suited for blogging (but of course they interact!), happens the other way around where you meet someone in person (or maybe on Twitter) and you’ve already built up a bit of a small online presence via your twitter feed or a blog. This might happen at a hackathon, or a conference, or even a non-work party.\nIt might happen something like this:\nYou find yourself in a conversation with someone and realize that you have similar interests. Maybe the person you’re talking to is part of a data science team somewhere and you’ve both attended the same meet up. Maybe they have a position open and are looking to hire a new data scientist. You bring up that you fancy yourself a bit of a data scientist.\nWho you’ve just met remains skeptical. And they should. Everyone wants to be a data scientist. And to make matters worse everyone who wants to be a data scientist seems to have some sort of lame blog (written by a data scientist who has a blog). The thing is, if you’re actually out there in the real world having a chat and enjoying yourself with someone you’ve done most of the hard work in the world of networking. You’ve convinced someone that you are someone who the other person could imagine spending a lot of time with. What do you do then?\nWell you might mention you are looking for a job or to switch jobs and have been writing blog posts not to add to the big bureaucratic pool of data science blog posts, but because you have a certain amount of expertise that you want to share and like linking your weird interest in your area of expertise (in my case, music theory) to the world of programming.\nWhen you depart, things are now out of your hands. That’s totally fine, but if the person liked you, you know they are going to social media creep you. If they find you on the web and like what they see, they can fill in the professional scaffolding of the person that they met or know. Of course this is no guarantee that you would always get a job, but having something out there like a blog, an active twitter feed, or your Github profile is some sort of tangible evidence that you were who you said you were. In the case of the blog, if you have written quality content for where you are in your career demonstrating you know what you know, have some idea of what you don’t know, and can articulate that clearly, you can really get yourself recognized.\nIn both cases, the end result here is to to help other people get to know who you are with the help of the internet. If your goal is to land some sort of professional relationship with someone, this social media presence allows the person that will end up working with you a brief glimpse into what that relationship is going to be.\n A Blog Warning Now I don’t think you don’t HAVE to blog to become a data scientist. I could write tons more about the many ways this practice/culture could go wrong. In my opinion there is a little too much of an emphasis on the blog thing.\nWhat if someone is not a great writer because English is their second language (assuming an English speaking workplace)? What if someone doesn’t have the luxury of doing their whole life AND blogging? (It takes a lot of time…) What if people just don’t like to write? What if people hate social media?\nThese are all valid questions that should sit in the back of someone’s mind.\nBut if you are interested in blogging and you think that you would enjoy sharing, why not give it a shot? Your first blogs might be lame, some of mine were/are. Eve this one is kind of lame. It’s a blog about blogs to just get me back in the swing of writing. It won’t be the contribution that everyone remembers me by (hopefully) but might help out a few people.\nYou might be really surprised about what good can come from it. I’ve been told that two of the positions that I’ve worked for recently, that people have read my blog before and that it positively influenced a hiring choice. We have numerous stories about blogs helping people get hired from my work at Flatiron School. I’ve also been offered some freelance gigs. Those would not have happened if I just had pushed some code to my Github.\nI will end by saying that you do decide to start a blog, it does take a long time. At first you will think you will blog a ton. That will eventually die down and that’s OK. Creativity burnout is real and life happens. Luckily a lot of content creators are very open about this and help normalize this. You should just blog when you want to or can or have something to say. At this point I think of mine as more like a small outlet for myself to express myself and a vehicle to help others given the experiences that I have had.\nI’ll end with some advice that I have been giving to some students that are on their blog game now, just so this isn’t a total ramble.\n Write blogs that only you can can write. (My default answer when people ask if they should write about an introduction to supervised versus unsupervised machine learning.) Write the kind of blog post you would want to read. (My default answer when people ask me how long they should be.) Literally write down the people you would want to read your blog at the top of your page when you’re drafting the post and write the post to them, erase their names when you’re done Cite your sources. Tweet/Post on LinkedIn about it when you’re done writing. Don’t take them too seriously. Demonstrate you know what you know and have a vague idea of what you don’t know Try to make other people feel smarter or more empowered to do something after they have read your blog. Everyone likes pictures. Include code people can easily copy and paste to try on their own. Don’t start too much drama, a little is kind of fun.  And lastly, if you you struggle to write, that’s ok. Writing is very hard. If you find it difficult, try to stick to some sort of template. If there’s any interest in sharing the materials we use at Flatiron School to help people get started, I’d be more than happy to for a future post. This one is now too long.\nSo, maybe start a blog in 2020, but take some time to really think about why you are doing it.\n  I assume if you clicked on the title you’re here for that reason…↩\n It probably should go without saying, but if you write factually incorrect information on your blog, you are going to have a lot of explaining to do…↩\n Also being tainted by a lot of suviorship bias↩\n The secret is you don’t have ot compete with it↩\n Though always very nice↩\n   ","date":1581033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581033600,"objectID":"e1049c6e0eecdacb74a830f62af0224a","permalink":"/post/blog-on-blogs/","publishdate":"2020-02-07T00:00:00Z","relpermalink":"/post/blog-on-blogs/","section":"post","summary":"We’re nearing the point of saturation when it comes to data science blogs. The amount of reccomended Medium, Towards Data Science, Analytics Vidhya, or whatever website you can post to articles that come up on my suggested reading on my phone is unending.","tags":[],"title":"Blog on (Data Science) Blogs","type":"post"},{"authors":[],"categories":["service","public musicology"],"content":"In addition to all the academic and professional things that you should do as an early career academic, I also am really interested in finding ways to communicate what I do to audiences beyond the Ivory Tower. A lot of this comes in the form of tweets and blogs, but here are some other projects I have collaborated on that I am particularly proud of.\nThe Science of Netflix   Collaboration with 12Tone Following an academic talk I did on the idea of measuring musicianship, I collaborated with 12tone to help produce a video on the same subject.\n  Interview on So Strangely   Podcast appearance on So Strangely Talking about Scale Degree Qualia  ","date":1570635508,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570635508,"objectID":"1420b7b2bd6cd3dff153b51848108d6b","permalink":"/my-projects/public/","publishdate":"2019-10-09T08:38:28-07:00","relpermalink":"/my-projects/public/","section":"my-projects","summary":"Helping Others","tags":["service"],"title":"Public Musicology","type":"my-projects"},{"authors":[],"categories":["cognition","replication","open science","psychology"],"content":"Here I talk about importance of replication in field of music psychology. Important for field. Important for helping train future students. Important for recrcling on old ideas in order to get new ideas.\n","date":1570635508,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570635508,"objectID":"a8b1f5bda0a5882e3313a92b2d89939a","permalink":"/my-projects/replication/","publishdate":"2019-10-09T08:38:28-07:00","relpermalink":"/my-projects/replication/","section":"my-projects","summary":"Collaborations to Replicate Findings","tags":["cognition"],"title":"Replication in Music Psychology","type":"my-projects"},{"authors":[],"categories":["service"],"content":"SMPC Student Representative  Things I did there Others?  Toynbee Hall  Reports I was a part of Hackathon Speaker at EARL London  ","date":1570635508,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570635508,"objectID":"1d801e0c493e7da244a58ea028aba929","permalink":"/my-projects/service/","publishdate":"2019-10-09T08:38:28-07:00","relpermalink":"/my-projects/service/","section":"my-projects","summary":"Helping Others","tags":["service"],"title":"Service","type":"my-projects"},{"authors":null,"categories":[],"content":" It’s been two months since I’ve last blogged! In the time since SMPC, I’ve done a fair bit of traveling and officially closed the graduate student chapter in my life. Most importantly, I ended up going back for my Ph.D. graduation back in Baton Rouge. I know a lot of people don’t walk and see graduation as kind of just another prom or wedding industry (it is, don’t get me wrong), but given that I have spent the majority of my 20s pursuing higher education, I wanted to fully experience the ritual that was its ending. I do not regret going at all. Below is a picture of me and the dedicatees of my dissertation.\nIt feels very good to be done. If you want to check out my dissertation (proof that I’m actually done) you can find it here and as I submit more “things” from it and post those pre-prints, I will blog more about it all.1\nEast Coast Living Post graduation, I flew up to New York City for both business and leisure. SMPC was a great success, I gave a small talk at CogMIR, and also met with some of my music industry friends. I got a quick tour of the Pandora office in New York City from Steve Keller and also got to meet some of the people I’ve done a bit of work with at Veritonic in person.\nIn addition to work, I got to catch up with some old friends from BW I hadn’t seen in a few years and did a small weekend trip out of the city. I also got to enjoy what it’s like to not be experiencing constant sound. I even saw a bald eagle fly over a river while the sun was setting.\n London Since returning, I have been settling back into life in London. Most of this has basically meant trying to sort out what my future will look like which is no small task and I elaborate on a bit below.\nI finished up some of my last “official” obligations with my volunteer position at Toynbee Hall sharing some of the work that I did at the London EARL Conference.\nI also took my fourth ever holiday (ever) of my adult life where the travel didn’t also include something having to do with music theory, music science, or data science. One of my best friends from Baldwin Wallace is currently the trumpet/guitar player for Kooza and the show was in Malaga for a couple of weeks so I went down to just sit on the beach and drink beer and watch people jump around on giant spinning wheels.\nGoing on that holiday made me realize I deeply, deeply regret not taking more breaks when I was a graduate student. My CV would have looked probably exactly the same, but I would have been far less depressed than I was most of the time.\n New Chapter My identity as a graduate student is now over. I know this because I now feel bad flexing my student ID card for discounts having changed a few of my titles from Mr. to Dr. on some forms (but never for a plane ride!).\nSo what’s next? I could just bullet point it out, but I guess most of the people that read my blog read it for the bigger picture and not just “the facts”.\nThat said, I think it’s important to tell people things don’t always go as planned.\nI’m not starting that sweet tenure track academic position this year or have a giant flow chart to show you how many of those jobs I applied to. In reality, I didn’t even apply to that many jobs (20? 30?) compared to some of my peers (some of which literally have sent off over 150 cover letters). I didn’t get even a single phone interview for any permanent position. Maybe there was something just wrong with my application materials? Who knows. There’s always next time.\nI did have some success and was offered a position that would have required immediate upheaval of my life (I was told moving back the start date was not possible) and required me to again go long distance with my current partner until she would basically just be pressured into following me for a temporary (though very nice) position. But the stars just were not aligned, it was difficult to pass on, I think about it a lot, and think that it’s worth mentioning to know that it’s OK to not always make the decision that is best for your “career”.\n Career Diversity In Real Time So… where to go from there?\nWell the plan a few months ago was to try to do the freelance data thing part time (put those research chops to use!) while chipping away at Mount Datamore from my dissertation and getting that all published. This was going to work until my main freelance gig fell through, which I was pretty gutted about.\nAnd not only from a professional standpoint, but financially as well. I had planned a whole summer of travel (Music Theory Midwest, up to Columbus, California for Music Theory Pedagogy, SMPC) to keep the career continuity going (I was accepted after all!?). Though the problem with having defended your dissertation and graduated is that no longer being in an academic position, you end up having to foot the bill yourself.\nHaving done the math at the start of summer, got all my flights out of major airports, not pay for any accommodation by staying with friends, not splurging too much, things were pricey, but I justified it as a professional treat to myself post grad school (hello, Stockholm syndrome!). But with the start of that freelance gig being moved back and back over and over again, it just ended up that I had to foot the bill to this myself (thank you, credit card!).\nI could try to establish all the blame for this on January’s government shutdown since the gig was going to be through the US government, but really it’s just a hard life lesson learned. My pathological need to be accommodating and low valuation of what I am worth ($$$$) lead to me thinking things would just work out eventually. It was a mistake. In the future, if someone wants to hire you and offers you money, the best thing is to get it on paper and be put on some retainer or something. I’ll just blame it on not having taken any “Business for Music Theorists” classes.\nSo I could lament about this, or regret about passing on an earlier offer, but if I’ve learned anything in graduate school it’s that things just never work out as planned. And that’s OK. In many ways one of the best skills that I learned as a graduate student was how to let go and to occasionally take off the horse blinders. You never know when other opportunities will arise. And I think that’s important to say especially as someone who advocates for ways to celebrate career diversity.\nRereading these past few paragraphs, I also have a better understanding of why I have been feeling a bit blue the past few months. It has not been that fun in general. The pictures from this post are not randomly sampled. Maybe a future post can dig a bit deeper into some of the less nice feelings that come along with doing a Ph.D. that never really make it to my social media presence.\nBut this whole post isn’t supposed to be totally grim.\n Reconstructing the Box Of course I wasn’t a total fool when things started to not look so great in terms of future employment. When work started to smell funny, I reached out to the many people I’ve tried to make friends with the past few years2 and tried to find new ways to combine old skills. Some of this work came in the form of doing some projects in the world of audio branding and some came from combining my love of teaching with my love of R and doing some weekend teaching for a small data company here in London developing weekend courses that looked to teach people with no programming experience the basics of R over an 8 hour course.\nOver the past few months, I also got better at skimming job boards (other than MTO) and I’d try to put in keywords like music and psychology and data science. Unsurprisingly the perfect Music Theory + Music Cognition + Data Science job didn’t seem to ever come up, but when I started to get a bit more creative, I started to think of other ways of what I learned in my Ph.D. could be put to good use.\nIf I just let go of music theory a bit in the name of opening up what might be available, I tried to think about what I had that might distinguish me from a typical data person and one thing that came to mind right away is how much experience I have had public speaking and teaching. This might seem totally obvious for most academics, but having now seen some others do the public speaking thing (sorry to have to put that in the negative) I felt a lot more… confident in my abilities.\nSo I started also adding teaching, education, and instruction to my searches. In some ways this really opened my imagination of what I thought might be possible.\nOne job that caught my eye was one as an instructor at a coding boot camp. I remember staring at it for a while before even pressing apply, but the more I thought about it, the more it kind of made sense. Over the past few years in graduate school I have sort of (un)knowingly been cultivating the skills required of most data scientists (and have done some data science side projects). Because of my work in computational musicology, I learned a decent amount of programming. Because of my work in music cognition, I have learned more that I ever thought I would about statistics and experimental design. And if there is anything that any music degree (especially a Theory degree) teaches you is the ability to just sit for hours on end and read something until you understand it (and whatever reading you have to read in order to understand what you are actually trying to read).\nIf you consider this skillset in tandem with experiences teaching both small labs (aural skills), large lectures (fundamentals of music theory), and my time as a teaching assistant for all the intermediate and multivariate statistics courses required of doctoral students in the psych department, everything kind of just made sense.\nAll of the requirements were there, I just had never thought about them in that way.\nAlthough this wasn’t exactly the job I thought I’d be doing straight out of my Ph.D., it’s a great example of how some constellation of data can be explained by more than one generating system. I ended up applying for this job, had three official rounds of interviews (all of which my Ph.D. research prepared me for) and was offered a position! I’m now one of the Lead Instructors of Data Science at Flatiron School.\n Flatiron Future All that said, next Monday will be my first day in a few years not doing the stay-at-home-academic thing, but I’m quite excited reflecting on it all. I will get to be back in front of a classroom (which I love!), have a say in curricula development, dive deeper into programming3, machine learning, and have a much better understanding of it all since I will be teaching it. Not only that, but I’m also looking forward to making more connections with people which will in turn allow me to help others out in their career trajectory (students and peers) all while getting a first row seat to learn more about education in a non-academic setting. I’m pre-registering my prediction that I will have even more opinions in the coming months about the “purpose” of education.\nOf course this does not mean that I will be abandoning all my research and “giving up on academia”.4 I love what I do and the questions I’ve been asking the past couple of years and am VERY excited to get all this work from my thesis published soon. I’ve got mountains of data and some three quarter baked manuscripts all scheduled for submission in the next few months.\nAll in all I just wanted to air out some more feelings for everyone to read. Blogging/writing really helps me fit a narrative to my life (especially with my current life that has been devoid of structure recently) and am really glad that anyone has gotten this far down in the post. I hope others find my sharing helpful.\n  I don’t want to give myself the satisfaction of writing a blog post about the work having not sent it off to a journal… though I probably should write a “WTF did I even just write blog post”…↩\n The lizard people among us would call this “networking”↩\n I’m finally going to be R and Python bilingual!!↩\n I will seriously punch the next person that says I am “transitioning” “out” of academia.↩\n   ","date":1569283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569283200,"objectID":"98e7ac06f1d697ee61ee8631a09d87d7","permalink":"/post/prelude-to-a-career/","publishdate":"2019-09-24T00:00:00Z","relpermalink":"/post/prelude-to-a-career/","section":"post","summary":"It’s been two months since I’ve last blogged! In the time since SMPC, I’ve done a fair bit of traveling and officially closed the graduate student chapter in my life.","tags":[],"title":"Prelude to a Career","type":"post"},{"authors":null,"categories":null,"content":"","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565222400,"objectID":"ede7fc03f552a86bb52ef215a3b47d89","permalink":"/talk/cogmir-2019/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/talk/cogmir-2019/","section":"talk","summary":"Which computational measures best predict how many notes someone can hold in memory? We present both evidence from a newly encoded corpus of over 783 sight singing melodies and a currently ongoing experiment (N = 11, Stopping = 75 ) in order to answer this question. Using a within-subjects design and musical series recall task using stimuli taken from our corpus with trained musicians, this paper will attempt to explain the results from the behavioral experiment using various models that have been put forward by both the computational musicology and cognitive psychology literature. Models we include range from number of items/notes (Miller, 1956; Tallarico, 1974; Long, 1977; Pembrook, 1983; Li, Cowan, Saults, 2012) , to an n-grams frequency distribution (Huron, 2006), to information content measures derived from multiple viewpoint models of music perception (Pearce, 2005; Witten and Conklin, 1995), sensory models of musical perception (Leman, 2000; Milne, Laney and Sharp, 2015), to static symbolic features (Müllensifen, 2009). We fit multiple models to our data predicting both accuracy and reaction time then discuss our results in terms of plausible cognitive explanations and discuss how insights from cognitive psychology can lead to more meaningful and robust models of musical memory.","tags":null,"title":"Modeling the Limits of Musical Memory","type":"talk"},{"authors":null,"categories":null,"content":"","date":1565049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565049600,"objectID":"73130771b512dcc0a7b242a1f2e4a26f","permalink":"/talk/smpc-openscience-2019/","publishdate":"2019-08-06T00:00:00Z","relpermalink":"/talk/smpc-openscience-2019/","section":"talk","summary":"The proposed session will provide an overview of the principles and practices of open science as they apply to the SMPC community. The session will begin with an introduction to open science (10 minutes). This introduction will cover the principles of open science (accessibility, transparency, accountability), the practices that arise from the application of those principles (pre-registration, data and code sharing, open access publication), and some of the common arguments for and against open science. After this introduction, we will present the life cycle of an open science project through case studies describing pre-registration, data sharing, code sharing, and open access publication from actual work conducted by members of SMPC (60 minutes). Next, we will discuss the impacts of open scientific practices on academic careers in music science (10 minutes). Finally, we will close with a Q\u0026A period that allows participants to ask questions and discuss session content (30 minutes). The goal of the proposed session is to facilitate discussion of open scientific practices in the SMPC community, and to provide the templates and tools necessary for adoption. We believe that such an effort will advance our field by fostering a rigorous and collaborative spirit of inquiry in SMPC.","tags":null,"title":"Symposium on Open Science","type":"talk"},{"authors":null,"categories":null,"content":"","date":1564963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564963200,"objectID":"4aa3c1a191c1af667683ee000803c132","permalink":"/talk/smpc-comp-2019/","publishdate":"2019-08-05T00:00:00Z","relpermalink":"/talk/smpc-comp-2019/","section":"talk","summary":"Teaching melodic dictation-- the process of hearing a melody then noting it-- involves instructing students on what and where to direct their attention in order to improve their abilities. As students’ experience increases, they are able to memorize larger chunks of music and can dictate melodies they once found difficult. But what is happening in the student’s mind over the course of aural skills instructions that allows for this growth? This research puts forward a computational, cognitive model of melodic dictation with the goal positing a falsifiable theory on how students improve at melodic dictation. The model is based in research from both cognitive psychology (Cowan, 2011) and computational musicology (Pearce, 2018) and incorporates relevant theoretical aspects such as working memory (Chenette, 2019; VanHandel et. al 2011) and the structure of the melody itself. The model consists of three main modules: Prior Knowledge, Selective Attention, and Transcription. First, the model is trained on a corpus of melodies using a computational model of auditory cognition (Pearce, 2018) that derives measures of expectancy based on prior listening experience. Second, the melody is “heard” by the computer and the incoming music is chunked based on the information content of the melody. Third, the model searches for a match within the Prior Knowledge and if found, the contents of Selective Attention are successfully notated. If not, the model truncates the chunk and recursively repeats the process. The model outputs a difficulty rating of the melody relative to the Prior Knowledge and also make several testable predictions about how melodies are learned. Presenting a computational model additionally demonstrates every ontological commitment, thus making it completely amenable to criticism. This research directly address the recurring call (Butler, 1997; Klonoski, 2006; Karpinski, 2000) to address the chasm in research between music cognition and music theory pedagogy.","tags":null,"title":"What is happening in a student’s mind when they perform melodic dictation?","type":"talk"},{"authors":["D Baker"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"94b6c1d0bf86c550f82e3fcdf6ff64f8","permalink":"/publication/mmd-2019/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/mmd-2019/","section":"publication","summary":"Melodic dictation is a cognitively demanding process that requires students to hear a melody, then without any access to an external reference, transcribe the melody within a limited time frame. Despite its ubiquity in curricula within School of Music settings, exactly how an individual learns a melody is not well understood. This dissertation aims to fill the gap in the literature between aural skills practitioners and music psychologists in order to reach conclusions that can be applied systematically in pedagogical contexts. In order to do this, I synthesize literature from music theory, music psychology, and music education in order to demonstrate how tools from both cognitive psychology as well as computational musicology can be used to help inform pedagogical practices. In the second chapter, I discuss factors that might play a role in a student's ability to take melodic dictation and put forward a taxonomy of factors that are assumed to contribute to an individual’s ability to take melodic dictation. The third chapter of the dissertation investigates individual factors that are theorized to contribute to melodic dictation using a cross-sectional experimental design. The chapter corroborates claims on the importance of understanding individual differences in working memory capacity in research on melodic dictation. The fourth chapter discusses how aural skills pedagogy can incorporate methodologies from computational musicology in order to inform teaching practice. In my fifth chapter, I introduce the MeloSol corpus, a new collection of 783 digitized melodies encoded in the **kern format. In the sixth chapter, I synthesize the previous research in a melodic dictation experiment to show how using robust statistical methods can be used model melodic dictation. Finally, in my seventh chapter, I introduce a computational, cognitive model of melodic dictation with the goal of helping explain how students improve at melodic dictation. I demonstrate how modeling the cognitive decision process during melodic dictation helps provide a precise framework for pedagogues to understand student’s inner cognition during melodic dictation and can help inform teaching practice.","tags":null,"title":"Modeling Melodic Dictation","type":"publication"},{"authors":null,"categories":[],"content":" Next week is SMPC!!! I’m super excited for it (as always), but this year I’m justifiably a bit more excited because this will be my first experience going to a conference as a member of the Executive Board representing the student members of the Society for Music Perception and Cognition. I’ve technically spent the last six years as a graduate student1 and even though I’ve just recently shed my graduate student status and will emerge this coming weekend from my chrysalis as beautiful doctoral butterfly, the feelings of being a grad student larva are still very fresh.\nSo given six years of knowing what it’s like to be a grad student, I really wanted to take all of what I have learned and try to incorporate it all by both carrying on the traditions that former student reps have done in the past and try to put a little bit of my own spin on things.\nSo what does this mean in practice?\nWell as any professional society, we can say that we value X,Y, or Z, but at the end of the day we have to put our money where our mouth is. So part of this post is talking about what those things are so they don’t go unnoticed (also for any of my SMT or AMS readers where I’m pretty sure some of these types of things might go over well at those annual conferences) and the other part is just a big advertisement on my little blog about what’s going to be available for students.\nThe Main (Student) Events Even though we only have three official days together, there are going to be four events that are special events that are directed towards the student members (who according to my last account make up a little over 50% of people registering for the conference!!).\nThe four events are:\n Day One: Meet and Greet with Human Bingo Day Two: Early Career Advice Forum Day Three: Applying to Graduate School Forum The old-as-time-itself2 Faculty-Student lunch  Meet and Greet The first event, timed accordingly, is going to the graduate student meet-and-greet. When I first ran the idea on Twitter, there seemed to be a positive response from not only students, but faculty that were interested in getting to know other people. At a conference where you have people coming from many perspectives on what it means to study music ranging from neuroscience, to music theory, to psychology, to music information retrieval, it’s important for attendees to be able to talk to other researchers. At the time of the Twitter conversation in early July, Blair and Brian suggested a human bingo as a way to help facilitate this.\nI’ll be printing out the this weekend and will be distributing them post-Monday’s keynote. Anyone is welcome to grab them and play in that middle awkward period between the talk and the reception (see image) and will hopefully get people talking. If you have some last minute ideas of blanks to add, put them here!\nA little into the reception, we’ll be pulling some of the grad students to the side so everyone can have a chat with one another. What I’m hoping for here is just a lot of “Hello, you’re interested in music research?! That’s crazy! I’m also interested in music research!?” conversations happening. Hopefully there will be a lot of small hellos and then as the night progresses there will be some nice integration between schools, departments, and labs.\nOne thing that is tricky that was pointed out by Dom is that in events like this is that activities like human bingo or just letting people roam free in a giant room really favors people who can flex their extroversion muscles. For this reason, I really tried to add questions to the human bingo that allows people further out on the orbit of socializing to be sought out and valued via their status as newcomers with bingo spaces such as “First time at conference” or for the more intrepid “Came to SMPC solo”.\nBut this of course doesn’t still ensure that the less boisterous people will still feel engaged. And one thing that is also tough about all of this is that the whole thing is very fleeting in that sometimes people (at least me) feel that if they don’t make the connections at the time, those opportunities are lost until next time. To ameliorate this problem, I’ve really tried to push the whole Twitter thing so people can at least connect passively and watch things happen until they feel a bit more inclined to strike up a conversation. I also have some Google docs that we will be using for the next two sessions that will allow some sort of conference trace to be left behind so that if someone decides go to event A instead of event B that they can still be somewhat aware of what went on.\n Early Career Reserach Forum The next two events we have here are forum events that have now been a part of SMPC for the past few years. From what I have gathered from previous student reps, the Early Career session was created first and in its first version a lot of the question time was taken up with many of the even career-younger people asking about applying to grad school, so these sessions were both split.\nThis year’s Early Career Advice panel consists of four members and hopefully covers a diverse array of opinions and advice that could be offered. This year we have Caitlyn Trevor who just completed her Ph.D. in Music Theory from Ohio State University and is now living in Europe on a Marie Curie Postdocotral fellowship. We also have Psyche Loui, a psychology and neuroscience researcher who is an assistant professor at Northeastern. There’s also Brian McFee, an assistant professor at NYU who is cross appointed in music as well as data science. Then last but not least there is Steve Keller who just recently was appointed as the Sonic Strategy Directory at Pandora.\nHopefully this permutation of people covers all the possiblities that someone might have questions about and also hopefully people will mention that the panel is not only traditional, senior, tenure track academics bestowing knowledge to students two academic generations removed. At this point I am not sure exactly how the conversation will go, but I have created a Google doc accessible here where people can write some questions ahead of time so we can see what kind of topics will be popular and for those who don’t want to ask a big question in a large room can feel more comfortable.\nI’m imagining this whole thing wanting to stretch longer than the 45 minutes we have at lunch, but I will be encouraging everyone to at least link up on Twitter or later via email if they need an “in” to get a question answered they did not have. Hopefully questions from this session can then go to a bigger FAQ that the SMPC website could hold so we don’t have to re-invent the wheel every year with this.\n Applying to Grad School Similar in many respects, but just earlier in the typical pipeline of people who attend SMPC, is the Applying to Grad School session. This session was a bit tricky to organize in order to make sure I covered all my bases, but we’ll see.\nIn the early career session we have Daniel Shanahan, who has just recently started a position at Ohio State University in a Theory department (also my Ph.D adviser!); Ed Large a former SMPC president, associate professor and head of UCONN lab where he does neuroscience; Hayley Kragness a post-doc in Canada at The TEMPO Lab; and last but certainly not least is Kelly Jakubowski who has experiences with UK Post-graduate system where she did both her MSc and PhD in Psychology and now runs the new lab at Durham where they will be having a new music psychology program come 2020.\nYou’ll notice that this list hopefully again strikes a balance between state and private schools, regions (America, Canada, Europe), seniority, as well as discipline. Again, I hope given this krewe, there will not be a question where they can’t answer or at least be able to point someone in the right direction of who to talk to. The same thing goes with having a google doc to list out questions before for similar reasons as above.\n Lunch Lastly, we have the Faculty-Lunch session. At this point we’ve had both faculty and students sign up for this event and it’ll give students a chance to meet one-on-one with someone they may or may not have gotten to chat to face to face. I’ve always thought this would be something nice for SMT to adopt, just throwing that out there.\nAs an aside, The student faculty lunch was always something I looked forward to and hope that all the faculty take it quite seriously as things that they say and pass on can have quite the impression on greener researchers. I specifically remember getting tacos in Nashville before starting my Ph.D. and bringing up my concern I had about doing a Ph.D. in Music Theory having come straight from a Psychology Master’s and I was worrying about being an interdisciplinary researcher (aka not being taken seriously by either theorists for doing psychology or by psychologists for doing music theory..). I remember my faculty mentor saying that you don’t really need a specific set of coursework to be a good music psychology researcher. What you do need is to read all the papers of what you are interested in, learn all of the techniques, and then engage in a meaningful way with the literature. They noted their Ph.D. was not in Psychology and pointed out that really no-one historically has ever got a Ph.D. IN music science and that I should just follow the questions I want to ask. I wouldn’t be that surprised if they doesn’t even remember saying this to me, but it really left a mark on how I thought about research in general during my Ph.D. I can only hope that other students get similar experiences.\n Future So going into the conference, I have no idea how any of this will land. This whole post was sort of just a pre-registration of how I think it will go, but no telling how it all will turn out! They could all be catastrophic failures, but they also have the chance of being very helpful to everyone involved.\nAnd lastly for anyone getting this far in the post, it’s worth mentioning that this year SMPC has a new code of conduct, so if you see anyone being insufferable towards students, especially during a presentation, please say something and report that. Also, don’t be that guy. Part of me wants to say “BuT I cOulD neVeR iMagIne tHaT haPPenIng At SmPcCcC”, but I’m not that naive.\nI just want every student to leave SMPC as I have the past few years, super inspired to do great work, new contacts to ask questions of on the stuff you just don’t understand, and with more friends than you had prior to the conference.\nSee ya’ll soon!\n   year of MSc, year being an RA, then four in the PhD…↩\n Just kidding, I just have no idea what the first year this was done?↩\n   ","date":1564531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564531200,"objectID":"0be53da1ce877d0016ff3a1f16ea1393","permalink":"/post/smpc-for-students/","publishdate":"2019-07-31T00:00:00Z","relpermalink":"/post/smpc-for-students/","section":"post","summary":"Next week is SMPC!!! I’m super excited for it (as always), but this year I’m justifiably a bit more excited because this will be my first experience going to a conference as a member of the Executive Board representing the student members of the Society for Music Perception and Cognition.","tags":[],"title":"SMPC for Students","type":"post"},{"authors":null,"categories":[],"content":" I read somewhere on the internet that if more than three people email you with the same question, it’s worth writing your answer up in a blog post. Over the the course of my Ph.D I’ve gotten a few questions from younger students ranging from MCCL@LSU lab members to complete internet strangers (mostly because of a very old blog post I did for MajoringInMusic.com ) about pursing music cognition at the master’s level as a means of a career path.\nConsidering if it’s worth it to do a master’s degree in something as specific as music cognition (#musicscience, music psychology, whatever you want to call it) touches on some very hot issues such as “What’s the point of graduate school?” and I think it’s worth writing about. It’s my opinion (though obviously very biased) that music cognition is a good career move for people with music backgrounds if you are going to pick a field of graduate study. This of course does not come without its caveats.\nI’ve had this draft sitting, waiting to be published for a while now so I figured might as well brush off the dust and press publish. It’s not the most smooth thing I’ve ever written, but that’s because this post is basically an amalgamation of ideas I would send to people who emailed me asking about this.\nGrad School In this post I hope to share some of my opinions about what it means to get a master’s degree in fields related to music science and give my opinion of what those one or two years could look like. This post is hilariously timed given recent discussions on Twitter regarding the idea of what IS music science, but I’m not about to edit this post to reflect exact operational definitions of all of that. Maybe a post for another time?\nBefore reading on and taking advice from some random guy (or anyone giving advice on graduate school for that matter), be very aware that there are many ways to accomplish your career goals.\nWhat worked for me might or might not work for you and advice about gradute school often relies on a heavy, heavy dose of surviorship bias.\nThere is a huge tendency to look to those who have succeed, think you too can just tick the same boxes they did, and get approximately what they have. Success at anything involves a huge amount of luck, a fat bill, and tons hard work in order to succeed. I really encourage any peers or those in more senior positions to write similar posts on this question so prospective students have as much information as they can before making the choice to go on to graduate school. It’s a huge deal to spend the amount of time and money to attend graduate school and it’s kind of terrifying to think how little information some people have when they make this choice. If you’re thinking about grad school I also encourage you to ask as many people as possible about their experiences. If everyone says the same thing, you probably have not spoken with enough people.\nOne of the reasons I am writing this is because there is a demand for advice on this if people are contacting a some grad student based on his Twitter feed and a blog from over five years ago. There is a dearth of information available besides what is passed between those in and not in the know.\nBefore getting into this, I also want to take a second and point out that this whole conversation is not founded on the assumption that the point of higher education is to go on to academic work. Although I have been primarily involved with academic work, doing a degree that emphasizes both music and science allows for career paths beyond academia in that you can pursue industry jobs and consultancy projects given the skill sets that you can pick up on the way. Of course this also could be said for other types of degrees, but compared to other programs I have seen, it’s just not nearly as encouraged. Music plus science can even pave a career path for jobs not having anything to do with music such as data science (something I know a lot of academics are interested in).1\n What’s The Point of a Master’s Degree? So with that disclaimer out of the way, let’s take our first swing at this by trying to answer the question:\n What’s the point of even getting a master’s in music science?\n If I had to distill my opinion on why someone should get a master’s degree if they are coming from a music background into one sentence it would be that a Master’s (in music science) gives the student a opportunity to autonomously re-define themselves as a (scientific) researcher via engaging with a research project from start to finish.\nThe master’s is a chance to have space to think much more critically about some topic of interest and then engage with research in a hands-on way in order to learn about and eventually contribute to some subject in a meaningful way. If you have the chance to teach along the way, just as in the PhD, remember this is a double edged sword! You might bolster your CV for Ph.D program admissions, but there is an opportunity cost for you learning new skills (of course teaching might be the skill you want!).\nThat said, if you can get into a master’s program where they will let you teach on an assistantship so you don’t have to pay, that’s a huge plus for the program. Assuming that you need this teaching experience also assumes that the way forward is an academic path in some ways. I’ll say it once and will say it again: academia should not be considered the only career option for those going on to get higher education degrees.\nIn 2019, if you think that doing a master’s will entitle you to access to esoteric information, I think you might be a bit misguided. Most information today can be accessed via the internet or hunting down a paper on Sci-Hub or Research Gate. For people coming from music backgrounds, the adage of “Practice rooms are the same at every music school” basically becomes “Everyone has a library card and access to the internet”. (Which might be helpful to hear if you are trying to decide between programs).\nIn my opinion, what you are paying for (or if you’re lucky, being paid) is access to a new community and resources which you should use to open doors that are best for YOU! One way to consider how much you get out of your master’s will be how you best navigate the finite time in your degree and best make use of those resources.\n How Do You Do It? So having assumed that you want to do a master’s that incorporates both music and science, how do you go about figuring out where to go?\nMy advice to people is to pick a program where you will be able to connect to a valuable network of people. This could mean just academic connections, but also could mean industry or other interdisciplinary connections.\nPick a program whose network you want to be a part of. If the program does not have a happy little family of current students and alumni, it should throw up some red flags. I lucked out three times with each of my degrees and consider myself very lucky to be member of the Baldwin Wallace Conservatory alumni, the Music, Mind and Brain group at Goldsmiths, and the School of Music and Department of Psychology at Louisiana State University. Some people have asked if you have to go to a program that specifically does music cognition or has a lab and I’m tempted to say that you really should try to get yourself in somewhere where there is a lab or network of people doing working on topics related to this. If you’re not familiar with where these places might be, check out SMPC’s lab map for some labs that are involved with this kind of research.\nAdditionally, you should know you probably will spend an exorbitant amount of time with these people and you need to make sure you like the people and that the current group is happy. Having colleagues that are bitter and toxic can destroy the entire experience of graduate school. Make sure you ask the current or former students about the vibe of the program. This includes both your future fellow colleagues and whoever is running the show (the P.I. or Principal Investigator).\nNext, I would highly suggest programs where you can undergo a complete research project from start to finish. This normally entails writing a master’s thesis. There are programs that allow you to graduate having just done coursework, but being able to talk in-depth about a specific topic along with appropriate methodologies is a valuable part of your professional and personal development. Doing a project, as opposed to a healthy serving of classes, also allows you the freedom to learn to write a very large document (a very important skill to organize your thoughts), learn new skills (experimental design, data analysis), and will also situate yourself to pick a question you can one day have published (a ticket into PhD work?).\nThe research that you do as part of any music science project should also open up doors to what you want to do after you finish. This could mean contributing to a larger conversation on playlist recommendations if you want to jump over to one of the music industry giants like Spotify or Pandora, looking at questions of rhythm perception if you want to a Ph.D on rhythm, or maybe look at music based interventions for health research.\nWhatever you choose, it’s nice when you find a program where all types of research are valued and not just academic master’s thesis that are the hot issue of your field. You of course also just do a master’s for the fun of it, there is nothing wrong with that either. In my experience, it just seems like a lot of people who have asked my advice are thinking of the master’s as a means to accomplish something else.\nIt’s my opinion that the point of the time in the master’s is to learn a lot about a new topic area and to become useful as a researcher. By useful, I also just don’t mean that in a neo-liberal “learn skills that you can sell”, but to engage with your master’s in a way that will allow you to develop a skill set that makes sense outside of academia.\nKnow that most people getting PhDs will not work in academia so before you enroll in your master’s, probabilitically speaking, you are even less likely at this point to continue to go on to academia as a full time gig. Of course this shouldn’t deter you from pursing further education. Pursing something you find intrinsically rewarding has value in itself. Just know that setting out for that sweet professorship job is not an easy ride. For more reading on that, check out Karen Kelsky’s book The Professor is In.\nI would also suggest to make sure that you not find yourself in master’s program that is basically a “pay to play” scheme for academia. While many people might try to convince you that taking out $50,000+ in loans to pay for a master’s is a good choice, it might not be! This also goes for unpaid internships, especially if whoever is offering you a position comes from a school with a great marketing department!! As an fresh-out-of-undergraduate applicant, you might be tempted to just start googling all the “prestigious” schools and checking to see if they have cognition programs. If you’re all about this, that’s fine, but you should also know that “great” school does not always mean “great” education and it’s OK to say “No” to “great” schools.\nPart of me wishes that I could go ham here on listing out all the music psychology master’s programs that there are (like Vicky Williamson used to have on her blog), but maybe that’s a post for another time.\nOther Considerations But once you have a picked a program, what are the other things to then look out for or suggest?\nOne thing that I think is very important is to spend a significant amount of time choosing the right question to try to answer in your research project. My friends and I always joke that the first lesson of grad school is “Be on the lookout for false dichotomies” which without a doubt applies to the research question you choose. In the entire universe of questions you can answer as part of your master’s, it is possible to answer a question where you can cultivate skills that you’ll find useful after you leave while simultaneously making an original contribution to your field of study. It’s always nice to pick a question that will challenge you to learn new skills, but be guided in that process.\n(Thinking about choosing the right question will probably be a whole blog post in itself in the next few months.)\nIt’s also worth mentioning on this note that there is a not-so-secret currency in academia of publications for those higher up the ladder and you should be aware of that when picking both a project/thesis topic and your adviser. What often alarms a lot of rookie graduate students is the immense amount of pressure that their professors are under to publish at all times. I think it’s important to mention this because this whole sub-culture that students may or not be aware of is going to have a big impact on how you choose your adviser and project. This is important because your choice of project and adviser will also be a huge contributing factor in how grad school goes for you. So my advice to you is to pick a question that your adviser has just enough invested interest in trying to answer that they will go out of their way to try and help you solve it provided that you are going to put in the work to help out.\nBy helping your adviser answer a question they are (partially) working on, you will enter into a intense, but hopefully productive symbiotic relationship where they are able to mentor you into learning new skills that help them answer questions they are interested in. You will get a new skill set, hopefully a publication (again, has almost no relevance outside of academia), and a chance to establish yourself in a new network. This seems almost too obvious at this point, but to someone starting out in a field, so many students are not aware of the hidden curricula of graduate school that often determines how well someone does. I think this is especially true for first generation graduate students who are navigating this territory for the first time. I really don’t think we can be too explicit in laying out as much of the hidden curricula as possible.\n  Done right? So how do you know you are on the right track or have done it right? For a music + science master’s (though a lot of this post could have relevance to Music Theory), I think this means ending with a new skill set and knowledge base that you didn’t have when you started.\nA basic understanding of experimental design, ability to do some programming (ideally in R or Python since you can’t take SPSS, Matlab, SAS, Stata with you when you graduate), statistics, and the ability to engage with issues relevant to your field, and talk at length about whatever you did your master’s thesis on would all be marks of a a successful venture in my opinion.\nIn addition to all these lines on your CV from projects, classes, and references, it’s also always good to have some sort of tangible “thing” to show people as well. Eventually this hopefully will be something like a publication of your master’s research, but if you also can upload some of your code or writings to something like Github or your own website it’s always nice to be able to point directly to something you did so you can show and not tell. Over the last year in doing some work outside of academia I can 100% say being able to directly point to something you “did” is worth its weight in gold when it comes to non-academic employment.\nSome people forget about this one, but you should also try to have references from a few new people in your department, and if you were especially intrepid during those two years, maybe some people outside of it (the Dean?, a collaborator?, the Librarian?) who can vouch for your new identity as a researcher.\nHopefully those who are now your references can now introduce you to jobs or PhD positions (if that’s your end goal). And most importantly you also have a new peer network and many new friends.\nIt’s nice to think that your advisers will be the one helping you out the most, but you never know which one of your peers is going to land a sweet job somewhere or land a position in a PhD program where you also want to get into. The world is so much who you know, which is only made easier when you make yourself useful.\n Future Posts So this was quite a long post here and sitting here writing it makes me realize that I could keep writing about this for hours. Re-reading it, it doesn’t flow that well and reflects that this was just a document where I would cut and paste advice I’d give undergrads when they asked me questions, so sorry if it’s a bit choppy.\nSome future posts on career advice might consider diving more into the idea of what it means to use “usefulness” as a measure of the type of question you ask, what does career diversity in graduate school look like, and how does this advice translate to people wanting to pursue a master’s in Music Theory as a way to do music science.\nHopefully this blog post is helpful for people that find themselves in the position that I was in six years ago. Having just finished a Ph.D looking at this topic, I can say with complete certainly that I never imagined myself as a music science researcher even half way through my undergrad, yet the past six years have been better than I could have ever imagined. I’ve gone from a finishing a degree where I just played trumpet for hours and hours every day to learning a whole new skill set, answering questions I once thought I’d never be smart enough to ask, and most importantly have met the nicest group of people I could have ever imagined.\n  Since I am a card carrying music theorist, many of the people that have asked me questions about making this career move come from a background in music. I am imagining the audience of this blog post to be someone currently in or having finished their undergraduate degree in music. I am kind of writing it to the person that I was 6 years ago.↩\n   ","date":1562544000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562544000,"objectID":"00cca01bbb2087e2c591459ce5290d0d","permalink":"/post/thoughts-on-doing-a-master-s-degree-in-music-science/","publishdate":"2019-07-08T00:00:00Z","relpermalink":"/post/thoughts-on-doing-a-master-s-degree-in-music-science/","section":"post","summary":"I read somewhere on the internet that if more than three people email you with the same question, it’s worth writing your answer up in a blog post. Over the the course of my Ph.","tags":[],"title":"Thoughts on Doing a Master's Degree in Music Science","type":"post"},{"authors":null,"categories":["charity","rstats"],"content":" After a year, my time is now up as a Residential Volunteer Worker at Toynbee Hall. Given everything that’s happened this year (dissertation, travel, destroying my shoulder), I haven’t really got to blog about my experiences at Toynbee, but seeing as it’s been a sizable part of my life the past year, I wanted to take some time to reflect on what I have been up to the past year. Keeping in line with my ideas on career diversity, many of my reflections here will be for my presumably more academic-ish readership.\nI want to talk about two things here:\nWhat was I even doing here? What new ideas do I have that I now want to share?  What was I doing here? To give a bit of background on this, a little more than a year ago I applied to be the ‘Research and Evaluation Residential Volunteer Worker’ (RVW) at Toynbee Hall. As I’ve blogged about before, Toynbee Hall was in the process of re-booting this program that brought people on-site to volunteer at the charity (non-profit) in exchange for residency. I was accepted along with three others, though they worked on the Heritage side of things on projects ranging from story telling, to art, to social justice.\nWhen I started, I thought I’d be roving between the Research and Evaluation teams, but while I was there Toynbee had a bit of a departmental re-shuffling and as a result I ended up spending all my time in Research. As an RVW, and just like academia, I had a jack-of-all-trades position where I generally played to most of my strengths. For me, this included doing all things data, writing, helping organize events, and doing some field research (in that order). I’d imagine that my readership would notice that this laundry listing of things is actually quite similar to that of what people get up to in academia, yet without the overwhelming amount of teaching. I worked on both larger projects as well as smaller ones. Different from academia, the end goal of all projects was not to end up with the research in a peer-reviewed academic journal, but rather to have some sort of immediate impact on the local community. More about this later.\nJob Crafting for Data Science I was quite lucky in this position in that I was part of the first wave of RVWs in a long time and was not an employee at all (just a volunteer), and had a supervisor who was very open to exploring what the role could be. As a result of this, I was able to do a bit of job crafting. Very early on last year, I tried to push many of my weekly goals towards all things data science so I could see first hand what of all skills I have been doing in my Ph.D research actually transferred outside my own little world.\nTurns out the biggest overlapping part of the Venn diagram was cleaning data. I distinctly remember many days where I would put on my headphones (I also learned I do not function well in open office designs) and spend hours taking pre-formatted Excel workbooks with awful column names with strangely coded values and formatting the data over to tidy formats so I could learn about what was in the data. After things were tidied, I then spent a lot of time with ggplot2 really getting to know the data sets that I was dealing with. Doing this, I really started to get some light bulb moment about the importance of exploratory data analysis and thinking about what we are even doing with quantitative research.\nHilariously, I made many data science rookie mistakes that I had read about on all the How-to-Data-Science blog posts I had been reading in the years prior to this position. I wanted to show off all the machine learning I had been reading about in order to show of the power of these tools. But in my first few exchanges of what I would go off and analyze and then share with others, I quickly found that my time and skills were much better spent making a lot of simple visualizations of our survey data (like the plot below) in order to help facilitate the start of conversations about the larger projects. The data analysis I was doing really meant nothing unless I could directly point to why a finding was important to a team of very intelligent people, but without a background in statistics.\nThis process also rid me of any delusions of making models that had overly predictive accuracy. My job was to be critical researcher who used data, not a data shaman that created impressive predictive accuracy.\nIn thinking about this, I also realized that there is not much you can do as the “numbers guy” when someone hands you a dataset you had no hand in designing. Unlike academic research where you pretty much have an idea of all the variables you’re collecting in a cross-sectional study, working here you’re often just given a dataset with a potpourri of variables and asked to finding meaning and value.\nI should mention that when someone does just hand you a dataset with variables another person came up with, the job here is not to be “well you should have done XYZ”; you have to do your best to extract as much meaning and value from the dataset as you can, then actively advocate for the best quantitative data practices that the team can eventually adopt so that future team members will not run into the same dead-ends.\nSo the TL;DR of much of this year is that I had a very nice crash course in what you read in the data science blogs versus what you need by Friday.\nOf course there were a lot of other “researcher”-y things that I also did that were not just hunching over my computer doing R. I spent a lot of time writing reports. Doing this much writing also affirmed my belief that as researchers, one’s ability to immediately create a clear and compelling narrative is one of the best ways you can be most impactful.\nJust like academia, it’s very important to know your audience, why you are writing, and what you hope to accomplish. I also helped out with a couple small things here or there like going out to do interviews for projects and had the chance to run a training session on best quantitative researcher practices. About a month or so ago, I also got to blog about the Datathon, which combined many of these other skills and enabled me to talk to other charities in the process.\nImportantly, thinking about how all of this fits in for my more academic readership, one thing that was great to experience is that many of the skills that I focused on in academia were both applicable and relevant. In many ways, I was close in what I thought going into this, being that working in this context would be similar in form, but different content. This was almost correct… It’s almost a similar form, but with a totally different language and underlying value structure. And it’s this last point that leads me to my next major section here: what is new and what have I learned over the course of the past year?\n  New Ideas So as discussed in my previous post, what I was up to at Toynbee Hall this year was markedly different than what I get up to in my academic life. And more importantly, my experience there has really shaped some new opinions on academic ideas.\nOne of the first— and almost most obvious ones— worth addressing right off the bat is that if you want to go out and help people or make the world a better place or whatever, just go help and volunteer time at a charity. From my Twitter feed, it seems like there are lot of people who struggle with what they do in the context of everything else going on in the world right now. What much of this seems to conveniently forget to mention is that 1) no one is making people stay in academia and 2) there is a whole division of labor in the world whose goal is to directly change the lives of people for the better (Higher Education does not have a monopoly on long term help). This seems almost silly to type, but I feel like we as trained academics get serious career blinders about what we could do with our training. I know I’m often guilty of this, but it’s one reason I’m thinking out loud here: this experience really made me question a lot of assumptions I had about my career going forward.\nTo that, some people might react with a little mix of both academic Stockholm Syndrome and retort with the fact that moving from academia to another sector is not as easy as I am making it sound. Of course, I am not blind to that, but one thing that I am hoping to accomplish in writing about this is the need to talk about career diversity for academics and how this might be possible.\nA secondary point I am hoping to make here is that instead of doubling down on academic research at feelings of not being able to help in the world, I’d bet more people would be better served if academics were to be able to say “No” to the ever growing bubbling over of demands of academia so that people do in fact have free time which they could use to help others in a more meaningful way (if this is something of interest). Now I really don’t want this to come across as a “well this academic research doesn’t help anyone” type of sentiment. I’m a firm believer in the need for researchers, especially in the humanities, to pursue questions that do not seem to have any immediate relevance whatsoever to “the real world”.1\nThe reason I am thinking about this is because of that earlier dread I was talking about and feeling that the time I am investing in academia writing papers about memory for melodies is not really helping anyone immediatly at the end of the day and seems to consume my entire being. I (secretly) tell myself that “Well, when you get that sweet”job\u0026quot; and all this gets published, it’ll aalllll be worth it and people will benefit from my work\u0026quot;! But even IF that obvious lie I tell myself is true, the person who benefits the most from that long cycle of work the most, if I am being honest, is me. I think it’s good research (or else I wouldn’t do it), but I don’t think I should kid myself about who really benefits from this work.\nAnd this idea of who benefits from the research that is done is also something central to the work done at Toynbee Hall.\n Who Benefits? To give a bit of context to this, it’s worth establishing that Toynbee Hall is a charity that strives to have deep roots with the local community it exists in. It started as a University settlement where people could see the local area and become personally invested in it. And this relationship is supposed to be symbiotic. The local community is invested in many things Toynbee Hall. A lot of people in the area, especially older people, go to Toynbee Hall’s community center for a sense of belonging and meaning in their lives. Toynbee Hall is part of their lives, but it also seeks to improve the quality of their lives (through research in this case). . But this introduces an interesting question of power, primarily, what does it mean to do research on people that you have both a vested interest in and personal relationship with?\nThe stereotypical ‘academic’ approach according to popular imagination attempts to address this question the long way. To truly learn about your local community, one must put oneself at a distance, try too stay objective about the questions at hand, look to the literature to establish the central discussions on the topic, conduct research methods with the established and recognized tools of the field, then submit said findings for publication in a peer reviewed journal. From here this apolitical-as-possible piece of research (of course written in the passive voice to absolve the writer of even more bias!!) then can be used as a small building block to influence policy change that the higher levels. This long, circuitous route is certainly the narrative many academics tell themselves that they do. But it is not the only route.\nIn contrast, researchers can dispel themselves of the illusion that what they do is objective (as Qualitative researchers will tell you, so please let me keep going with this strawman here) and instead just get your hands “dirty” and fully embrace the subjectivity of the your research. One way of doing this in a very hands-on way is to use Participatory Action Research or PAR. The idea here is to instead just ask those affected by the research about their problems and include them as peers throughout the entire research process to guide the research rather than treat them as data points to be observed.\nIn order to make this idea a bit clearer, instead of theoretically describing it, it’s better to show than tell so let me show you an example of this from the past year. Imagine that there is project where a local charity (non-profit) wants to directly address the needs of older people in the local community. One might be tempted to read deeply on the issue, conduct a proper lit review, develop yet another survey, yadayadayda. Instead, the charity could try to rely on the already established community connections in order to start to get in touch with the community from Day One. Instead they could assemble a small team of local community members that all have an interest in having some sort of impact, (contrasted to the disinterested interest we read about in research methods) then as a team, come up with question that are impacting the community and in need of change, work with those with more experience in creating better questions, then use this network of co-researchers to go on foot and reach over 500 people in the local community.\nNot only does this result in a better sampling than just throwing up a Mechancial Turk (which this population in particular would probably not use), but in taking this approach the team is able to solve problems in the process. For example, if “everybody knows” that access to information about local services and isolation is a problem for older people, why not additionally equip those adminstering the survey with tools to connect those being interviewed with information that could start to solve these problems in process. Doing this jumps the model of waiting for rounds of peer review before research can be “used”. And more importantly, when it finally does get published, who benefits more? The academic with a new line on their CV or the people who were surveyed?\nToynbee Hall did complete this project last year and you can read about it here. What then results is a report where you get the best of both worlds. You have both a quantitative, representative description a sample of the population (for the quant nerds) as well as narratives of individual voices and also developed a stronger community in the process (for people who don’t need data to be convinced this is something important2).\nThis is all to say: I think that it’s important to take a very long, hard look at who benefits from the research choices we make on a daily basis. This speaks to questions of outreach, ownership, power, and false illusions about objectivity. But as someone who lives and breathes the quantitative stuff, I learned a lot via this experience and will be tabling many of these ideas in new research projects where I am a team member.\nAn Answer So now taking this all back to a larger question about relationships between inside and outside the Ivory Tower: what can be done? Having the privilege of this experience really reinforced an opinion that I was thinking about throughout my Ph.D in that I think Ph.D programs in the humanities would be really strengthened by having long (obviously paid) internship programs as part of their degree options. I have a couple of reasons now as why I would argue for this.\nThe first is that it gets you out of the Ivory Tower for a bit. Not only can you actually begin to see what skills are transferable in a meaningful way3, but you can also just see if it really is [the subject of your Ph.D] that you can’t live without, even though you haven’t ever experienced anything else. Doing an internship like this (or however it’s cast) allows Ph.D students to learn a totally different language and value structure that without learning, will make applying for non-academic jobs much more difficult. I’d like to think that incorporating something like this would make for a stronger application when it comes to applying for academic jobs, but I guess that is something yet to be found out.\nIt’s also a chance to live a lifestyle that is outside of the Ivory Tower grind. Again, it might be that it really is this very specific subset of a work field called Music Theory (in my case) that you can’t live without. Maybe it’s just that people like research and talking about the subject area at hand as a part of a community with a shared set of values. This might not be charity as it was in my case, but I seriously think there is a lot that Music Ph.Ds (humanities) could offer in non-profits arts settings (thinking orchestral management, arts organizations). I’d bet there are probably some arts organizatoins that could use a bit of help in summer from someone with rigerous research training, in-depth domain knowledge, and if you paid them an honest day’s work it’d be be more per hour than what’s expected in a Ph.D program. Doing this would also have people put their money where their mouth is about career diversity and the real values and value of doing a Ph.D in the arts where the only real end goal is “the job”.\nAgain, this is very easy for me to armchair theorize about4, but if cultivating these relationships from academia to outside the Tower is that hard, I feel like that might just speak to the insularity of academia. We (as Ph.Ds) should be able to move back and forth between the bricks of the Ivory Tower.\n  Wrapping Up As I finish here, I can retrospectively see that this was a really a great opportunity. I learned a lot and personally benefited a lot from having this chance and only hope that I was able to provide value beyond what I received. Much of this would not have been possible without having my manager Dr. Xia Lin be such a great leader and let me do that job-crafting I spoke of earlier. I am really looking forward to finding out new ways to help out with this work in the future. My time here has really changed how I think about my own career trajectory and who benefits from all the choices that I make. I am excited to carry that forward in my own career and to hopefully use what I have in a way that helps others in a truly meaningful way.\n  AKA the possibility to improve neoliberal markets AKA how can this make me some money↩\n {r}emo::ji(keyword = \u0026quot;fire\u0026quot;)↩\n and begin to ameliorate the exestential dread of what happens if I don’t get “a job”↩\n What else are blogs good for other than mindless armchair theorizing and not thought out footnotes?!↩\n   ","date":1560816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560816000,"objectID":"cabe2eef474fabcaeb14e22038a4988a","permalink":"/post/thank-you-toynbee/","publishdate":"2019-06-18T00:00:00Z","relpermalink":"/post/thank-you-toynbee/","section":"post","summary":"After a year, my time is now up as a Residential Volunteer Worker at Toynbee Hall. Given everything that’s happened this year (dissertation, travel, destroying my shoulder), I haven’t really got to blog about my experiences at Toynbee, but seeing as it’s been a sizable part of my life the past year, I wanted to take some time to reflect on what I have been up to the past year.","tags":[],"title":"Thank You, Toynbee","type":"post"},{"authors":null,"categories":[],"content":" Things Are Different Now Hello, first-blog-post-since-I-defended-my-Ph.D! It’s wild to think that after five or so years (between starting the the MSc. and the final Ph.D. defense), I am finally done with graduate school. There are no more exams, no more term deadlines, and no more giant documents to write. The last few months felt like quite the pressure cooker between writing everything up, flying back to defend, then doing a bit of travel. And now after the sprint at the end of the marathon, it’s finally over. It kind of feels like a scene out of a movie where the soundtrack builds to a climax, but then it all just goes silent. I feel a bit caught in suspended animation and have a lot of thoughts swirling around and no structure to place them within.\nAnd what’s a boy with a lot of feelings in 2019 to do? Start a podcast.\nJust kidding, the last thing the world needs is another dude telling you his opinions about the world on a podcast. I am not starting a podcast. But I do want to blog more. It’s something I personally find very helpful to do myself and have found reading other’s blogs to be very helpful. In this post, I want to do a bit of self-justification and mindless rambling.\n bigsat So as noted above, I’m finally done with the Ph.D. (Yippe!) It was a trudge to say the least. Not only was it a lot of work, but living in a work environment for a few years as an intellectual underling is not exactly great for your self esteem. The hours and pay were not great (if you think about the Ph.D. like a job), but more importantly, the grind of graduate school is not sustainable. The rate that I was working in the months leading up to my dissertation submission consumed every part of my life. Having spent so much time on my #bigsit and all the graduate school that lead up to that, I also was starting to feel a deeper dread that large portions of my life were passing me by and I was loosing time that I could not get back.1 The good news is that retrospectively I don’t regret any of it. As a result, I have a dissertation that I am very proud of on many levels. I’ll be blogging about that soon as well. But I also think that it is important to acknowledge all these thoughts and feelings and reflect on them. And one thing that really helped me while I was a graduate student was reading about others who seem to have similar feelings. So what I want to do here is explore those thoughts and feelings.\n Thoughts and Feelings What I am finding is there are a lot of those thoughts and feelings post-PhD. In drafting this post, I found that I wanted to talk about EVERYTHING. For this post alone, I thought about having sub-sections or digressions that talked about: finding personal validation through the structure of academia, feeling disconnected from academic peers as someone not going on to some fancy TT job this year, all the strange things people said at conferences last month, and this list goes on. And in starting to sketch out what each of those digressions might look like, I realized here I had mountains of thoughts and feelings I wanted to share. It was almost like that for every new fact I had learned in grad school, I was given two feelings to go along with it.\nBut given all these feelings, and no academic cohort anymore, who can I then share all these niche thoughts with? This question becomes even more important when you consider that you really only are who you think you are in relation to other people. I noticed this the most when I moved to London last year and didn’t have daily access to my cohort at Louisiana State Univeristy. You don’t realize that you’re the “that guy” of your group until you leave them. If you can’t define yourself as “the guy who really likes music theory and music cognition” in your new group, it’s like that part of you goes away as well. This is a pretty big deal, because for many aspiring academics like myself, this is a huge part of our identity.\n Crescendo And the grim truth about this is realization is that post-PhD, these feelings might only increase with more time away from academic settings. In a few weeks I am sure my dbake29@lsu.edu email address will be disconnected. I’ll move further away in many respects because I’m not taking that next logical career step of doing a post-doc, a VAP, or taking a “job”2. In taking another path for right now, I will drift a bit further away from the rhythm of the academic year. And in many ways, I have already started to experience this.\nIn the past few Theory conferences I attended, the invevitalble “What are you doing next year?” question conversation always got a bit awkward when I told them that I didn’t have a “job” for next year. Many times I felt compelled that I had to add some sort of caveat to my response saying that I was planning on applying for more “job”s next year, but next time with greater effort! Though what made it the most weird, is that many people seemed almost not interested in what I was planning on doing this next year or, even worse, actually told me that if I did not take other work that was as close as possible no matter what to being a Music Theory professor, I was just shooting myself in the foot for the rest of my academic career. This was off putting to say the least, but talking about this sad state of affairs is not a digression worth going on.\nThough this whole situation brings up quite the conundrum. As people move through the Ph.D system, not everyone who gets a Ph.D in [Insert Your Subject Here] is going to move on right away to a “job”. If they don’t, the way things are currently set up they will start to spin a bit more out of the central orbit of the field. At some point they might swing back in or they could drift off. But it brings up what I think is an important question:\n What does the space look like for academics who are interested in all things academic, but don’t neatly fit into a comprehensible box of the current infrastructure?\n Throughout my PhD, conversations about this really didn’t come up as the part of any formalized curriculum. And saw it coming. Yet here I am. Or more appropriately– here we are. This group of people is only an ever growing population. And will continue to be since it’s numerically impossible to place every person in a “job” each year.\n My Answer Of course, this is a very large question and problem that exists far beyond that of Music Theory. It might even be happening in Musicology as well.\nBut regardless, there ought to exist some sort of space for people who are card carrying academics without a clubhouse to return to. And I guess what I want to start thinking out-loud about is thinking about what this kind of career diversity looks like that is not just academic purgatory or a career waiting room. I want to try to think about establishing myself (and others) in this space. And not just talk about it so it’s a bit more normal, but also write about what it might look like to enjoy being in this space.\nI don’t really know what that will look like right now. But in addition to all those thoughts and feelings that I want to talk about, I also have a lot of practical work I want to share. And I think my twitter feed and website might be a good place to still stay in the loop and document experiences here or there.\nDrafting up what I want to talk about post-Ph.D (even if it’s just for me to be therapeutic thoughts) I want to blog about career diversity, life as a graduate student, computational musicology, music science, how music theory relates to everything, and a whole host of other things.\nSo who knows. I wanted to blog throughout much of my Ph.D, but just did not have the time. Now on other side, I really want to explore all the topics I couldn’t justify while a grad student and use the whole process of a way as continuing to express my academic identity.\n  Also add in a bit of fear that everyone that I talked to that left academia willingly said doing so was one of the best choices they’ve made in their life…↩\n For my non-academic readers, the “job” refers to a tenure track (TT) job in North America at a decent school near a city that you could imagine asking your partner to uproot their entire life so you can be a professor↩\n   ","date":1560297600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560297600,"objectID":"e6121e3aaaca8bb12c2c61efce77686c","permalink":"/post/embracing-career-diversity/","publishdate":"2019-06-12T00:00:00Z","relpermalink":"/post/embracing-career-diversity/","section":"post","summary":"Things Are Different Now Hello, first-blog-post-since-I-defended-my-Ph.D! It’s wild to think that after five or so years (between starting the the MSc. and the final Ph.D. defense), I am finally done with graduate school.","tags":["career diversity","diary"],"title":"Embracing Career Diversity","type":"post"},{"authors":null,"categories":null,"content":"","date":1558569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558569600,"objectID":"1c17b66475cdc7f8bf359a9d4df3468e","permalink":"/talk/pedagogy-compmodel-2019/","publishdate":"2019-05-23T00:00:00Z","relpermalink":"/talk/pedagogy-compmodel-2019/","section":"talk","summary":"Teaching melodic dictation involves instructing students on what and where to direct their attention in order to improve their abilities. This process has been formalized by Gary Karpinski into four discrete steps of hearing, memorizing, understanding, and notating, which help students break down the overwhelming amount of mental processes they need to coordinate in order to successfully complete a melodic dictation (Karpinski, 2000). As students’ experience increases, they are able to memorize larger chunks of music and more easily able to dictate music they once found difficult. But what is going on in the student’s minds over the course of aural skills instructions that allows for this growth? This paper puts forward a computational, cognitive model of melodic dictation with the goal of helping explain how students become better at melodic dictation. The model is based in research from both cognitive psychology and computational musicology and incorporates relevant theoretical aspects such as working memory and the structure of the melody itself that contribute to a student’s performance. In this paper I demonstrate how modeling the cognitive decision process during melodic dictation helps provide a precise framework for pedagogues to understand the inner workings of cognition during melodic dictation and can help inform teaching practice. Using a cadential passage from Schubert’s Octet in D Major (D. 803), I walk through an iteration of the model and show how the the model’s choices aligns with both intuitions of aural skills pedagogues to establish the model’s verisimilitude. I then argue the model’s implications for teaching melodic dictation and suggest how combining research from music cognition and music theory can help create a more linear path to success amongst students. Presenting a computational model additionally demonstrates every ontological commitment, thus making it completely vulnerable to criticism allowing it to serve as a point of conversational departure in discussions of best practice for melodic dictation pedagogy. This paper directly addresses the recurring call (Butler, 1997; Klonoski, 2006; Karpinski, 2000) to address the chasm in research between music cognition and music theory pedagogy.","tags":null,"title":"What is going on in someone's head when they do melodic dictation?","type":"talk"},{"authors":null,"categories":null,"content":"","date":1557446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557446400,"objectID":"a2e77519b1729be11729031c961c622a","permalink":"/talk/mtmw-compmodel-2019/","publishdate":"2019-05-10T00:00:00Z","relpermalink":"/talk/mtmw-compmodel-2019/","section":"talk","summary":"Teaching melodic dictation involves instructing students on what and where to direct their attention in order to improve their abilities. This process has been formalized by Gary Karpinski into four discrete steps of hearing, memorizing, understanding, and notating, which help students break down the overwhelming amount of mental processes they need to coordinate in order to successfully complete a melodic dictation (Karpinski, 2000). As students’ experience increases, they are able memorize larger chunks of music and become more able to dictate music they once found difficult. But what is going on in the student’s minds over the course of aural skills instructions that allows for this growth? This paper puts forward a computational, cognitive model of melodic dictation with the goal of helping explain how students improve at melodic dictation. The model is based in research from both cognitive psychology and computational musicology and incorporates relevant theoretical aspects such as working memory and the structure of the melody itself. I accomplish this by walking through an iteration of the model using a cadential passage from  from Schubert’s Octet in D Major (D. 803). I demonstrate how modeling the cognitive decision process during melodic dictation helps provide a precise framework for pedagogues to understand student’s  inner cognition during melodic dictation and can create a more linear path to success amongst students.","tags":null,"title":"What's going on in someone's head when they do melodic dictation?","type":"talk"},{"authors":null,"categories":null,"content":"","date":1556150400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556150400,"objectID":"a0db8e5155bfaf5fb7fee12356dff32c","permalink":"/talk/againagain-ffh-2019/","publishdate":"2019-04-25T00:00:00Z","relpermalink":"/talk/againagain-ffh-2019/","section":"talk","summary":"Music is made up of many small repeated patterns (Margulis, 2014). Research in music perception demonstrates that these patterns are learned implicitly (Rohrmeier \u0026 Rebuschat, 2012), and importantly, they are related to listern’s sense of musical anticipation (Huron, 2006). While research on patterns and music has established how these patterns are related to expectancy and melodic segmentation (Pearce, 2018), how music’s repetitive nature affects a listener’s load on memory has not been explored to the same extent. This paper presents a novel theory of music perception that links music’s repetitive structure to the limits of working memory. I first draw from research in cognitive psychology that hypothesizes that more predictable events are less taxing on memory. Given research in music perception based on the statistical learning hypothesis and the probabilistic prediction hypothesis (Pearce, 2018), I posit that more predictable musical events would be less taxing on memory as a result of more efficient processing. To demonstrate this, I provide both evidence from a newly encoded corpus of over 750 melodies and a small, pilot experiment (N = 15). I argue that a motive’s frequency distribution in a corpus is related to its load on memory when quantified using the information content measures derived from Pearce’s computational model of auditory cognition. The paper concludes by asserting that studying the repeated patterns in music can help inform both work in memory for melodies, as well as music pedagogy. I end by further speculating that similar research might provide a useful theoretical link to investigate interactions between musical memory and the finite window of working memory, and thus help answer questions of musical perception.","tags":null,"title":"The Frequency Facilitation Hypothesis","type":"talk"},{"authors":null,"categories":["charity","rstats"],"content":" Background Not everyone might be aware, but in addition to living that dissertation life this past year, I have also been volunteering at Toynbee Hall as part of their newly re-started Residential Volunteer Worker program. The program started WAY back in the late 1890s. The idea was that all the bougie people from Oxbridge would come and live on-site in East London in order to do charity work and help out a bit before they were to go on to do other things like be elected to Parliament and then pass laws and help those that were not as well off. I probably will not be elected to Parliament anytime soon, but will hopefully go on to be a helpful member of society in other ways. Toynbee Hall restarted this program this past year and I was lucky enough to have my application accepted for the position. Since then, I have been helping out as part of the research team.\nI’ve helped out with a few smaller projects here or there (basically crunching the datasets they have and putting my other research skills to use doing interviews and what not), but before my year was up, I wanted to try and do my own mini-project which will be the focus of this post: a #data4good datathon at Toynbee Hall.\nCheck out the event and sign up here!!\n History of Helping You can read my glitzy description of the event in the link above, but if you’re reading my personal blog, as opposed to the event page, I assume this readership would want a bit more context on the Datathon and the ideas behind it. To give a bit of background, Toynbee Hall is a very, very old charity based in East London. It has a long history and was a touch point for a lot of important people in London back in the day who were interested in charity work (non-profit for all my North American readers).\nAges ago, some of the people here at Toynbee Hall helped out Charles Booth in his creation of the London maps of poverty that essentially created the first data visualizations in what might be considered a #data4good cause. Fast forward over 100 years and instead of going door to door and trying to use colors to highlight different levels of wealth on a map needing several panels, we can instead download publicly available data from something like the Open Data Institute or The London Datastore and make data visualizations with the same goals in mind.\nGiven Toynbee Hall’s history and current research interests that engage the community by bringing different people together, it almost seemed like a no-brainer to try to put together something where we could try to combine all of these into one coherent event.\n Goals Knowing that there are community issues in need of addressing in Tower Hamlets, as well as community members who know the area, we wanted to continue in the tradition of bringing people together to answer a question. Because my background has a bit of #rstats and data science (because of my work in music science), what seemed like the logical move here was to combine the history that Toynbee Hall has on data visualization and the local community using the data community.\nSo what were our goals in trying to do this event?\nWell the first goal of this was to see what goes into organizing this kind of event. I don’t really have too much of an event planning background and wanted to know what I would have to get in order to front load most of the data work for everyone. This means gathering all the data to start with, figuring out how to best host it, and setting everyone up for success the afternoon of the event. This data curation is most of what I do anyway, and I figured if this process did not prove too hard, then maybe I could help out remotely on events like this after I leave Toynbee in order to still be helpful.\nThe second goal is the most important one:\n to have the event result in the creation of meaningful material that Toynbee hall can use in their upcoming advocacy campaigns.\n The research team is planning on looking at issues of both Youth Homelessness and Safety in Tower Hamlets this summer. Any data that we could find and could include in future reports could really help provide evidence that could eventually be used in a policy campaign. In order to do this we are using data that Centrepoint has given us, publicly available data on crime in London, and maybe one more source that I am putting together today.\nThe third, a by-product of having this event both locally and on Twitter, is to hopefully create some visibility to these issues. Just having people tweeting their data visualizations and maybe having some be caught by the community at large would be great for making people aware of these issues. It would also help us connect people who are interested in #data4good with charities or non-profits that they can help.\nLastly, in line with solving problems long term, I am also hoping to build up a network of people that Toynbee Hall can turn to after I leave. The program that I am on is just one year long and I will go on to other work after this. After my time here is up, I want to have linkd up the research team at Toynbee Hall with a network of people that could help in the future. One way that we are hoping to accomplish this is take everyone that participates in our event and link them up with organizations like DataKindUK and hopefully foster new relationships.\nI have a thousand other things I want to say about this event, but instead of editing more content that I originally drafted few weeks ago, I need to instead get all the content ready for us to go tomorrow. I am sure I’ll be talking a lot more about this in the future when things calm down (still also on the come down from my dissertation submission last week…) but I invite you to join us tomorrow either at Toynbee (in the beautiful Ashbee Hall!) or online on Twitter with #data4toynbee and download the data from our Github and help us out! If you do download the data and something does not make sense, please @ me so I can fix it.\nFeel free to try it out now as I put on the finishing touches!\nPlease get in touch if you want to chat more! Either ping me on Twitter or send me an email!\n ","date":1555977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555977600,"objectID":"143541c50a46da192816234f302385ba","permalink":"/post/toynbee-hall-datathon/","publishdate":"2019-04-23T00:00:00Z","relpermalink":"/post/toynbee-hall-datathon/","section":"post","summary":"Background Not everyone might be aware, but in addition to living that dissertation life this past year, I have also been volunteering at Toynbee Hall as part of their newly re-started Residential Volunteer Worker program.","tags":[],"title":"Toynbee Hall Datathon","type":"post"},{"authors":null,"categories":[],"content":" Since I am planning on spending the whole day today writing, I figured: What better way to warm up to some writing than with some writing?1 And what better way to get the juices flowing than to write about the wonderful experience that was the Sempre graduate student conference at Cambridge this past Monday.\nIf you’re reading this in North America, you might not be familiar with Sempre, so for the uninitiated, Sempre or the Society for Education, Music and Psychology Research is a professional organization in the UK dedicated towards bringing together those in various fields of music, but specifically music education and music psychology. I don’t think we really have something like this in the USA. The music psychology people tend to hang out at the Society for Music Perception and Cognition, and I am kind of embarrassed that off the top of my head I don’t know where graduate students in music education might submit research they are working on to present to their peers? According to Sempre’s website, they’re very keen on helping out researchers at the start of their career and having this graduate conference is just one of the ways they do that.\nThe conference this week was a short and sweet one-day affair held at the Faculty of Music at Cambridge. Of course the weather was perfect that day, but luckily all the talks made it worth it to sit indoors all day. Ian Cross welcomed everyone with a short the opening address and if my memory serves me correctly, he noted there were about 80 submissions to this conference. He said most of those submissions did happen to come the day of the deadline, which is quite in line with any stereotypes one may have about graduate students. But thinking about that, I feel like if all professional music organizations were to release the timestamp data of their abstract submissions, there wouldn’t be a single one that didn’t exhibit an exponential growth in submissions in the hours leading to the deadline.\nAfter his short introduction, the keynote speaker Martin Rohrmeier, head of the Digital and Computational Musicology Lab at École Polytechnique Fédérale de Lausanne was introduced. Martin’s talk surveyed a lot of his previous work on using computational tools to model various aspects of tonality which showcased the work of his graduate students in attendance that day. He also presented an overview of his lab’s work looking at hierarchical, embedded structures within larger forms. All of this empirical work then was tied together with what Martin put forward as a possible general theory of cognition and aesthetics that would be available to the listener, were they to choose, to engage deeply with a work of art.\nKnowing his audience as the next generation of music researchers, he made several pleas throughout his talk to encourage future work in music to work at the intersection between music theory, music psychology, and computer modeling. This is an idea often gets poked at over here in Europe quite explicitly, but seems to live more as a tacit assumption in a lot of work in North America unless you’re at a talk by David Huron.\nThe rest of the day took an ABA form with five minute flash talks surrounding a longer 12 minute talk session and the poster session scheduled for lunch. As I tweeted earlier, I was skeptical of the five minute talks at first, thinking “How could people cram all they need to say into five minutes?!”2 but seeing as this was only a one-day conference that was more about community building rather than getting in the nitty gritty about methods, the format worked well. Because of it, in a one-day session we were able to hear about the research of 20 graduate students on a diversity of topics (not even including the 25 posters at lunch!).\nAlmost all of the talks in the morning session addressed issues at the intersection of music and well-being with the exception of a paper by Daniel Harasim looking at unsupervised machine learning methods to examine jazz harmonies. Listening to all these talks only reiterated assertions made earlier in the keynote of how many human resources are needed in order to push knowledge limits in the field of music.\nThe start of the afternoon session was reminiscent of last summer’s multi-hub ICMPC conference as the winner of Sempre’s Hickman award, Lindsay Warrenbug, skyped into the conference to deliver her paper on Melancholy and Grief in Music. It’s nice to see the music science community putting their money where their mouth is on making remote presentations a possibility for presenters (something also heavily advocated for by the SysMus Series). Not to throw shade, but this is something that my home discipline of Music Theory needs to do a bit of catch-up on.3 I look forward to the inevitable SMT Discuss or Twitter feud where we discuss if a person who can’t be to the national meeting in-person should instead be swapped out for someone who can.\nThe rest of the talks in the afternoon were all extremely interesting. It’s hard to go through and single out work I was loving since most of the work I liked was done by people who were good friends, but one paper that I feel like the theory and psychology community should be looking out for very soon was the work by Peter Harrison and his advisor Marcus Pearce. In 12 minutes, Peter was able to explain the current theories in the literature surrounding instantaneous consonance and dissonance (is it periodicity, is it spectral interference, is it cultural familiarity?), introduce an aggregated behavioral datasets from four prior studies where participants rated various sonorities, then introduce a new R package, “incon” that they used to computationally model 15 models of consonance to get to the bottom of the story. If you’re interested in his findings, I know he also has a pre-print of it out right now.\nI only mention his paper in detail because I think what he is doing is very in line with the music theory world and will hopefully start an interesting conversation once it gets published.\nBy the end of the day we’d have heard talks on musical chills, emotions and music, using music to help rehabilitate stroke survivors, music’s role maternal mental health in The Gambia, and posters on topics from more traditional music theory (thinking about hierarchical voice leading structure ala Schenker with Graph Transformations) to decentering the dominant discourse of the ‘Dead White Men’ cannon. It was a great day. Not only did the conference have a diversity of talks, but of the 20 talks, there appeared to be 50/50 split in the gender of the speakers. I only note that so when the inevitable “But is it even possible to have a conference where we just have men talk?!” question arise, there’s documentaiton of this happening.\nGoing to this conference and thinking about things like the general rage from last week regarding the Google Doodle made it very clear that there are many, many ways to do music research. It made me wish that there were more conferences that follow this design beyond the graduate level. I am only lamenting about this because if all goes well in May, this past Sempre conference will have been my last as a graduate student.\nOf course this brings up ideas about the “point” of the conference, but one important “point” is that I don’t think conferences should just be be about apex research at its final moment of metamorphosis before publication. I feel like we as researchers need more of a chance to air out our ideas in a less, shall I say… performative setting. I obviously acknowledge that the content and subject matter with a conference like this is very different to that of AMS or SMT, but honestly think that the culture of something like Sempre is much closer to what people want and need.\nThis is especially true for early career researchers where point is to build relationships, explore new ideas, and not start to drink that academic Kool Aid right away since numerically speaking we will not all be academics, but will hopefully all engage in some kind of music research. I’m also advocating for this because I’m currently in the peak of my PhD isolation phase now writing and going to this and being inspired by peers is what I think I’ll need to cross the finish line.\nSo big thanks to Sempre for hosting it, allowing for a day of intellectual curiosity, and continuing to grow a healthy network of next generation of researchers.\n My thought yesterday morning, posting it a day later↩\n In retrospect, an obvious personal projection↩\n Which I only mention as personal grivence since I was not able to present some of my own work this year at a conference that I was not able to attend because of travel…↩\n   ","date":1553731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553731200,"objectID":"8da1d79fb52b44fde9ddbc30ce2eab44","permalink":"/post/sempre-grad-conference-2019/","publishdate":"2019-03-28T00:00:00Z","relpermalink":"/post/sempre-grad-conference-2019/","section":"post","summary":"Since I am planning on spending the whole day today writing, I figured: What better way to warm up to some writing than with some writing?1 And what better way to get the juices flowing than to write about the wonderful experience that was the Sempre graduate student conference at Cambridge this past Monday.","tags":["review"],"title":"Sempre Grad Conference, 2019","type":"post"},{"authors":null,"categories":[],"content":" I obviously could not resist writing a blog post about yesterday’s Google Doodle twitter buzz. I’m not going to try to talk about some of the smaller issues that came up. And tons of people have already said interesting things, but instead I’m hoping to ride the buzz so I can talk about something very relevant to the discussion: encoding and digital representation.\nUsing Imani Mosley’s blog post as a jumping off point, she makes many great points about what Musicology twitter was interested in. One thing that I was most struck by were claims the reason for the snark was the tired Bach-as-machine trope to explain why scientists gravitate to Bach. In this post, I want to provide an alternative possibility of why researchers gravitate to Bach. Researchers like Bach because it’s already encoded.\nAs Imani points out in her post, the dataset is made up from 308 compositions that the model was trained on. You can read more about the model here, but each Chorale has many more musical events/data points. Navigating to where the Bach chorales live on my computer and running the census -k *.krn humdrum command on the first 308 of the Bach Chorales (just to ballpark) that were edited by Craig Sapp results in 72,030 note heads that can be analyzed with tools that can interpret kern data. This is a subset of 120,528 humdrum tokens.\nI bring this up not to start debates about how much data is needed to train a model, but rather to point out that each little bit of data here had to be entered by hand for it to be used in any model. And there is no way that you could get a model to even come close to output that would be interesting without very clean data.\nAs a researcher looking to tackle problems in this domain, having something like a complete set of Bach’s chorales in a digital format is a fantastic resource. But one of the problems in computational musicology is not the depth of data used to try to make a machine part-write in style of Bach, but rather the breadth of representation researchers can chose from when they pick which data to model. How is it possible to make generalizable claims if you only look at a very small subset of music?\nIn general, I work a lot with music and often use computers as a tool in that research. Unlike the Bach problem here, I am interested in melodies, but the types of data and tools to analyze this data are similar. One of the major problems I face in my work is that in order to make robust claims about music, you need to be able to show similar phenomena happen with new data. But where does this new data come from?\nIf you’re a data scientist, you might get this from scraping a website or a government database. If you’re a music psychologist, you often get data from experiments. If you’re a computational musicologist, new data is not nearly as accessible. Most likely, you need to either find some or create a digitized version of that data yourself.\nIf you take the former option, you first inclination might be to go to somewhere like kern scores or see what something like the SIMSSA project curates and to see what is available. If you pop on over there for a second, you see a sample of what you can select from. For the most part, it’s like a worse version of the old dead white dude problem that is being faced in both curricula and the performance world right now. That said, there is still tons of data there. Based on what is current there, the kern scores page has 7,866,496 notes from 108,703 files. In addition to a lot of dead men, we also get a lot of folk songs, primarily with Hurons’s digitizing of Schaffrath’s work with the Essen collection.1 You’ll notice that there are not a lot of complete sets of lieder, full-on symphonies, or much new music (copyright and public domain are obviously also an issue).\nIf you take the latter option, you have to do encode these melodies yourself and digitizing music is not fun at all. Over the past year, I encoded 778 monophonic melodies using the MuseScore platform. These 778 melodies resulted in 36,387 notes, a vast majority of which I did personally.2 It sucked. I hated having this giant encoding project living over my head throughout the dissertation writing process. And since I am fresh out of it and still in peak bitter stage, thinking about this whole thing feels very much like a little red hen situation.\nIn the future, this kind of work could remain to be resigned to the individual. Encoding a corpus could follow the lonely scholar in the ivory tower model where digitizing so much music is a sort of rite of passage. But I don’t think that’s the best way forward for anyone involved. I think encoding needs to be much more of a community effort and we can look to fields like psychology of how we as music-types might remedy this problem.\nFor example, in psychology, undergrad students taking psych courses in the USA are generally required to take part in experiments which generate the data for the papers that the field of psychology generally reads. Using such restricted samples leads to problems in that this data is basically WEIRD, but I think you could argue it’s better than not collecting any data. In music, we don’t really have this problem as much because our research is not this-kind-of-data driven, nor should it in many topics. But in cases like the one I’m describing here, we have our own litte weird problem.\nTo remedy this, I imagine a world where in teaching music theory– where often competence in a computer notation is a learning objective– we spend a bit of time talking about this problem of (digital) representation in music.3 If every classroom were encodes some pop melodies (also a transcription exercise!), encode a Shostakovitch Fugue (a project I have been putting off for years now), or even encode one part of a symphony, all of this data would slowly start to accumulate for anyone to model. Doing this would lead to a deluge of new data for researchers to use, allow for Meyerian comparisons between styles, and teach students about serious pressing problems in research while having them learn about notational software. This also will expose them to the field of computational musicology, something that I didn’t even know existed until half-way through my Masters in psychology. Additionally, once compositions cross into the public domain, we as a research community can work towards having more music, more accessible to more people in the form of open scores.\nSo it could be that there is this tired trope of Bach-as-machine (which certainly plays into it), but I’d be interested to know how much of what we do is just by-products of the means we have to answer those questions. Either way, as noted by Robert Komaniecki, hats off to the Google team for getting people to get so fired up about part-writing.\n Schaffrath, H. (1995). The Essen Folksong Collection in Kern Format. [computer database]. D. Huron (ed.). Menlo Park, CA: Center for Computer Assisted Research in the Humanities, 1995.↩\n A couple of my LSU colleagues helped out on some of them.↩\n Also a great place to talk about human biases in algorithms, the need to be ethical in research…↩\n   ","date":1553212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553212800,"objectID":"c2bbcab3d9c15ced447e66d299dcfa65","permalink":"/post/digital-representations/","publishdate":"2019-03-22T00:00:00Z","relpermalink":"/post/digital-representations/","section":"post","summary":"I obviously could not resist writing a blog post about yesterday’s Google Doodle twitter buzz. I’m not going to try to talk about some of the smaller issues that came up.","tags":[],"title":"Digital Representations","type":"post"},{"authors":null,"categories":[],"content":" One of the biggest issues that I lament about regarding Twitter is how there is so much great information just falling through my feed, never to be recovered again. A perfect example of this appeared yesterday when Will Mason tweeted asking for people to help provide examples of idiomatic instrumental constraints leading to funny pitch choices.\nWithin one day, there were loads examples of this along with some discussion contextualizing the responses. Examples like this happen all the time on Twitter ranging from people needing specific musical examples to killer Twitter rants like this one of Doug Shadle that took me way too long to find, which only proves my point. After liking the tweets, that’s usually where my engagement with the information ends, then I get sad that this information will never be available again for anyone who was not on Twitter yesterday. I know it lives on Twitter and can dig it out, but I honestly feel I should be spending less time on my computer when I find myself telling more senior music scholars things like\n “Oh, actually, there was a really good Twitter thread about this the other day”.\n So what can be done?\nWell, one thing that I think a lot of people forget about is that all this information can and should be put on Wikipedia. After looking at Will’s thread, I put “Idiomatic Music” in the Wikipedia search bar, and was directed to this stub of a page.\nThis is where I would have hoped to find the information on Will’s tweet if I were looking for it on another day, but it’s not there. So in true “knowledge should be for everyone” fashion, I added it to the page and thought this might be a good opportunity to procrastinate writing my dissertation by writing a blog about my favorite dissertation writing procrastination activity: editing Wikipedia.\nEditing Wikipedia One thing I have been meaning to blog about, and this obviously has given me a good reason to, is how easy and important I think it is for academics to edit Wikipedia. It’s so important that I have even turned my love of editing Wikipedia into something I personally refer to as #wikiwednesday.1\nFor me, #wikiwednesday is just a weekly reminder that I should add something to Wikipedia every Wednesday. For the past few weeks, I have been doing it on topics related to my dissertation, but in this post I honestly wanted to show how easy it is to edit Wikipedia for anyone that hasn’t yet. Hopefully after seeing a few screenshots, you too will be inclined to help out as well!\nThe first thing that you need to do is make an account. I made a quick one just for this post.\nOk, now with an account made, the next thing that you need to do is click the Edit Source button found in the top right corner.\nHere (at least my my Firefox?) there’s the option to use the Visual Editor mode. If you’re not familiar with Markdown and HTML and what not, this is the way to go in my opinion.\nFrom here, this opens up the hood and you can start editing. I am personally in the process of doing the last 50 days of my dissertation, so I seriously do not have time to restructure this page, but I do have time to add in a bit of text (and do this quick blog post).\nNext, all I wanted to do was create a new heading that would provide some examples of this, then write some very OK text with some concrete examples from Will’s thread. The text is not poetic by any means, but it’s better for it to be there than not. And if you think what I wrote sucks, you can always change it!\nAfter adding all this in, I then tried to link in some of the composers and examples with the link tool. It might take a second to get a handle of how this works, but it’s time well spent in my opinion. After a few minutes of editing….Tah Dah!\n wiki-ing Part of me wanted to just keep going and adding to this page, but my hope is that people will read this post and do a bit of their own #wikiwednesday-ing and just try to do one small edit on this page or another. I find that once you start, I think it’s pretty fun and actually kind of hard to stop.\nNot only is it fun (my opinion), but I also think that if you are an expert in something (cougheveryonewithaphdcough), you have a responsibility to edit Wikipedia in your own subject area. Not only is it important to give people up-to-date and accurate information, but Wikipedia is also a place where most people probably start most of their inquiries. And this isn’t just undergrads, the amount of times I have seen people crack open Wikipedia on their phones at an SMT or AMS talk on a subject outside of their realm of expertise is pretty funny.\nThe reason I think this is so important is because this writing will be more read than any article I write in any journal (especially if it’s written behind a pay wall!) and probably have more of a cumulative impact than most of my research. Also, knowing that Wikipedia is probably the best empirical evidence of what we as a community canonize, it becomes so important to make sure that Wikipedia is representative of the world we want to exist in. I’m obviously not the first by any means to point this out. There has recently been coverage of Wikipedia’s diversity issues by the Washington Post and anyone that is not living under a rock (aka not on Twitter) knows that the music community is actively working towards Rebalancing the Music Canon as blogged about by Anna Kijas and working towards more accessibility of music from historically under-represented composers by initiatives like Music Theory Examples by Women and The Institute for Composer Diversity. All I am trying to get across with this post is how important I see it for anyone with skin in the game to take a bit of time out of their day each week and make a habit out of putting information out where most people read it.\nOf course this is being done too, I guess I just am hoping for more of it. I’ve seen that some people like Kendra Leonard starting to organize Wikipedia sessions (couldn’t find this exact tweet to link in?) to create and edit pages on Women composers. In my opinion, if both SMT and AMS put aside 2 hours each national conference to edit the music pages of Wikipedia, we as a society could take most of the year off of #publicmusicology (just kidding, but maybe not because that level of human resources editing Wikipedia would be unheard of).\nSo on that note, I think it’d be nice if anyone interested in #publicmusicology would join me in my weekly small edits of Wikipedia and share small portions of what they did with #wikiwednesday to remind others to also help out. Or you could get back to devoting all your time to that article that most people probably will not read (#shotsfired).\n  My love for editing Wikipedia pages grew out of taking extensive notes on a server that my advsior, Dan Shanahan set one up for our department to facilitate collective knowledge for our department. Of course those posts live in on a “private” wiki, but they will one day see the light of day when I either blog about them or try to get all that info to Wikipedia.↩\n   ","date":1552435200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552435200,"objectID":"fbebac75bf6d5d2ca9f4635a6ac1e2fd","permalink":"/post/wikiwednesday/","publishdate":"2019-03-13T00:00:00Z","relpermalink":"/post/wikiwednesday/","section":"post","summary":"One of the biggest issues that I lament about regarding Twitter is how there is so much great information just falling through my feed, never to be recovered again. A perfect example of this appeared yesterday when Will Mason tweeted asking for people to help provide examples of idiomatic instrumental constraints leading to funny pitch choices.","tags":[],"title":"#wikiwednesday","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"322dbaccf72a6d71f827fdb2866be935","permalink":"/teaching/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/teaching/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"About / Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c9b5771543b03b8149b612b630936a56","permalink":"/experience/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/experience/","section":"","summary":"Experience","tags":null,"title":"Experience","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"See some of the projects I have worked on","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":["music"],"content":" The past few times that I have sat down to work on my dissertation I have run into a writer’s block. Though it is not the kind of writer’s block where nothing comes out. The past few times I have set aside time to write I have had so many thoughts that I have not been able to just stick my nose to the ground and write. My brain feel like it’s on fire. It’s the end of the year and it has been an intense one and it’s important to acknowledge that. Too often in grad school we work and work and work and work, and never reflect on how we even ended up in this situation in the first place. There is too much doing and not enough reflecting.\nSo to attend to that problem, as opposed to just tweet about how we need a change of culture (but really be frantically work on yet another manuscript) I set aside some time yesterday to write about something I have been wanting to share for a while: my auto-biographical musical playlists.\nDarling The general background and inspiration from this post and idea comes from some music science literature investigating how music has the power to evoke strong emotional memories that are not related to the structure of the music itself. There has been some cool work on it that you can read about here or here that digs into it properly, which also runs in tandem with research suggesting why really like the music we listened to in our formative years. But for all the intents and purposes of this post, all you really have to know is that there is good evidence to suggest that a strong link exists between music and memories of specific moments in our lives. Ages ago, when I first started getting into music science I heard this referred to as the “Darling, they are playing our song!” effect and I wanted to see if I could use some of this research in my own life.\nSo as a result of reading about this area of research in music science, a few years ago I decided to start keeping a playlist diary on Spotify. The idea of it was that I would add a song to the playlist whenever I had a strong experience associated with music. The experiences did not have to be exact mappings such as this exact song must be playing during a certain moment for it to be added to my playlist; the idea was that any memory that might be bottled up in a song could get added as not to make the inclusion criteria that strict. This of course is naturally much easier for a musicophile like myself who basically always has music playing or is around music. All that needed to happen was for me to be a bit more mindful as I went about my life. If a song comes on that seems like it is painting a nice soundscape to the backdrop of my life, it gets added.\nResults So I have been running this experiment on myself for the past four years. It’s the end of 2018 now, so what got added to it this year? Here is a link to the playlist itself.\n As I am writing this post I have been listening to the playlist and, like the past few years, when listening to it, I get immediately transported back to the memories I actively chose to catalog. The first couple of songs take me right back to celebrating the New Year with my friends in Baton Rouge. For example, I remember the second tune, Lambada, coming on the radio as my friends drove to buy food to make breakfast after a long weekend in New Orleans. The moment felt right, so I whipped out my phone to Shazam the tune, and then later added it to my list.\n It’s not just exact moments that I catalog, but I also record re-occurring songs. For example, later on the list is some Debussy.\n I like the music, but more importantly it’s used in Westworld and I used to have weekly Sunday screenings of the show at my apartment in Baton Rouge. I wanted to remember sitting quietly in a room with my friends having a bit of a break before Monday would come up and grad school would restart.\n Patterns One day I’d like to look at the list and maybe see some patterns over a lifetime. For example, I’d imagine that more songs/memories would cluster around highly anticipated emotional events. I saw this play out last year with two salient songs from my PhD General Exams. The first example is this great tune my friend Crystal put on in my living room after a day of drinking celebrating being done with three days of writing for the first part of our General Exams and learning the line dance to this.\n Fast forward a few weeks and the night before my oral defense, I remember sitting in a Sonic parking lot eating ice cream with my friends Sasha and Jacob, listening to Eye of the Tiger at full volume. It was a moment where it felt like God had momentarily DJ’d for us.\n Cruising through this playlist I also realize how much traveling I did this year and all the great people I got to hang out with. Just weeks after Eye of the Tiger, I got to hang out at a public musicology conference in South Carolina and ended up hanging out with both darkmusictheory and 12tone late at night in an AirBnB and remember really liking the piano intro to this tune.\n Much of my playlist comes from memories traveling. In Florida I remember sitting with my friend Rory while talking about how we are spending our lives and this recording of A Mi Manera came on the radio.\n This was the first vacation/holiday I had ever taken just for the sake of a vacation (not academic work related) in years and I distinctly remember sitting on the patio, asking the server to start happy hour early, mis-ordering our 2 for 1 margaritas, then both being served 2 drinks at 2PM and boozing it up in full Jimmy Buffet regalia. Of course I won’t go on a play by play1 of my whole list, but the meaningful thing is that as a whole this playlist only ‘makes sense’ to me.\nSomeone looking at the playlist might be able to figure out some of the tunes given that they knew where I was at what time such as this recording added at the start of July.\n But other songs that I have known for a while get emotionally recycled and obtain new meaning just for me.\n  Zooming Out And the cool thing is, as I mentioned above, I have been doing this for the past four years. Going all the way back to 2015 I can still vividly transport myself back to eating donuts in an NYC apartment listening to Laura Mvula.\n And even as the 2015 playlist plays, I still anticipate each song within this playlist’s context with each memory being anticipated as well. So what I guess I am basically suggesting is that on a personal level, if you have an assortment of external musical meaning your life, maybe it is time to consider starting an autobiographical musical memories playlist yourself in 2019? I have found knowing that this playlist is there helps me stay more present when music is playing (which is a lot of the time) since you never know if a moment will meet your inner arbitrary threshold for adding it to the list.\nI would also be interested to know if anyone after reading this does go ahead and do it. I’d be very keen to have an intense music psychology conversation about similarities in partaking in this activity. I’ve also linked here the past four years just for proof, but also want to show off how diverse this kind of playlist can get since it’s not bound by anything like genre, style, mood, or some sort of lyrical similarity.\n       Pun intended↩\n   ","date":1545350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545350400,"objectID":"76b0064d71e08511f76f1c398d620b68","permalink":"/post/cataloging-memories-with-music/","publishdate":"2018-12-21T00:00:00Z","relpermalink":"/post/cataloging-memories-with-music/","section":"post","summary":"The past few times that I have sat down to work on my dissertation I have run into a writer’s block. Though it is not the kind of writer’s block where nothing comes out.","tags":["music","playlists","memory"],"title":"Cataloging Memories with Music","type":"post"},{"authors":null,"categories":null,"content":"","date":1542240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542240000,"objectID":"cb37ce44162b2a50fb76f3986e94ebdc","permalink":"/talk/open-university-2018/","publishdate":"2018-11-15T00:00:00Z","relpermalink":"/talk/open-university-2018/","section":"talk","summary":"Despite its ubiquity in music conservatory curricula, research on topics pertaining to aural skills is limited at best. While anthologies of materials for sight singing and melodic dictation exist, how people commit melodies to memory is not well understood. This problem is difficult to tackle given the amount of factors that may contribute to the process, such as the complexity of the melody, the degree of exposure needed to commit a melody to long-term memory, and individual differences in cognitive ability that have been shown to contribute to an individual's performance on musical tasks. In this talk I present findings from an experiment (n = 39) modeling performance on melodic dictation using both individual and musical features. Results from the experiment suggest that computationally abstracted musical features can be used in predicting task performance. While these results are useful as a descriptive model, I additionally propose the basic framework for a cognitive, computational model meant to explain how an individual takes melodic dictation. The model is inspired by from both cognitive psychology, as well as computational musicology with the aim of predicting how individuals perform on melodic dictation exercises.","tags":null,"title":"Modeling Melodic Dictation","type":"talk"},{"authors":null,"categories":[],"content":" Replication Replication. Is it popular and important for research in psychology? Yes. Will reading a replication paper get you all fired up about research? Probably not. That’s OK though, research really shouldn’t be all about new and flashy findings. Every so often we should stop and think about what we are doing.\nToday our lab has a new publication “Examining Musical Sophistication: A replication and theoretical commentary on the Goldsmiths Musical Sophistication Index” that’s accessible online that replicates the Goldsmiths Musical Sophistication Index. The “just-tell-me-if-I-can-keep-using-it” verdict of the paper is: “Yes, knock yourself out”. Scroll to the bottom of the page, grab the APA citation and cite us as Baker, Ventura, Calamia, Shanahan, and Elliott (2018).\nIf you’re keen on psychometrics and are interested in a few of the points we make in the paper, then read on.\nTo give the very short version of the paper in plain English (not abstract-ese) the paper generally goes like this. Over the past century the world of music science has struggled to quantify what it means to be musical. We clearly need some measure of it if we want to make claims about how much engaging with music relates to other constructs, but every time you try to pin down what you think being musical is, you realize you forgot something.\n Measuring Musicality A lot of people have tried to get around this problem a few different ways, but in this paper we decided to focus on the Goldsmiths Musical Sophistication Index since it’s a tool that has getting very popular, very quickly within the music science world and had previously been shown to be valid and reliable in a large UK sample.\nThe inspiration for the paper came from a lab meeting one day at LSU where we were discussing open science and our own research practices, and how we wanted to participate in this important movement. Given the resources we had at the time, we decided it would be a good idea to investigate the replicability of the Gold-MSI.\nWe had a couple of specific reasons for this as well:\n Most researchers are not going to be using samples like that in the original paper (N= 147,000+) and will probably just use it on their WEIRD subject pool, therefore we should investigate if there are any “weird” things that happen when you use it like this. Some researchers break off a part of the Gold-MSI and use just one of the sub scales; is that a good idea? An independent replication of the Gold-MSI would be a valuable contribution and it’d be good to write a paper that might be helpful for others to read and cite.  We had also just read this great paper on Process Overlap Theory and saw a lot of parallels between what Kovacs and Conway had to say when arguing against the idea of g and the idea of measuring anything musical with a latent variable.\nSo we went ahead, grabbed us some data, and spent a lot of time reading Alex Beaujean’s Latent Variable Analysis with R book to dive deep into the world of latent variable modeling. So what did we find?\n Practice In general, the Gold-MSI pretty much behaved as you would expect it to. Using a WEIRD sample, we had a bit of a skewed distribution that people should look out for, but the sub-scale scores pretty much lined up with the means and standard deviations that were originally reported.\nOverall Skew\n Similar Descriptive Statistics\n That said, we did look at the item level data and found anything but normal distributions.\nItem Level Distributions\n This is not the biggest deal in the world due to the estimators you can use with the lavaan package that were used in the original paper, but if you are going to break off a scale to use just a part of the Gold-MSI, it’s worth keeping in mind. We noted the biggest problems with this using the Emotion sub-scale.\nWe also show have a lot of tables in the paper that go over everything from descriptive statistics to model fits.\n Theory Given our generally successful replication, is there anything else researchers should know? We we zoomed out a bit (from painfully up close to the Gold-MSI) and expanded outwards in our discussion. In the paper we highlighted that the Gold-MSI uses latent variables, which basically relies on the same math as calculating g. The construct g is calculated using a bunch of objective tests that correlate with one another, but you still use factor analytic techniques to derive the larger constructs, in this case the General Sophistication score and its sub scales (actually it is even fancier than that since they use a bifactor solution).\nAfter pointing this out we draw upon a few arguments made by Kovacs and Conway and argued that even though someone might score highly on some of the Gold-MSI scales, it doesn’t mean that they are using their “musical sophistication” to actually carry out musical tasks. Just like you wouldn’t say you used your general intelligence to solve a mental rotation puzzle, you wouldn’t say you used your musical sophistication to do a melodic memory task. We note that this is great for descriptive theories, but we just want to remind people that they shouldn’t confuse a statistical abstraction for any sort of process that is actually happening.\nWe get a lot more fanciful with the language in the article, but hopefully you’ll give it a read if you’ve gotten this far in the blog post. The paper ends saying that people should of course keep using the Gold-MSI as we desperately need more consistency in measurements, but as a community we need to think about what our models are actually telling us. This is especially true in that we are now coming up on the 100 year anniversary of Seashore’s Measures of Musical Talents and it’s a perfect time to stop and reflect on the tools we are using and the questions we are trying to ask.\nIf this sounds up your alley, give the paper a read. You can access it here, and if you’d like to cite us, just copy and paste below!\nWe’re big proponents of collaborating, sharing data, and being transparent, so if anyone wants to get their hands on our data and analysis, please get in touch and check out our lab’s OSF page!\nAlso a big thanks to our reviewers! The paper was of much higher quality after the suggestions!\n Baker, D. J., Ventura, J., Calamia, M., Shanahan, D., \u0026amp; Elliott, E. M. (2018). Examining musical sophistication: A replication and theoretical commentary on the Goldsmiths Musical Sophistication Index. Musicae Scientiae. https://doi.org/10.1177/1029864918811879\n  ","date":1542153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542153600,"objectID":"95a46eb5cc1660d81e2ef944e252a6aa","permalink":"/post/examining-musical-sophistication/","publishdate":"2018-11-14T00:00:00Z","relpermalink":"/post/examining-musical-sophistication/","section":"post","summary":"Replication Replication. Is it popular and important for research in psychology? Yes. Will reading a replication paper get you all fired up about research? Probably not. That’s OK though, research really shouldn’t be all about new and flashy findings.","tags":[],"title":"Examining Musical Sophistication","type":"post"},{"authors":["D Baker, J Ventura, M Calamia, D Shanahan, E Elliott"],"categories":null,"content":"","date":1539216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539216000,"objectID":"bff75485b3d8af39b6798fa4c845922e","permalink":"/publication/gmsi-replication-2018/","publishdate":"2018-10-11T00:00:00Z","relpermalink":"/publication/gmsi-replication-2018/","section":"publication","summary":"The difficulties associated with measuring the complex construct of musicianship have received conseriable attention in the music psychology literature. Multiple measures exist for various constructs, yet the need for careful replication and documentation of the use of these measures remains an area of critical importance. Here, we describe the replication of the Goldsmiths Musical Sophistication Index in a sample of 346 university students, drawn from both a School of Music and a Department of Psychology. The original approach to modeling the Gold-MSI was followed as closely as possilble, and the results replicated well. Issues were noted, however, with the characteristics of the sample, the skew of some of the individual items, and the overal used of the bifactor structure. These findings are discussed in relation to the state of measuring musicianship in the current literature, as well as in relation to the larger theoretical concerns surrounding the modeling of complex psychological and musical constructs.","tags":null,"title":"Examining Musical Sophistication: A replication and theoretical commentary of the Goldsmiths Musical Sophistication Index","type":"publication"},{"authors":["J Akkermans, R Schapiro, D Mullensiefen, D Shanahan, D Baker, V Bush, K Lothwesen, P Elvers, T Fishinger, K Schlemmer, K Frieler"],"categories":null,"content":"","date":1538870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538870400,"objectID":"bf6a8e0c58cbb1cc17c5ca1d23d7cbf1","permalink":"/publication/emotion-replication-2018/","publishdate":"2018-10-07T00:00:00Z","relpermalink":"/publication/emotion-replication-2018/","section":"publication","summary":"With over 500 citations reported on Google Scholar by February 2017, a publication Juslin and Gabrielsson (1996) presented evidence supporting performers' abilities to communicate, with high accuracy, their intended emotional expressions in music listeners. Though ther ehave been related studies published on this topic, there has yet to be a direct replication of this paper. A replication is warranted given the paper's influence in the field and the implications of the results. The present experiment joings the recent replication effort by producing a five-lab replication using the original methodology. Expressive performances of seven emotions (e.g. happy, said, angry, etc.) by professional musicians were recorded using the the same three melodies from the original study. Participants (N = 319) were presented with recordings and rated how well each emotion matched the emotional quality using a 0-10 scale. The same instruments from the original study (i.e. violin, voice, and flute) were used, with the addition of piano. In an effort to increse accessibility of the experimentand allow for a more ecologically-valid environment, the recordings were presented using an internet-based platform. As an extension to the original study, the experiment investigated how musicality, emotional intelligence, and emotional contagion might explain individual differences in the decoding process. Results found overall hihg decoding accurancy using the method of analysis from the orignal study. Unlike in the original study, the voice was found to be the most expressive instrument. Generalized linear mixed effects regression modelling revealed that musical training and emotional engagement with music positively influences emotion decoding accuracy.","tags":null,"title":"Decoding emotions in expressive music performance: A multi-lab replication and extension study","type":"publication"},{"authors":null,"categories":null,"content":"","date":1532736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532736000,"objectID":"7e8f2708bc1ec469849e388214fdf888","permalink":"/talk/icmpc-wmc-2018/","publishdate":"2018-07-28T00:00:00Z","relpermalink":"/talk/icmpc-wmc-2018/","section":"talk","summary":"Recent work in music psychology has examined the relationship between individual differences and factors that predict various aspects of musical sophistication. Some of the recent research has begun to model how musical sophistication or aptitude relates to various cognitive measures, ranging from executive functions, to measures of general fluid intelligence. Recent research has also investigated how differences in musical training may lead to differences in working, short-term, and long-term memory capacity.While some of the previously mentioned work uses continuous measures of musical sophistication, many only collect data on years of formalized musical training as opposed to a more multi-faceted view of musical sophistication. The aim of this paper is to share findings from a large study investigating how musical sophistication,as measured by the Goldsmiths Musical Sophistication Index (Gold-MSI), relates to measures of working memory and general fluid intelligence. Results using structural equation modeling (SEM) suggest working memory capacity and general fluid intelligence explain more of the variance in perceptual tasks than self-report measures of musical sophistication. In light of these findings, we suggest that further models of music perception should focus on modeling what processes contribute to a task, rather than using large,composite latent variables.","tags":null,"title":"Examining Objective and Subjective Aspects of Musical Sophistication: Insights from General Fluid Intelligence and Working Memory Capacity","type":"talk"},{"authors":null,"categories":[],"content":" Staying Organized The semester is finally over meaning it’s time to put some serious dents into my research projects. I’ve got a couple floating around at this point, but the one that I need to think about the most is my PhD dissertation. From what I’ve heard it’s quite an intensive ordeal and given that over the next year I need to write a multi-chapter document with a dissertation committee spread across thousands of miles, I think it might be important to try to stay organized.\nA couple of different people I follow on Twitter have suggested that I use Scrivner to keep track of writing and what not, but for the past year or two I have really been trying to push myself to commit exclusively to using Free and Open Source Software for everything I do. For some time now I have been thinking about trying to write my entire dissertation using all things R and RStudio and I think I’m ready to try it. I don’t want to get on a rant here about FOSS and how great R is, but let me just give a few reasons why I want to head down this path as opposed to using proprietary, paid software.\nIt’s free Learning the ins and outs of all of this is going to make me a better programmer/researcher over the course of the next year (more skills = more employment options!) Working with this will help make my work more accessible and reproducible  That said, I’m not only going to try to write my thesis using only free software (I’m sure there will be exceptions), but one thing I also want to do is to document as much of the process as I can so that anyone else (especially those without computer backgrounds!) can do the same if they would ever want to learn these sets of tools. I’ve always thought it’s a shame that a lot of this software is not common place in the Humanities and hope that through blogging about how to do it, it will help others who want to use it find it easier.\nSo where do you even start?\n R Projects and Git(hub) When starting a new project, the first thing that I normally do is to create a place on my computer where the entirety of that project can live and then link that up to github so I can share what I do with the people I work with.\nIn this post I will just walk through how to just set up a version controlled project with R via Github.\nStep One: Create a repository on Github I find it’s easiest when starting a project to start on github first, then copy that onto your computer.\nThe first thing to do is to log into your github account (assuming you have one, it’s easy to sign up!) and create a new repository (repo) using the little plus at the top right of your home page as you can see below.\nNavigate to New Repository Page\n From here you want to give your repo a name, a small description, and tick the box that asks you initialize it with a README file. The screenshot below shows the page right before I click the green “Create Repository” button.\nSetting up your repo\n When choosing a name, it’s important to pick something easy to recognize and type. Once you click ‘Create Repository’, it will take you to your repo’s home page and should look something like this:\nThe Blank Page\n From here you are going to want to click the green ‘Clone or download’ and then copy that link to your clipboard. The next screen shot shows what that will look like.\nClone the Git HTTPS\n With the link copied, you then want to open up your terminal (if you’re a Windows user, make sure to get Gitbash!!) and then navigate to where you want your project to live. I keep all my projects in a directory called projects on my desktop for easy access. From here, you then want to use git to clone your project. If you’re totally new to using this kind of software to do work, you’ll also have to install git. If you don’t have git, this page will show you how to get it via homebrew at the command line, which, SURPRISE! you also have to download separately!! You can get that here.\nSide note: when you are first starting out with all things computer-program-tech-whatever, you are going to need to get a lot of software before you can even start to use all the fun stuff. I remember this being particularly discouraging and no fun when I started. If you can, it’s best to try to get someone to slowly walk you through this via some pair programming. If not, don’t be afraid to ask questions when you start!\nSo getting back to what we were doing, you need to navigate to where you want your project to live (the first line) and then clone your directory by typing git clone yourproject.git.\nCloning Your Repo\n Now if you look in your projects folder (or just press ls as in “list” all the files that are in your directory), you’ll notice you have a new directory sent directly from github heaven!\n Step Two: Making It an R Project Since I’m going to primarily using R to manage this project, I then need to turn this directory into an R project so that every time I open this project, it can be governed with R.\nIn order to do that, I need to get out of terminal, and then open up RStudio. From here you need to go to File \u0026gt; New Project \u0026gt; Create From Existing Directory \u0026gt; \nThen navigate and select to open the folder you just cloned from github.\nIf you then tick the box that says to open the box in a new project, you’ll then see something like this:\nTah dah! We’ve now made our directory an R project and can save our changes on github.\n  Step Three: Pushing it the Cloud Now all that’s left to do is sync up the changes on our local machine with that on github. We can bring terminal back and assuming that we are in our project’s directory (we can check that by typing pwd in Terminal), we then need to enter the work life cycle of updating a github account with the following commands:\ngit add . git commit -m \u0026quot;Whatever you did\u0026quot; git push  Or how I did it with my project here:\nIf we then go back to our project page on Github.com, we can see that our changes should have been updated!\nI should note that each time you start your project you want to git pull to make sure that you have the most updated version of the project (for when you are eventually collaborating with other people).\nIn writing this I actually realized there is SO MUCH that underlies this seemingly basic process of starting a project, so if anyone has any questions on this whole process, please let me know and I can try to make this post (and future ones) clearer.\n ","date":1525737600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525737600,"objectID":"93a830c31a8a4d439cb968e1be644fb3","permalink":"/post/project-management-with-git-and-r/","publishdate":"2018-05-08T00:00:00Z","relpermalink":"/post/project-management-with-git-and-r/","section":"post","summary":"Staying Organized The semester is finally over meaning it’s time to put some serious dents into my research projects. I’ve got a couple floating around at this point, but the one that I need to think about the most is my PhD dissertation.","tags":[],"title":"Project Management with git and R","type":"post"},{"authors":null,"categories":["Grad School","Productivity"],"content":" As I write this sentence I know exactly how much time until my next break (45 minutes and 39 seconds if you’re interested). When my Activity Timer dings I’ll stop what I am doing, even if I am mid-thought and do something else. Maybe I’ll make a cup of tea, or maybe attend to that unread Facebook message, but most likely get on Twitter. I really have no idea what I will do, but I do know that as soon as my clock is not counting down I am technically ‘off the clock’.\nThis might seem absolutely insane, but when it comes to productivity and being a graduate student, if you don’t find something that works for you, you’ll find yourself feeling unbelievably stressed at all times. I’m weirdly proud of my own productivity system and after a brief exchange with Thomas Mock on Twitter yesterday I figured it might be worth sharing. By documenting exactly how I do what I do, it’d also remind me to practice what I preach as I enter the last four weeks of the semester here at LSU.\nMost of this falls into two categories:\nTo-do lists and Boundary setting at the local and global level.  I’ll explain each in turn.\nTo Do Lists Some of the best advice on to-do lists I ever received was from Richard Lewis when I was an RA on the Transforming Musicology project. He was managing a huge amount of the project, as well as his own work at the time and I asked him how he did it. He showed me his to-do list. It was a simple text file (come to think of it, I think it was emacs org-mode) and gave me the sage advice of\n “only write down something on a to-do list that you can do.”\n As dumb as it sounds, think of all the times you have written down something like ‘Write Final Paper for American Music Class’ as an item on a to-do list. You can’t actually DO that in one sitting (and if you DO do things like that, you might have a perfectionism/procrastination problem and HIGHLY recommend this book to read), but you can DO all the component parts of said project. What you have do is find a way to organize all the ‘things’ you have to do and break them down into manageable steps. I structure my to-do list hierarchically into all of the projects or classes that I am currently working on. My to-do list is a text file that I edit with VIM that takes advantage of vimoutliner where you can find here. I access it by opening up my terminal (I also work as much as I can in terminal on Mac because I am easily distracted and like to feel like I am hacking in The Matrix) via an alias command. I just type ‘workflow’ into my command line and it opens up the to-do list. To learn how to set this up, check out this link here.\nThe file is organized hierarchically and vimoutliner color codes the indents. At the top of the file I have major deadlines that I shouldn’t forget about. As you look down the list there are high level headings like industry, lsu, and personal. Under each larger level umbrella is the actual project which has its own folder/directory (often linked to a github repo because I love those green commit squares). Then under the project are the actual to-dos. Each line starts with an action verb in all caps followed by something I could do in one sitting. Important to-dos get a ; which my text editor colors as red, which I associate with as important.\nHere is what my current one looks like:\nSo when I am ready to work, all I do is type ‘workflow’ into terminal, then I have a whole list of things I can pick based on priority and what I have the mental energy for. By having a few things to choose and having everything written down my work is out of sight, out of mind for when I am not “working”. I often tell people that if I don’t write it down, I won’t do it. It’s a double edged sword because on the positive side, you don’t feel the mental heat I was tweeting about yesterday as things pile up. They are just all on your to-do list and you’ll get to them when you sit down to work. The negative side is that if you use this system and forget to write it down immediately, you will forget to do it.\nSo with all this stuff to do, how do I actually then do it? In order to answer that question, I need to take a step back and talk about boundaries.\n Boundary Setting The best and worst thing about graduate school is the lack of structure. It’s fantastic in that I can work when I want, where I want, and as long as I meet my deadlines, I wont be hanged, drawn, and quartered by my advisers. It’s terrible in that with no set boundaries, it creates this constant ‘I should be working right now’ culture (a world I am very familiar with coming from an undergraduate degree in music where you just replace ‘working’ with ‘practicing’). To be fair, my advisers would never do that, they might get a bit irritated but they are the nicest, most supportive people ever. I have found that the best way to work around this structure problem is to set very firm boundaries both at a local and global level.\nLocal Boundaries One of the hardest things to do with work is to just start. Often I don’t start to work because I feel like the problems that I have to tackle are just too big. Add in a bit of self-doubt, fear of your work not being good enough, and looming thoughts of a terrible academic job market that your peers constantly remind you of and it’s almost like why even bother? The best way I get around that is to just start working by picking a certain amount of time you think you can commit your full attention to something. It might be five minutes, ten minutes, the holy twenty five minute pomodoro, or you might have the juice to sit down for 50 minutes (this session). With that in mind, or knowing what I kind of work I feel like doing (writing, coding, emails) I press ‘start’ on my timer, open up my workflow and pick one of the things I already wrote down to do that could be done in one session. While the clock is ticking down ONLY WORK ON THAT ONE PROJECT. No Facebook, no phone, no Twitter. Don’t even start a conversation with your lab mate that walks in. If they start talking to you, practice saying ‘No’ to them and tell them you are doing a pomodoro session and will be done in 9 minutes and 19 seconds (current clock). When the timer goes off, stop. Even if you are almost done! The reason for this is that so you can get a quick breath so when you press ‘Start’ the next time, you will know exactly how to start your work!\nI find that doing this just a few times will get me in a much healthier place in terms of work life boundaries. And the best thing about it is as soon as the clock stops, you’re done and can make a cup of tea or coffee.\n Global Boundaries The other important catch to this system is setting global boundaries. What I mean by this is that you need to start working at about the same time every day and end at a reasonable time. I try to get into the office everyday at about 9AM and leave at about 6PM. It’s a long-ish work day (I take an hour lunch), but the best part is that having this regularity makes me happy. It might seem like I am just doing a 9-5 thing, but since I’m a grad student (and ABD right now) I can just stop whenever I want! The thing is I often do not because I weirdly really like what I do. The whole point of setting boundaries it to avoid burnout!\nIn addition to keeping semi-regular work hours I also do not have email or slack on my phone. My first music theory professor on my first day of class of undergraduate told us ‘there is no such thing as a music theory emergency’. He is correct. Most problems are not emergencies and the rest you get at night is much better than being on call 24/7. If you are staying on top of your work and working healthily to a goal, this system is supposed to prevent you from running into a place where you are submitting something at the last possible second.\n Closing Thoughts If after reading that you think “Dave, sounds like you just have a cushy grad position, you probably don’t work that hard. On the other hand I work 60-70 hours a week as a grad student!” I would have two things to say to you. The first is that I don’t believe you are as productive as you could be. I’d personally pay for the Go-Pro for you to wear all week and would be interested to know much time people accidentally waste each day looking at social media and doing other unproductive things. The other is that you might consider your ability to say ‘No’ to people. One of the hardest things I have hard to learn is to say ‘No’ and not let productivty creep happen (become more productive, get more time, fill free time with work). Saying ‘No’ is very hard but I actually got a lot of help learning about it when I read Henry Cloud’s book on Boundaries about six years ago. I could do a whole blog post on that book.\nWhile there is something to be said about just falling deep into your work, especially at the start of grad school, I always try to do a bit of self reflection and ask myself if what I am doing is sustainable. The first two years of my PhD were not and it lead to me being very depressed and irritable at times. I really credit some of these ideas I just detailed with moving away from a lot of those very toxic mindsets that permeate many academic’s social media feeds (WORK WORK WORK!). I’d really like to have my cake and eat it too with a healthy work/life balance and this is just one idea that moves me closer to that goal.\nAlso for those keeping score at home this post took 1 50 minute session, one 20 minute session, then I went to bed, and did three more 20 minute sessions to get it done and posted.\n  ","date":1523232000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523232000,"objectID":"9155930e8b8d18afc605e87c5e0cf50e","permalink":"/post/to-dos-and-boundaries/","publishdate":"2018-04-09T00:00:00Z","relpermalink":"/post/to-dos-and-boundaries/","section":"post","summary":"As I write this sentence I know exactly how much time until my next break (45 minutes and 39 seconds if you’re interested). When my Activity Timer dings I’ll stop what I am doing, even if I am mid-thought and do something else.","tags":["productivity","vim","pomodoro","grad school","10 Minute Reads"],"title":"To-Dos and Boundaries","type":"post"},{"authors":null,"categories":null,"content":"","date":1521763200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521763200,"objectID":"3d94048340da69c670cfbd4008a3288d","permalink":"/talk/scsmt-2018/","publishdate":"2018-03-23T00:00:00Z","relpermalink":"/talk/scsmt-2018/","section":"talk","summary":"Rock music differs in its functional harmonic behaviors from more well-studied repertoires, such as the music of the common practice. Before it is possible to conduct harmonic analyses of the rock repertoire, one must arrive at an understanding of those harmonic expectations that are unique to rock music. There have been several recent attempts along these lines, but the literature is conflicted on its basic approaches to the problem (Stephenson, 2002; Nobile, 2016; Acevedo, 2017). In this paper, we put forward a model of rock music schemas that synthesizes and streamlines past approaches. Rather than deriving harmonic exceptions from a chord’s placement within the scale, we propose instead that listeners derive their harmonic expectations from root motion schemas. We extracted these root-motion schemas through a corpus analysis of both the Temperley/de Clercq Rock corpus and the McGill Billboard corpus. Analysis based on our approach provides empirical evidence for claims made by prior scholars. In addition, we are confident that our model captures the essential differences between rock music and that of the common practice. The model of harmonic expectations generated by our theory provides a simple framework for harmonic analysis of rock music in future research. ","tags":null,"title":"Root Interval Schemas in Rock Music","type":"talk"},{"authors":null,"categories":null,"content":"","date":1520035200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520035200,"objectID":"6e8eda755267af68212c0ab61be8f957","permalink":"/talk/pmd-musician-2018/","publishdate":"2018-03-03T00:00:00Z","relpermalink":"/talk/pmd-musician-2018/","section":"talk","summary":"","tags":null,"title":"Who counts as a musician?","type":"talk"},{"authors":null,"categories":["R","10 Minute Reads"],"content":" Since this is my last post in the beer review series, I’ll keep this short and sweet in terms of analysis. Having done all of this, I do have a few reflections I would like to share after doing the One-Size-Fits-All-Data-Science Interview that I have included at the end.\nOur final question to answer is:\n Lastly, if I typically enjoy a beer due to its aroma and appearance, which beer style should I try?\n This is a pretty broad question and should be able to be answered without many pitfalls, so let’s get started.\nlibrary(ggplot2) library(data.table) beer \u0026lt;- fread(\u0026quot;data/beer_reviews.csv\u0026quot;)  From a bird’s eye view, it seems like the most sensible thing to do would be to look at our data from the highest level, then just zoom in until we have the level of granularity we feel answers the question well. Let’s first average all the beer styles to get a rough estimate of how a beer style fairs on the variables we are interested in, and additionally see how much variability is associated with that measurement.\nsem \u0026lt;-function(x) {sd(x)/sqrt(length(x))} question4.means \u0026lt;- beer[, .(mean_aroma = mean(review_aroma), mean_appearance = mean(review_appearance), mean_overall = mean(review_overall), sem_aroma = sem(review_aroma), sem_appearance = sem(review_appearance), sem_overall = sem(review_overall), sd_aroma = sd(review_aroma), sd_appeareance = sd(review_appearance),sd_overall = sd(review_overall)), by = beer_style] question4.means ## beer_style mean_aroma mean_appearance mean_overall ## 1: Hefeweizen 3.761735 3.828293 3.929626 ## 2: English Strong Ale 3.749427 3.852469 3.783288 ## 3: Foreign / Export Stout 3.828366 4.039015 3.877679 ## 4: German Pilsener 3.387159 3.572444 3.731573 ## 5: American Double / Imperial IPA 4.097782 4.078916 3.998017 ## --- ## 100: Gueuze 4.117574 4.034864 4.086287 ## 101: Gose 3.783528 3.908163 3.965015 ## 102: Happoshu 2.595436 2.925311 2.914938 ## 103: Sahti 3.827992 3.655985 3.700283 ## 104: Bière de Champagne / Bière Brut 3.734704 4.045889 3.648184 ## sem_aroma sem_appearance sem_overall sd_aroma sd_appeareance sd_overall ## 1: 0.003668940 0.003595729 0.004051038 0.6129217 0.6006912 0.6767538 ## 2: 0.008118012 0.007674182 0.009323636 0.5623738 0.5316275 0.6458931 ## 3: 0.007222404 0.006830955 0.008163490 0.5581381 0.5278874 0.6308640 ## 4: 0.004611304 0.004323963 0.005097580 0.6863721 0.6436027 0.7587521 ## 5: 0.001937927 0.001600133 0.002171618 0.5682357 0.4691883 0.6367582 ## --- ## 100: 0.007225256 0.006450020 0.008273156 0.5600855 0.4999910 0.6413163 ## 101: 0.019413627 0.015860893 0.023754558 0.5084740 0.4154222 0.6221699 ## 102: 0.048722437 0.051373864 0.063538226 0.7563756 0.7975368 0.9863785 ## 103: 0.019516104 0.017677122 0.021691778 0.6356980 0.5757968 0.7065662 ## 104: 0.021782360 0.018920362 0.026781986 0.7044834 0.6119209 0.8661809 Knowing how each beer style fluctuates on our variables of interest (with our overall score thrown in for good measure!), let’s plot our results. Note that I have included standard error of the mean error bars as a sanity check to make sure that each beer’s ratings is not going to bleed into the others’ dimensions. By doing this, we can have a bit more confidence that we are looking at actually has some meaning. This plot shows the standard error on each of the two variables we are interested in, and for the most part it looks as if they are relatively well contained.\nggplot(question4.means, aes(x = mean_aroma, y = mean_appearance, color = beer_style)) + geom_point() + geom_errorbar(aes(ymin=mean_appearance-sem_appearance, ymax=mean_appearance+sem_appearance), width=.1) + geom_errorbarh(aes(xmin=mean_aroma-sem_aroma, xmax=mean_aroma+sem_aroma)) + theme(legend.position=\u0026quot;none\u0026quot;) + labs(title = \u0026quot;Mean Appearance and Aroma\u0026quot;, y = \u0026quot;Mean Aroma\u0026quot;, x = \u0026quot;Mean Appearance\u0026quot;)  This graph has a lot of beers, but what we are interested in is that top right quadrant where both the average appearance and aroma are maxed out. Let’s zoom in on it and throw in a sizing variable of the overall rating and inspect our graph.\nggplot(question4.means[mean_appearance \u0026gt; 4 \u0026amp; mean_aroma \u0026gt; 4], aes(x = mean_aroma, y = mean_appearance, color = beer_style, size = mean_overall)) + geom_point() + xlim(4,4.2) + ylim(4,4.25) + theme(legend.position=\u0026quot;none\u0026quot;) + # geom_errorbar(aes(ymin=mean_appearance-sem_appearance, ymax=mean_appearance+sem_appearance), width=.1) + # geom_errorbarh(aes(xmin=mean_aroma-sem_aroma, xmax=mean_aroma+sem_aroma)) + theme(legend.position=\u0026quot;none\u0026quot;) + labs(title = \u0026quot;Mean Appearance and Aroma\u0026quot;, y = \u0026quot;Mean Aroma\u0026quot;, x = \u0026quot;Mean Appearance\u0026quot;) + geom_text(aes(label=beer_style, hjust = .5, vjust = -.75))  Looking at this subsection, it appears we have a few choices for beers that score highly on both Appearance and Aroma. The American Double / Imperial Stout looks like a good option as it scores higher on how it looks, but our Russian Imperial Stout has a higher Aroma score. We could start crunching more numbers here to find “the best” option, or at this point we could take off our data science hats and put our psychology ones back on (assuming that’s what we were wearing…) and run some double-blind experiments on ourselves to make sure that our data is actually lining up with something in the real world.\nReflections What started out as a fun weekend project actually turned into a really great learning experience. I’ve definitely put a few solid hours into this and have gotten a lot out of it. I learned that my friends actually know a TON about beer and data science, that git-lfs is something I wish I would have known about earlier, and that I’m actually looking forward to doing more blogging in the future.\nI will probably have to go MIA for the next two weeks since my General Exams are coming up in early February, but I’m sure I will be back at it come late March. Until then, all I can hope for is that someone who is looking to hire a junior data scientist over the summer will come across these posts and think I might be a good temporary addition to their team.\n ","date":1517184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517184000,"objectID":"6f016b715d0ea380712ee0a263caf1c1","permalink":"/post/hire-me-as-a-data-scientist-part-iv/","publishdate":"2018-01-29T00:00:00Z","relpermalink":"/post/hire-me-as-a-data-scientist-part-iv/","section":"post","summary":"Since this is my last post in the beer review series, I’ll keep this short and sweet in terms of analysis. Having done all of this, I do have a few reflections I would like to share after doing the One-Size-Fits-All-Data-Science Interview that I have included at the end.","tags":["R","Data Science","Beer"],"title":"Hire Me (as a Data Scientist!), Part IV","type":"post"},{"authors":null,"categories":["R","10 Minute Reads"],"content":" Two questions down, two to go! For the third post I’ll explore the question:\n Which of the factors (aroma, taste, appearance, palate) are most important in determining the overall quality of a beer?\n Whereas the posts before were questions on sorting data, this is our first attempt to make some statistical models. In this case, we’re going to be doing a bit of regression modeling.\nThere are a couple of ways to tackle this problem. We could run some basic linear regression models and spend the whole post talking beta coefficients and what assumptions we violated. We could do a linear mixed effects model, then realize that doing so would be a terrible choice (I tried it for the fun of it, bad idea). Or we do a fun non-parametric, machine learning model that is on the easier-to-interpret side. Since machine learning is so hot right now, let’s stick with that.\nMachine Learning and Non-Parametric Models Non-parametric models make no assumptions about the data. The models do not assume that our points come from beautiful, normally distributed ideal populations; they just seek to find some way to give us a good rule of thumb about what is happening under the hood.\nIn this case, we need to make a model to predict the quality of beer (our dependent variable, review_overall) based on four different independent variables (review_aroma, review_taste, review_appearance, review_palate).\nAfter discussing the pros and cons of certain methods for tackling this problem, my friend Tabi, the data scientist over at Soundout, suggested that an easy way to get the answer I was looking for was to use a random forest model. Some great explanations about how these models work can be found here and here, and here and since this isn’t a post about how random forest models work, I’ll just note that basically the idea is that it is an algorithm that partitions your dataset into dimensions that help us either classify or predict observations in our dataset based on the variables you feed it. Relevant to our question: the ways in which the algorithm splits our dataset is going to help us figure out what are the important variables.\nBefore running this model though, we need to talk about a small dependence problem in our dataset. In my earlier post, I talked about how we probably should not just run a model on the data as is. Last time we found there were tons of NAs in our dataset, that not all beers were equally represented, and that not all reviewers were making even amounts of reviews. In addition to these problems, there was also the problem that some reviewers might rate generally higher or lower all the time. Since we know things like this exist, we wanted to account for them.\nThe simplest plan of action would be to try and make each of the points we want to model independent by averaging ratings across all beers so we don’t have cases where one person’s influence is spread across multiple beers. We also make sure to only use beers that have over 30 ratings as a quality check. Let’s index out the data we need!\nlibrary(ggplot2) library(data.table) beer \u0026lt;- fread(\u0026quot;data/beer_reviews.csv\u0026quot;)  # Make READABLE unique beer ID beer[, beer_name_unique := paste(brewery_name,beer_name, beer_style)] # Only beers with over 30 reviews reliable.beers \u0026lt;- beer[, .(ReviewsBeerHas = .N), by = beer_name_unique][order(-ReviewsBeerHas)][ReviewsBeerHas \u0026gt; 30] # Merge with Inner Join beer.reliable \u0026lt;- reliable.beers[beer, on = \u0026quot;beer_name_unique\u0026quot;, nomatch=0] # Make independent ratings prediction.data \u0026lt;- beer.reliable[, .(AvgOverall = mean(review_overall), AvgAroma = mean(review_aroma), AvgTaste = mean(review_taste), AvgApp = mean(review_appearance), AvgPal = mean(review_palate)), by = beer_name_unique] Now that we have some better data, let’s fit a random forest model. We are now attempting to predict the average overall rating from our four others measures. We’ll use the randomForest package and make sure to ask it to include a measure of variable importance.\nlibrary(randomForest) ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: \u0026#39;randomForest\u0026#39; ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## margin set.seed(666) random.forrest.fit \u0026lt;- randomForest(AvgOverall ~ AvgAroma + AvgTaste + AvgApp + AvgPal, data = prediction.data, importance = TRUE) random.forrest.fit$importance ## %IncMSE IncNodePurity ## AvgAroma 0.02293972 219.2851 ## AvgTaste 0.09678963 304.5727 ## AvgApp 0.01566894 198.3329 ## AvgPal 0.05588260 276.6567 The randomForest object gives us two different measures when it comes to variable importance. The first one, %IncMSE, is a measure that tells us how much our model’s Mean Square Error (MSE) would change if we were to take that variable out of our model. Average taste here is trouncing the other variables in terms of importance. The second variable, IncNodePurity gives us a measure of node purity from all of the trees that were used in creating our model. Here Taste again leads in terms of importance, but our Palate variable seems to be close behind. We see this visually below.\nvarImpPlot(random.forrest.fit, main = \u0026quot;Variable Importance Metrics\u0026quot;) The result that Taste seems to be taking the cake here is to be expected. Using some of the more basic tools of statistics and data science, we can look at the correlation between our taste variable and our overall, and it’s quite high. Actually looking at our average ratings, all of the correlations are really high!\ncor(prediction.data[,-1]) ## AvgOverall AvgAroma AvgTaste AvgApp AvgPal ## AvgOverall 1.0000000 0.8809340 0.9518373 0.8441311 0.9360873 ## AvgAroma 0.8809340 1.0000000 0.9593160 0.8971965 0.9370526 ## AvgTaste 0.9518373 0.9593160 1.0000000 0.8955728 0.9742888 ## AvgApp 0.8441311 0.8971965 0.8955728 1.0000000 0.9124038 ## AvgPal 0.9360873 0.9370526 0.9742888 0.9124038 1.0000000 Doing the analysis with this data presents some strange issues of collinearity. Having an r = .951 for our Overall and Taste variables is obnoxiously high. An r = .974 for Palette and Taste is also strangely large. If you come from more of a psychology background, you would almost never see correlations this high in the wild; it would be an immediate sign for concern.\nThe correlations go down a bit if you end up looking at the non-aggregated sets too (see below), but again remember that these values have those dependence and outlier issues with them. Still, review_taste and review_overall are the highest correlated variables.\ncor(beer[, .(review_overall,review_aroma,review_appearance,review_palate,review_taste)]) ## review_overall review_aroma review_appearance review_palate ## review_overall 1.0000000 0.6160131 0.5017324 0.7019139 ## review_aroma 0.6160131 1.0000000 0.5610290 0.6169469 ## review_appearance 0.5017324 0.5610290 1.0000000 0.5666339 ## review_palate 0.7019139 0.6169469 0.5666339 1.0000000 ## review_taste 0.7898156 0.7167761 0.5469804 0.7341351 ## review_taste ## review_overall 0.7898156 ## review_aroma 0.7167761 ## review_appearance 0.5469804 ## review_palate 0.7341351 ## review_taste 1.0000000 This effect is probably due to the fact that higher quality beers tend to score higher on everything. It would be pretty strange in practice to give a beer a 5/5 overall, but then think it’s deserving of a 2/5 in Taste. If I were on a team at Beer Advocate, I might suggest incorporating either a larger range of ratings (maybe a seven point scale) or maybe thinking about different dimensions to ask people to rate like ‘hoppiness’ or bang-for-your-buck. Variables like these would allow us to learn more about the beers since they would have less collinearity issues.\nSo the take-home here is that the taste variable is the most important variable for determining the overall rating and again we’re reminded about how messy this data is!\n ","date":1517097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517097600,"objectID":"bc74e1d5ee8cf0d69ac808b0fa5372f8","permalink":"/post/hire-me-as-a-datascientist-part-iii/","publishdate":"2018-01-28T00:00:00Z","relpermalink":"/post/hire-me-as-a-datascientist-part-iii/","section":"post","summary":"Two questions down, two to go! For the third post I’ll explore the question:\n Which of the factors (aroma, taste, appearance, palate) are most important in determining the overall quality of a beer?","tags":["R","Data Science","Beer"],"title":"Hire Me (as a Data Scientist!), Part III","type":"post"},{"authors":null,"categories":["R","10 Minute Reads"],"content":" Continuing on from my earlier post, I’m now looking to tackle the question:\n If you had to pick 3 beers to recommend using only this data, which would you pick?\n This is a pretty open ended question, which is kind of fun. I also don’t really have a ton of experience (yet!) in recommendation systems, though I have done a little reading here or there on it.\nMy goals in coming up with three beers to recommend were to:\nTry to find the most popular beer among super users of the website Find a bizzaro beer that matched the profile of my first beer, but lives in the long tail of the ratings distribution Find the best Beer sans Booze (Highest Rating with lowest ABV)  So let’s begin! Here’s how I went about tackling this question.\nPopular with Super Users #=====================================================================================# # Following suit of the last post... #=====================================================================================# # Library library(ggplot2) library(data.table) library(stringr) #=====================================================================================# beer \u0026lt;- fread(\u0026quot;data/beer_reviews.csv\u0026quot;) beer.complete \u0026lt;- beer[complete.cases(beer)] #=====================================================================================# Having more experience in experimental settings, one of the first things I needed to get used to when I started working with non-psychology datasets was the lack of complete sets in what felt like almost everything. Whereas in the lab we spend lots of time trying to design balanced studies that hopefully don’t violate the litany of assumptions that classic null hypothesis significance testing demands, my first few attempts at analyzing large amounts of data made me realize it’s almost risible to think that you’re going to have even, independent data, ever. This dataset is no different.\nOf all of the unique users on the site, most of them have done only a couple of reviews, but some have essentially made a job out of this. Looking at the distribution of reviews, this is quite clear.\nreview.counts \u0026lt;- beer[, .(.N), by = review_profilename][order(-N)] review.counts  ## review_profilename N ## 1: northyorksammy 5817 ## 2: BuckeyeNation 4661 ## 3: mikesgroove 4617 ## 4: Thorpe429 3518 ## 5: womencantsail 3497 ## --- ## 33384: beilfussd 1 ## 33385: MPHSours11 1 ## 33386: jennaizzel 1 ## 33387: hogshead 1 ## 33388: joeebbs 1 hist(beer[, .(.N), by = review_profilename][order(-N)]$N, breaks = 200, xlab = \u0026quot;Number of Reviews\u0026quot;, main = \u0026quot;Distribution of Reviews Per User\u0026quot;) This is pretty important when it comes to modeling the data (discussed in Part III), and not being fully aware of where your ratings are coming from could put the quality of your models at serious risk.\nSo looking at this dataset, I wondered if there were any sort of implicit assumptions I could make about this data that might be able to help me find a good beer. One assumption that I didn’t think was too wild was that a sample of this population that had gone out of its way to rate over 500 beers was probably more of a beer expert than those who have only done a couple of reviews on the site.\nOne thing I wanted to check was: of all the 1.5 million reviews, where were they coming from? Were there enough reviews among the super users that I could use? And what made someone a super user? I could have been a bit more scientific, setting an a priori threshold, but for this I kind of just looked at that chart above, spit balled thinking 500 might be a good number to check, and then went to see how much of the data would be accounted if I put my threshold there. I lucked out and got about half of it.\nsum(review.counts$N) # Number of Total Reviews  ## [1] 1586614 sum(review.counts[ N \u0026gt; 500]$N) # Number of Reviews from Super Users ## [1] 731066 731066/1586614 # Percent of Total Reviews from 500+ Super Users ## [1] 0.4607712 super.users \u0026lt;- review.counts[ N \u0026gt; 500] # I can settle for .75 Million Reviews So now I had a list of the users who had completed over 500 reviews and made up 46% of our entire data. I could use this new table I had made to index through our dataset of all the reviews that I had (that have their ABV ratings!) so I was then only dealing with these higher quality reviewers.\nsuper.reviews \u0026lt;- super.users[beer.complete, on = \u0026quot;review_profilename\u0026quot;, nomatch=0] As I continued to chop down the dataset (since this was a very exploratory process compared with cleaning up an experiment), it was important to do quality assessment steps. One thing worth checking here was to see if I was actually dealing with beer omnivores in our super users and make sure that all different types of beers were being represented in our smaller subset. This was done by just looking at the number of rows between the original dataset and our super user table.\nsuper.reviews[, .(beer_styles = unique(beer_style))] ## beer_styles ## 1: Hefeweizen ## 2: English Strong Ale ## 3: Foreign / Export Stout ## 4: German Pilsener ## 5: American Double / Imperial IPA ## --- ## 100: Japanese Rice Lager ## 101: Roggenbier ## 102: Happoshu ## 103: Sahti ## 104: Bière de Champagne / Bière Brut beer[, .(beer_styles = unique(beer_style))] ## beer_styles ## 1: Hefeweizen ## 2: English Strong Ale ## 3: Foreign / Export Stout ## 4: German Pilsener ## 5: American Double / Imperial IPA ## --- ## 100: Gueuze ## 101: Gose ## 102: Happoshu ## 103: Sahti ## 104: Bière de Champagne / Bière Brut Luckily they were the same. If I were to really do some more work on this dataset, I would also want to check things such as how many of the beers had each super user tried? Were there IPA experts in the group? If yes, should their opinions be taken more seriously if I had questions about IPA recommendations in the future? But for now, I just set out to see what the highest rated beer among all the super users of this dataset was.\nIn order to answer that question, I had to find out which beer in specific had the highest mean rating. The dataset ‘as is’ comes with a beer_id unique ID, but the data downloaded as is does not give us a key to this, so I had to make it myself. This was accomplished by just pasting together the brewery’s name, along with the beer name, and style into a new variable.\nAs another quality assurance step, it was worth checking to see if this recreated the unique ID variable, which it didn’t do exactly… but it was pretty close. I would chalk that up to some sort of encoding error.\nsuper.reviews[, beer_name_unique := paste(brewery_name,beer_name, beer_style)] length(unique(super.reviews$beer_beerid)) ## [1] 42825 length(unique(super.reviews$beer_name_unique)) ## [1] 42703 42703/42805 # Pretty close ## [1] 0.9976171 super.reviews.popular \u0026lt;- super.reviews[, .(most_reviewed_beers = .N), by = beer_name_unique][order(-most_reviewed_beers)] hist(super.reviews.popular$most_reviewed_beers, main = \u0026quot;Distribution of Number of Ratings by Super Users\u0026quot;, xlab = \u0026quot;Number of Reviews each Beer Recieves\u0026quot;, breaks = 200) Again I saw this was clearly not anything resembling a repeated measures experiment and not all beers were rated equally.\nContinuing in the same fashion above, I just grabbed the top 100 beers of our super users and merged that on to our table from earlier that had all of the ratings from our super users. Then from that table, I took the average of the overall rating and looked at our top ten beers.\nsuper.reviews.popular.100 \u0026lt;- super.reviews[, .(most_reviewed_beers = .N), by = beer_name_unique][order(-most_reviewed_beers)][1:100] super.reviews.cream.of.crop \u0026lt;- super.reviews.popular.100[super.reviews, on = \u0026quot;beer_name_unique\u0026quot;, nomatch=0] super.reviews.cream.of.crop[, .(mean_review_overall = mean(review_overall)), by = beer_name_unique][order(-mean_review_overall)][1:10] ## beer_name_unique ## 1: Russian River Brewing Company Pliny The Elder American Double / Imperial IPA ## 2: Bayerische Staatsbrauerei Weihenstephan Weihenstephaner Hefeweissbier Hefeweizen ## 3: Tröegs Brewing Company Tröegs Nugget Nectar American Amber / Red Ale ## 4: Ballast Point Brewing Company Sculpin India Pale Ale American IPA ## 5: Three Floyds Brewing Co. \u0026amp; Brewpub Dreadnaught IPA American Double / Imperial IPA ## 6: Founders Brewing Company Founders KBS (Kentucky Breakfast Stout) American Double / Imperial Stout ## 7: Bell\u0026#39;s Brewery, Inc. Two Hearted Ale American IPA ## 8: Bell\u0026#39;s Brewery, Inc. Bell\u0026#39;s Hopslam Ale American Double / Imperial IPA ## 9: Three Floyds Brewing Co. \u0026amp; Brewpub Alpha King Pale Ale American Pale Ale (APA) ## 10: Founders Brewing Company Founders Breakfast Stout American Double / Imperial Stout ## mean_review_overall ## 1: 4.536630 ## 2: 4.535072 ## 3: 4.449084 ## 4: 4.443287 ## 5: 4.367580 ## 6: 4.366876 ## 7: 4.353270 ## 8: 4.349810 ## 9: 4.346652 ## 10: 4.334526 And we have our winner! It’s Pliny The Elder from Russian River Brewing Company as my first beer recommendation!\n Bizzaro Beer Now Pliny The Elder seemed to be a pretty popular beer. But what if I was trying to sketch out some ideas about what other beers I could recommend to beer lovers who like Pliny The Elder? It needed to somewhat “look like” the target beer, but have way less reviews.\nPlaying with some of the fringe data here, I wanted to be careful not to again pick a beer with only one or two ratings on it. My rationale was coming from assuming there is some sort of true “population mean” for this beer and having a beer with too little reviews will not approximate the mean correctly.\n## Make Unique Beer Label for Larger Dataset beer[, beer_name_unique := paste(brewery_name,beer_name, beer_style)] ## Count Number of Reviews Each Beer Has number.of.reviews \u0026lt;- beer[, .(NumberOfReviews = .N), by = beer_name_unique][order(-NumberOfReviews)] ## Only get beers with over 30 reviews reliable.beers.list \u0026lt;- number.of.reviews[ NumberOfReviews \u0026gt;= 30 ] ## Join that to our big \u0026#39;beer\u0026#39; dataset only matching beers with over 30 reviews beer.reliable \u0026lt;- reliable.beers.list[beer, on = \u0026quot;beer_name_unique\u0026quot;, nomatch=0] With our dataset chiseled down to only ‘reliable’ beers, I needed to find a way to get some sort of profile of each of the beers. While my first instinct was to do some sort of data reductive type thing like a PCA on our continuous variables and use distances from certain scores as metrics of similarity (which I have done before and it ended up actually being the inspiration for a tool currently used by Soundout!), doing that on so few predictors seemed extra.\nSo instead, I figured why not just assume that there is some sort of wiggle room in my hastily made recommendation system and just match first on the overall review, then if there are some close contenders, look for matches on other metrics?\nThe next bit of code creates a table of the metrics I am interested in, gets beers that have over 30 reviews, but less than 100, and also creates a vector so I can pull out all of the IPAs on my less reviewed beers table. I then joined the tables for my candidates.\n# # Get metrics used for distance calculations beer.metrics \u0026lt;- beer.reliable[, .(mean_review_overall = mean(review_overall), mean_review_aroma = mean(review_overall), mean_review_appearance = mean(review_appearance), mean_review_palate = mean(review_palate), mean_reviw_taste = mean(review_taste), sd_review_overall = sd(review_overall), sd_review_aroma = sd(review_overall), sd_review_appearance = sd(review_appearance), sd_review_palate = sd(review_palate), sd_review_taste = sd(review_taste)), by = beer_name_unique] ## Get only IPAs with less than 100 reviews less.reviewed.beers \u0026lt;- number.of.reviews[NumberOfReviews \u0026lt;= 100 \u0026amp; NumberOfReviews \u0026gt;= 30] ## Make vector to help find IPAs find.IPA \u0026lt;- str_detect(string = less.reviewed.beers$beer_name_unique, pattern = \u0026quot;Imperial IPA\u0026quot;) bizzaro.candidates \u0026lt;- less.reviewed.beers[find.IPA] #Create Table bizzaro.candidates.metrics \u0026lt;- bizzaro.candidates[beer.metrics, on = \u0026quot;beer_name_unique\u0026quot;, nomatch=0] Of these less reviewed beers, I now needed to find the one that was “closest” on the few dimensions I had to work with. The simplest way to do this would be to just subtract our target beer (Pliny The Elder), from every other beer in our interested list, then check out the results.\n## Get metrics for our target beer rrbcpteadii.metrics \u0026lt;- beer[beer_name_unique == \u0026quot;Russian River Brewing Company Pliny The Elder American Double / Imperial IPA\u0026quot;, .(mean_review_overall = mean(review_overall), mean_review_aroma = mean(review_overall), mean_review_appearance = mean(review_appearance), mean_review_palate = mean(review_palate), mean_reviw_taste = mean(review_taste), sd_review_overall = sd(review_overall), sd_review_aroma = sd(review_overall), sd_review_appearance = sd(review_appearance), sd_review_palate = sd(review_palate), sd_review_taste = sd(review_taste))] ## Create vector for looping over key.vector \u0026lt;- as.vector(rrbcpteadii.metrics) ## Pull off the tags of our search search.vector \u0026lt;- bizzaro.candidates.metrics[, -c(1,2)] ## Sanity check that what we are going to subtract has same names names(key.vector) ## [1] \u0026quot;mean_review_overall\u0026quot; \u0026quot;mean_review_aroma\u0026quot; \u0026quot;mean_review_appearance\u0026quot; ## [4] \u0026quot;mean_review_palate\u0026quot; \u0026quot;mean_reviw_taste\u0026quot; \u0026quot;sd_review_overall\u0026quot; ## [7] \u0026quot;sd_review_aroma\u0026quot; \u0026quot;sd_review_appearance\u0026quot; \u0026quot;sd_review_palate\u0026quot; ## [10] \u0026quot;sd_review_taste\u0026quot; names(search.vector) ## [1] \u0026quot;mean_review_overall\u0026quot; \u0026quot;mean_review_aroma\u0026quot; \u0026quot;mean_review_appearance\u0026quot; ## [4] \u0026quot;mean_review_palate\u0026quot; \u0026quot;mean_reviw_taste\u0026quot; \u0026quot;sd_review_overall\u0026quot; ## [7] \u0026quot;sd_review_aroma\u0026quot; \u0026quot;sd_review_appearance\u0026quot; \u0026quot;sd_review_palate\u0026quot; ## [10] \u0026quot;sd_review_taste\u0026quot; ## And that the apply function I am going to run is doing what I think it will search.vector[1]- key.vector ## mean_review_overall mean_review_aroma mean_review_appearance ## 1: -0.7900277 -0.7900277 -0.6386031 ## mean_review_palate mean_reviw_taste sd_review_overall sd_review_aroma ## 1: -0.6263257 -0.8393187 0.09103239 0.09103239 ## sd_review_appearance sd_review_palate sd_review_taste ## 1: 0.1273011 0.11882 0.1073594 ## Run apply function ipa.distances \u0026lt;- apply(search.vector, 1, function(x) x - key.vector) ipa.distances.dt \u0026lt;- data.table(do.call(rbind.data.frame,ipa.distances)) ## Combine this back with vector with names bizzaro.candidates.distances \u0026lt;- cbind(bizzaro.candidates.metrics, ipa.distances.dt) ## Sort our data by overall and see if we have a good match! bizzaro.candidates.distances[order(-mean_review_overall)] ## beer_name_unique ## 1: Hill Farmstead Brewery Galaxy Imperial Single Hop IPA American Double / Imperial IPA ## 2: Lawson\u0026#39;s Finest Liquids Double Sunshine IPA American Double / Imperial IPA ## 3: Kern River Brewing Company 5th Anniversary Ale American Double / Imperial IPA ## 4: Alpine Beer Company Bad Boy American Double / Imperial IPA ## 5: Iron Hill Brewery \u0026amp; Restaurant Kryptonite American Double / Imperial IPA ## --- ## 132: BrewDog Sink The Bismarck! American Double / Imperial IPA ## 133: Blue Frog Grog \u0026amp; Grill The Big DIPA American Double / Imperial IPA ## 134: Hermitage Brewing Hoptopia American Double / Imperial IPA ## 135: Florida Beer Company Swamp Ape IPA American Double / Imperial IPA ## 136: BrewDog Storm (Islay Whisky Cask Aged IPA) American Double / Imperial IPA ## NumberOfReviews mean_review_overall mean_review_aroma ## 1: 76 4.592105 4.592105 ## 2: 85 4.588235 4.588235 ## 3: 41 4.475610 4.475610 ## 4: 79 4.468354 4.468354 ## 5: 44 4.443182 4.443182 ## --- ## 132: 76 3.197368 3.197368 ## 133: 53 2.867925 2.867925 ## 134: 32 2.687500 2.687500 ## 135: 52 2.615385 2.615385 ## 136: 92 2.440217 2.440217 ## mean_review_appearance mean_review_palate mean_reviw_taste ## 1: 4.368421 4.440789 4.559211 ## 2: 4.317647 4.311765 4.552941 ## 3: 4.329268 4.219512 4.500000 ## 4: 4.234177 4.335443 4.474684 ## 5: 4.284091 4.318182 4.420455 ## --- ## 132: 3.835526 3.401316 3.539474 ## 133: 3.433962 3.047170 2.801887 ## 134: 3.500000 2.890625 2.500000 ## 135: 3.442308 3.009615 2.586538 ## 136: 2.902174 2.836957 2.706522 ## sd_review_overall sd_review_aroma sd_review_appearance sd_review_palate ## 1: 0.3337716 0.3337716 0.3861642 0.3997258 ## 2: 0.3289275 0.3289275 0.3845766 0.4293993 ## 3: 0.3865103 0.3865103 0.3641730 0.4749840 ## 4: 0.3698733 0.3698733 0.3741267 0.4060561 ## 5: 0.3768892 0.3768892 0.3796836 0.4586495 ## --- ## 132: 1.2438621 1.2438621 0.7136206 1.1431528 ## 133: 0.8995241 0.8995241 0.6866707 0.7355280 ## 134: 1.2427207 1.2427207 0.4918694 0.8005479 ## 135: 0.9108033 0.9108033 0.5994593 0.7637009 ## 136: 0.9829379 0.9829379 0.6843587 0.7883377 ## sd_review_taste mean_review_overall mean_review_aroma ## 1: 0.3826844 0.002077562 0.002077562 ## 2: 0.3620669 -0.001792407 -0.001792407 ## 3: 0.4873397 -0.114417945 -0.114417945 ## 4: 0.3660140 -0.121673270 -0.121673270 ## 5: 0.4026537 -0.146845883 -0.146845883 ## --- ## 132: 1.1277209 -1.392659280 -1.392659280 ## 133: 0.8165336 -1.722103173 -1.722103173 ## 134: 0.9418581 -1.902527701 -1.902527701 ## 135: 1.0035288 -1.974643085 -1.974643085 ## 136: 1.0086226 -2.149810310 -2.149810310 ## mean_review_appearance mean_review_palate mean_reviw_taste ## 1: -0.02018203 -0.01053621 -0.07177483 ## 2: -0.07095603 -0.13956098 -0.07804418 ## 3: -0.05933479 -0.23181349 -0.13098536 ## 4: -0.15442587 -0.11588264 -0.15630181 ## 5: -0.10451218 -0.13314386 -0.21053081 ## --- ## 132: -0.55307677 -1.05000989 -1.09151167 ## 133: -0.95464082 -1.40415587 -1.82909857 ## 134: -0.88860309 -1.56070068 -2.13098536 ## 135: -0.94629539 -1.44171030 -2.04444690 ## 136: -1.48642917 -1.61436916 -1.92446362 ## sd_review_overall sd_review_aroma sd_review_appearance sd_review_palate ## 1: -0.12136909 -0.12136909 -0.01935577 -0.04762635 ## 2: -0.12621327 -0.12621327 -0.02094339 -0.01795284 ## 3: -0.06863039 -0.06863039 -0.04134702 0.02763182 ## 4: -0.08526747 -0.08526747 -0.03139329 -0.04129607 ## 5: -0.07825155 -0.07825155 -0.02583641 0.01129741 ## --- ## 132: 0.78872139 0.78872139 0.30810063 0.69580063 ## 133: 0.44438341 0.44438341 0.28115074 0.28817587 ## 134: 0.78758001 0.78758001 0.08634938 0.35319581 ## 135: 0.45566254 0.45566254 0.19393929 0.31634876 ## 136: 0.52779720 0.52779720 0.27883874 0.34098558 ## sd_review_taste ## 1: -0.03307969 ## 2: -0.05369722 ## 3: 0.07157561 ## 4: -0.04975012 ## 5: -0.01311039 ## --- ## 132: 0.71195677 ## 133: 0.40076950 ## 134: 0.52609404 ## 135: 0.58776473 ## 136: 0.59285853 Of course if I were building a real recommendation machine I could start talking about what factors are more important for what users and what factors are more predictive than others, but this seems like an OK enough solution to at least have completed my a priori goal.\nBased on this solution, it looks like I will have to find myself a bottle of Pliny The Elder and the Hill Farmstead Brewery Galaxy Imperial Single Hop IPA American Double / Imperial IPA and do some of my own empirical work to see if this was a good idea.\n Beer sans Booze The last beer that I think I wanted to recommend would be one that tastes great, but does not have a lot of alcohol in it. The reason this question kind of interests me is because if we are really going to talk about how tasty a beer is, it would be nice to be able to factor out of the equation how drunk we are actually getting from it.\nI can see first of all IF this relationship exists if we look at the mean overall rating of a beer as a function of its ABV content.\nbeer.complete[, beer_name_unique := paste(brewery_name,beer_name, beer_style) ] # ABVs and Mean Scores abv.vs.mean \u0026lt;- beer.complete[, .(Abv = mean(beer_abv), MeanOverall = mean(review_overall)), by = beer_name_unique] ggplot(abv.vs.mean[Abv \u0026lt; 20], aes(x = Abv, y = MeanOverall)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;, color = \u0026quot;blue\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, formula = y ~ poly(x,2), color = \u0026quot;orange\u0026quot;) + labs(title = \u0026quot;Rating as Function of ABV (Beers with than 20% ABV)\u0026quot;, x = \u0026quot;ABV Content\u0026quot;, y = \u0026quot;Mean Overall Rating\u0026quot;) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; Surprisingly, when I ran some quick and dirty regression models (that yes, I know violate tons of assumptions) I saw that only a very small amount of variance was being explained by its ABV. Note that although the models were significant, the R squared values hovered around 3-5%!\n# \u0026quot;The More Booze The Better\u0026quot; Model abv.linear \u0026lt;- lm(MeanOverall ~ Abv, data = abv.vs.mean[Abv \u0026lt; 20]) # The \u0026quot;Diminishing Returns Model \u0026quot; abv.poly \u0026lt;- lm(MeanOverall ~ poly(Abv,2), data = abv.vs.mean[Abv \u0026lt; 20]) # The \u0026quot;Dissapointing Amount of Variance Explained Summaries\u0026quot; summary(abv.linear) ## ## Call: ## lm(formula = MeanOverall ~ Abv, data = abv.vs.mean[Abv \u0026lt; 20]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.06793 -0.27278 0.09684 0.38850 1.72081 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 3.276138 0.008995 364.20 \u0026lt;2e-16 *** ## Abv 0.060975 0.001369 44.55 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.6013 on 48822 degrees of freedom ## Multiple R-squared: 0.03906, Adjusted R-squared: 0.03904 ## F-statistic: 1984 on 1 and 48822 DF, p-value: \u0026lt; 2.2e-16 summary(abv.poly) ## ## Call: ## lm(formula = MeanOverall ~ poly(Abv, 2), data = abv.vs.mean[Abv \u0026lt; ## 20]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.88738 -0.28351 0.09695 0.37907 2.17585 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 3.658087 0.002706 1351.73 \u0026lt;2e-16 *** ## poly(Abv, 2)1 26.785156 0.597970 44.79 \u0026lt;2e-16 *** ## poly(Abv, 2)2 -13.922969 0.597970 -23.28 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.598 on 48821 degrees of freedom ## Multiple R-squared: 0.04961, Adjusted R-squared: 0.04957 ## F-statistic: 1274 on 2 and 48821 DF, p-value: \u0026lt; 2.2e-16 This actually surprised me and might be worth looking into at a deeper level another time, but for now I want to keep going on and find a beer knowing that how much booze is in it doesn’t really affect how good it is.\nSo let’s take one final dive into the dataset, grab only our quality reviews then plot a subset of our data so I can see beers that have a very high overall rating with a very small amount of booze in them.\n# Quality Assurance Step reliable.and.abv.beers \u0026lt;- reliable.beers.list[beer.complete, on = \u0026quot;beer_name_unique\u0026quot;, nomatch=0] ## Get mean ratings and keep ABV (which won\u0026#39;t change if I average it) dd.beers \u0026lt;- reliable.and.abv.beers[, .(mean_overall = mean(review_overall), abv = mean(beer_abv)), by = \u0026quot;beer_name_unique\u0026quot;] # Only Beers that Fit Our Criterion dd.beers.2 \u0026lt;- dd.beers[mean_overall \u0026gt; 4.6 \u0026amp; abv \u0026lt; 10] # Plot It! ggplot(dd.beers.2, aes(x = abv, y = mean_overall, label = beer_name_unique, color = beer_name_unique)) + geom_point() + geom_text(aes(label=beer_name_unique),hjust=-.01, vjust=0) + labs(title = \u0026quot;High Quality Beers with Low ABV\u0026quot;, x = \u0026quot;ABV\u0026quot;, y = \u0026quot;Mean Overall Rating\u0026quot;) + theme(legend.position = \u0026quot;none\u0026quot;) + xlim(0, 20) + scale_y_continuous(breaks = c(seq(4.6,5,.1)), limits = c(4.6,4.85)) These are all OK choices (most of the beers are still above 5% ABV), but we do have one beer clocking in at 2.0% ABV giving us our final beer recommendation – the Southampton Publick House Southampton Berliner Weisse Berliner Weissbier!\n Summary After all of this, I know have three beers to check out. Pliny The Elder is our winner for the top rated beer among our Super Users of the site, the Hill Farmstead Brewery Galaxy Imperial Single Hop IPA American Double / Imperial IPA is a beer to maybe follow up on from our first choice, and then lastly we have the the Southampton Publick House Southampton Berliner Weisse Berliner Weissbier which supposedly tastes great, despite its lack of alcohol content.\n ","date":1515974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515974400,"objectID":"c3b66a9dcb0b347c2037ab5135d5bf88","permalink":"/post/hire-me-as-a-data-scientist-part-ii/","publishdate":"2018-01-15T00:00:00Z","relpermalink":"/post/hire-me-as-a-data-scientist-part-ii/","section":"post","summary":"Continuing on from my earlier post, I’m now looking to tackle the question:\n If you had to pick 3 beers to recommend using only this data, which would you pick?","tags":["R","Data Science","Beer"],"title":"Hire Me (as a Data Scientist!), Part II","type":"post"},{"authors":null,"categories":["R","Data Science","Beer"],"content":" Background I read Medium blog posts on “How to Become a Data Scientist” more often than I care to admit. Much of this comes from a fear that after doing all this work on the PhD and then hitting the Music Theory job market, I won’t fit the mold of the kind of theorist most schools want to hire. Not coming from one of five schools that seem to have a monopoly on the tenure track jobs can be a bit discouraging, but I also won’t deny that having a non-academic job with a regular 9-5 schedule and a decent salary is pretty tempting after spending the vast majority of my twenties in school. And even if I don’t go on over to industry after the PhD, I’ll probably always be looking for a bit more work in summer.\nOn top of all of that, I believe that skills that are acquired in a PhD (especially if you do computational musicology and music cognition!) are very transferable to most jobs, and it’s just a matter of being a bit more pro-active in promoting myself that might help me one day land a stable, non-academic job.\nThat said, one tweet I saw last week by Jesse Meagan linked to this really interesting Linked-In post by Tanya Cashorali that purported to have a one size fits all data-science interview process which has candidates take home a big dataset with a bunch of beer reviews and answer four very broad questions. Considering myself an aficionado of How-To-Become-a-Data Scientist articles, this of course caught my eye.\nAfter reading the article, I figured why not give it a go? It’s the start of the semester, I’m basically ABD, need more of a portfolio beyond my github, and I have nothing to do with my Saturday morning. Why not see what I can produce in 4 or 5 hours? At the very least I’ll hopefully just have something to point to if a future employer wants to see how I think through data-science problems.\nAnd if anyone is reading this that does have comments on my code or thought process… please let me know what you think on Twitter! I’d love some feedback!\n Exploring the Dataset The first thing I did was to grab this dataset which you can get here and then I set up my R script with a few of my favorite packages (again, big love to Ben at GormAnalysis for helping me learn data.table).\n#=====================================================================================# # Beer Script #=====================================================================================# # Library library(ggplot2) library(data.table) library(stringr) #=====================================================================================# beer \u0026lt;- fread(\u0026quot;data/beer_reviews.csv\u0026quot;) #=====================================================================================# The dataset has about 1.5 million observations across 14 different observations, so don’t try to open it in LibreOffice. The reviews come from a variety of different users that have rated the beers based on five different attributes (Appearance, Palate, Aroma, Taste, Overall) and then each beer has a few other variables listed such as its ABV, the brewery it comes from, the beer’s name (duh), and what kind of beer it is.\nnames(beer) ## [1] \u0026quot;brewery_id\u0026quot; \u0026quot;brewery_name\u0026quot; \u0026quot;review_time\u0026quot; ## [4] \u0026quot;review_overall\u0026quot; \u0026quot;review_aroma\u0026quot; \u0026quot;review_appearance\u0026quot; ## [7] \u0026quot;review_profilename\u0026quot; \u0026quot;beer_style\u0026quot; \u0026quot;review_palate\u0026quot; ## [10] \u0026quot;review_taste\u0026quot; \u0026quot;beer_name\u0026quot; \u0026quot;beer_abv\u0026quot; ## [13] \u0026quot;beer_beerid\u0026quot; beer[, .(Number = unique(beer$brewery_name))] ## Number ## 1: Vecchio Birraio ## 2: Caldera Brewing Company ## 3: Amstel Brouwerij B. V. ## 4: Broad Ripple Brew Pub ## 5: Moon River Brewing Company ## --- ## 5739: Gattopardo Cervejaria ## 5740: Brauerei Lasser GmbH ## 5741: Wissey Valley Brewery ## 5742: Outback Brewery Pty Ltd ## 5743: Georg Meinel Bierbrauerei KG beer[, .(Number = unique(beer$review_profilename))] ## Number ## 1: stcules ## 2: johnmichaelsen ## 3: oline73 ## 4: Reidrover ## 5: alpinebryant ## --- ## 33384: jennaizzel ## 33385: mine2design ## 33386: hogshead ## 33387: NyackNicky ## 33388: joeebbs beer[, .(Number = unique(beer$beer_name))] ## Number ## 1: Sausa Weizen ## 2: Red Moon ## 3: Black Horse Black Beer ## 4: Sausa Pils ## 5: Cauldron DIPA ## --- ## 56853: Bear Mountain Ale ## 56854: Highland Porter ## 56855: Baron Von Weizen ## 56856: Resolution #2 ## 56857: The Horseman\u0026#39;s Ale beer[, .(Number = unique(beer$beer_style))] ## Number ## 1: Hefeweizen ## 2: English Strong Ale ## 3: Foreign / Export Stout ## 4: German Pilsener ## 5: American Double / Imperial IPA ## --- ## 100: Gueuze ## 101: Gose ## 102: Happoshu ## 103: Sahti ## 104: Bière de Champagne / Bière Brut From a bird’s eye view we have 56,857 unique beers in 104 different categories from 5,743 different breweries and 33,388 unique beer aficionados who have gone out of their way to tell this website what they think about the beers they drink.\nBefore diving in further, it’s worth doing a preliminary check of the quality of the data (aka we should know if this is BAD (Best Available Data) or has undergone a fair deal of cleaning). As someone who comes from more of a psychology background, I’ve noticed what certain people consider “clean” when it comes to data varies a lot.\nThe first thing I check for is if there is any kind of data missing and if there is, is it due to chance? Or is it due to some sort of systematic variation?\ntable(complete.cases(beer)) ## ## FALSE TRUE ## 67785 1518829 67785/1518829 ## [1] 0.04462978 So about 4% of our rows don’t have every entry, so probably not too much cause for concern unless we start getting into specific questions about specific beers. Looking into this a bit further it seems like it’s just beers missing the ABV of the beer. Anyone who has made some beer ratings has made ratings on all five variables. And although it’s only 4% of our entire ratings that don’t have their ABV, comparing that to every beer we have, we see we are actually missing ~25% of the ABV ratings of all of our beers. That could be a problem later, but it’s good to know about it sooner rather than later.\nbeer.complete \u0026lt;- beer[complete.cases(beer)] beer[!complete.cases(beer)][, .(.N = unique(beer_name))] ## .N ## 1: Cauldron Espresso Stout ## 2: The Highland Stagger ## 3: Alpha Beta ## 4: Imperial Stout ## 5: Megalodon ## --- ## 14106: English Nut Brown ## 14107: Hop Common ## 14108: Very Hoppy Pale Ale ## 14109: Prohibition Lager ## 14110: Resolution #2 14110/56857 ## [1] 0.2481665 beer[969] ## brewery_id brewery_name review_time review_overall review_aroma ## 1: 12770 City Grille and Brewhaus 1145738954 4 3 ## review_appearance review_profilename beer_style review_palate ## 1: 4 UncleJimbo American Pale Ale (APA) 3.5 ## review_taste beer_name beer_abv beer_beerid ## 1: 4 City Pale Ale NA 30088 More problems might come up here or there, but let’s move on the first question.\n Which brewery produces the strongest beers by ABV%? Answering the first question on the list is pretty straight forward. Essentially all you need to do is grab all of the observations that have an ABV associated with their rating, and then get the average ABV of all the beers that that brewery produces.\n# Use object before that has only ratings with ABVs beer.complete[, .(AvgABV = mean(beer_abv)), by = brewery_name] ## brewery_name AvgABV ## 1: Vecchio Birraio 5.675000 ## 2: Caldera Brewing Company 6.168849 ## 3: Amstel Brouwerij B. V. 3.816373 ## 4: Broad Ripple Brew Pub 6.006202 ## 5: Moon River Brewing Company 5.724103 ## --- ## 5152: Gattopardo Cervejaria 6.033333 ## 5153: Brauerei Lasser GmbH 5.200000 ## 5154: Wissey Valley Brewery 5.133333 ## 5155: Outback Brewery Pty Ltd 4.787879 ## 5156: Georg Meinel Bierbrauerei KG 5.850000 # Create table that has means and standard deviations of beers by brewery # Order them from most to least abv.counter \u0026lt;- beer.complete[, .(AvgABV = mean(beer_abv), SdABV = sd(beer_abv)) , by = brewery_name][order(-AvgABV)] abv.counter ## brewery_name AvgABV SdABV ## 1: Schorschbräu 19.2288235 12.3273042 ## 2: Shoes Brewery 15.2000000 0.0000000 ## 3: Rome Brewing Company 13.8400000 1.9718012 ## 4: Hurlimann Brewery 13.7500000 0.5752237 ## 5: Alt-Oberurseler Brauhaus 13.2000000 NA ## --- ## 5152: Cerveceria Vegana, S.A. 2.2608696 2.2455490 ## 5153: Moskovskaya Pivovarennaya Kompaniya 2.1500000 1.6881943 ## 5154: Fentimans Ltd. 1.3750000 1.6201852 ## 5155: Borodino ZAO 0.9666667 0.4041452 ## 5156: All Stars Bakery 0.5000000 0.0000000 Having this table could be good enough for government work, but looking at the output there are clearly problems, and one thing to consider in this table (and pretty much this whole dataset) is “Is this data point a good representation of what I am trying to measure?”. Note for example the huge variability as measured by the standard deviation in our top answer as well as the fact that some of the SDs have NAs and there is a value of 0. Given that, I think it’d be good to put on some sort of threshold that would up the quality of our answers. One way to do this would be to see exactly how many beers each brewery makes and use that as a proxy for how big the brewery is.\nThe code below does just that and reveals the variability in terms of size of breweries within this dataset.\n# Create table that counts number of beers NoOfBeers \u0026lt;- beer.complete[, .(NameOfBeer = unique(beer_name)), by = brewery_name][, .(.N), by = brewery_name] NoOfBeers ## brewery_name N ## 1: Vecchio Birraio 4 ## 2: Caldera Brewing Company 25 ## 3: Amstel Brouwerij B. V. 9 ## 4: Broad Ripple Brew Pub 40 ## 5: Moon River Brewing Company 34 ## --- ## 5152: Gattopardo Cervejaria 3 ## 5153: Brauerei Lasser GmbH 1 ## 5154: Wissey Valley Brewery 3 ## 5155: Outback Brewery Pty Ltd 6 ## 5156: Georg Meinel Bierbrauerei KG 2 # Make table that lists each beer with it\u0026#39;s ABV and the name of the brewery abv.table \u0026lt;- NoOfBeers[abv.counter, on = \u0026quot;brewery_name\u0026quot;] abv.table ## brewery_name N AvgABV SdABV ## 1: Schorschbräu 10 19.2288235 12.3273042 ## 2: Shoes Brewery 1 15.2000000 0.0000000 ## 3: Rome Brewing Company 2 13.8400000 1.9718012 ## 4: Hurlimann Brewery 3 13.7500000 0.5752237 ## 5: Alt-Oberurseler Brauhaus 1 13.2000000 NA ## --- ## 5152: Cerveceria Vegana, S.A. 2 2.2608696 2.2455490 ## 5153: Moskovskaya Pivovarennaya Kompaniya 2 2.1500000 1.6881943 ## 5154: Fentimans Ltd. 3 1.3750000 1.6201852 ## 5155: Borodino ZAO 2 0.9666667 0.4041452 ## 5156: All Stars Bakery 1 0.5000000 0.0000000 # Create z scores abv.table[, zAvgABV := scale(AvgABV)] After visually inspecting the graph on the size of breweries (below), I figured I could just look at breweries that make over five beers (which hopefully wipes out your hipster friend’s “micro brewery” in his basement where he is just trying to make the most potent IPA ever) and then only look at beers that score 4 standard deviations above the mean of all beers in terms of ABV content to narrow down possible candidates.\n# How many beers to count for a big brewery? hist(NoOfBeers$N, breaks= 200, main = \u0026quot;Distribution of Size of Breweries\u0026quot;, xlab = \u0026quot;Number of Beers Produced by a Brewery\u0026quot;) NoOfBeers[N \u0026gt; 200] # Clearly some big breweries here!  ## brewery_name N ## 1: Minneapolis Town Hall Brewery 243 ## 2: Goose Island Beer Co. 304 ## 3: Iron Hill Brewery \u0026amp; Restaurant 269 ## 4: Rock Bottom Restaurant \u0026amp; Brewery 522 abv.table[N \u0026gt;= 5, ][order(-AvgABV)] ## brewery_name N AvgABV ## 1: Schorschbräu 10 19.228824 ## 2: Brasserie Grain d\u0026#39; Orge (Brasserie Jeanne d\u0026#39;Arc SA) 10 12.445860 ## 3: Brauerei Schloss Eggenberg 14 11.779681 ## 4: Brasserie Dubuisson Frères sprl 14 11.432746 ## 5: Kuhnhenn Brewing Company 142 11.345839 ## --- ## 2539: Berliner Kindl Brauerei 12 3.532627 ## 2540: Yanjing Pijiu (Guilin Liquan) Gufen Youxian Gongsi 5 3.440000 ## 2541: Ochakovo 16 3.203150 ## 2542: Grogg\u0026#39;s Pinnacle Brewing Co. 6 3.200000 ## 2543: Deka Brewery 7 2.620000 ## SdABV zAvgABV ## 1: 12.3273042 10.235332 ## 2: 1.7054879 5.048898 ## 3: 3.0759353 4.539520 ## 4: 1.6583471 4.274244 ## 5: 3.5788003 4.207793 ## --- ## 2539: 1.0372607 -1.766396 ## 2540: 0.6426508 -1.837221 ## 2541: 1.2735986 -2.018323 ## 2542: 0.0000000 -2.020731 ## 2543: 1.6356553 -2.464215 hist(abv.table[N \u0026gt;= 5, ][order(-AvgABV)]$zAvgABV, xlab = \u0026quot;z Score of ABV\u0026quot;, main = \u0026quot;Distribution of ABV in Breweries that make more than 5 Beers\u0026quot;) abv.table[N \u0026gt;= 5 \u0026amp; zAvgABV \u0026gt; 4, ][order(-AvgABV)] ## brewery_name N AvgABV SdABV ## 1: Schorschbräu 10 19.22882 12.327304 ## 2: Brasserie Grain d\u0026#39; Orge (Brasserie Jeanne d\u0026#39;Arc SA) 10 12.44586 1.705488 ## 3: Brauerei Schloss Eggenberg 14 11.77968 3.075935 ## 4: Brasserie Dubuisson Frères sprl 14 11.43275 1.658347 ## 5: Kuhnhenn Brewing Company 142 11.34584 3.578800 ## zAvgABV ## 1: 10.235332 ## 2: 5.048898 ## 3: 4.539520 ## 4: 4.274244 ## 5: 4.207793 Doing it this way puts Schorschbräu as the highest ABV brewery, which makes sense because they claim to make the world’s strongest beer. Making a quick plot of the data for our winner and the second place finisher, we see how strong Schorschbräu really is.\nschor.abv \u0026lt;- beer.complete[brewery_name == \u0026quot;Schorschbräu\u0026quot;, .(beer_name = unique(beer_name)), by = beer_abv] ggplot(schor.abv, aes(x = beer_name, y = beer_abv)) + geom_bar(stat = \u0026quot;identity\u0026quot;) + labs( title = \u0026quot;Schorschbräu Beer ABV\u0026quot;, x = \u0026quot;Beer Name\u0026quot;, y = \u0026quot;ABV\u0026quot;) + theme(axis.text.x=element_text(angle = -90, hjust = 0))  brassOrg.abv \u0026lt;- beer.complete[brewery_name == \u0026quot;Brasserie Grain d\u0026#39; Orge (Brasserie Jeanne d\u0026#39;Arc SA)\u0026quot;, .(beer_name = unique(beer_name)), by = beer_abv] ggplot(brassOrg.abv, aes(x = beer_name, y = beer_abv)) + geom_bar(stat = \u0026quot;identity\u0026quot;) + labs( title = \u0026quot;Brasserie Grain d\u0026#39; Orge Beer ABV\u0026quot;, x = \u0026quot;Beer Name\u0026quot;, y = \u0026quot;ABV\u0026quot;) + theme(axis.text.x=element_text(angle = -90, hjust = 0)) + ylim(0, 60)  I think that saying Schorschbräu is technically correct here, but after sharing my findings with my one beer drinking friend he pointed out that one thing that this analysis did not take into account was that beers that are traditionally brewed to have a higher ABV (like IPAs and Belgiums) might skew my results. So if you are a big IPA brewery, you are going to have higher average ABV because of the beers you decide to brew!\nSo in the true spirit of that data science Venn diagram noting that data scientists need to be flexible in incorporating others’ domain knowledge, I did another analysis to just show how much an answer can change depending on how you change your operationalization of the question!\nLet’s do another one!\nFirst up for this one is making a plot of the data to see how much beers actually vary from type to type.\n# Get mean and SD of each beer type abv.by.type \u0026lt;- beer.complete[ , .(MeanAbvType = mean(beer_abv), SdAbvType = sd(beer_abv)), by = beer_style] # For Graphing, order, set style as factor ordered.abv.by.type \u0026lt;- abv.by.type[order(-MeanAbvType)] ordered.abv.by.type$beer_style \u0026lt;- factor(ordered.abv.by.type$beer_style, levels = ordered.abv.by.type$beer_style) # Code for plot, blogdown crunches the images # Average ABV by beer type # ggplot(ordered.abv.by.type, aes(x = beer_style, y = MeanAbvType)) + # geom_bar(stat=\u0026quot;identity\u0026quot;) + coord_flip() + # labs(title = \u0026quot;Average ABV by Type of Beer\u0026quot;, # x = \u0026quot;Beer Style\u0026quot;, # y = \u0026quot;Mean ABV, bars represent SD\u0026quot;) + # geom_errorbar(aes(ymin=MeanAbvType-SdAbvType, ymax=MeanAbvType+SdAbvType)) + # theme_bw() So you can see here that if you wanted to have a higher ABV on average for your brewery, you’d benefit from having more IPAs, Barley Wines, and Belgian Stouts.\nNow with average ABV for each beer, let’s then match that to our big list, find how each beer fairs against its own category, sort them, and then combine them with our information from before on how big the brewery is. For the purposes of this example, let’s only look at breweries that have over 100 beers in the database and look the top 20.\n# Combine ABV per type data with complete data beer.complete.avg.abv \u0026lt;- abv.by.type[beer.complete, on = \u0026quot;beer_style\u0026quot;] # Make new z score variable based on other beers in group beer.complete.avg.abv[, zABV := (beer_abv-MeanAbvType)/SdAbvType] # Get averages per brewery on z variable zAvgBeers \u0026lt;- beer.complete.avg.abv[, .(AvgAbvZ = mean(zABV)), by = brewery_name][order(-AvgAbvZ)] # Combine back with our data on proxy of size of brewery BreweryAndAvgAbv \u0026lt;- zAvgBeers[NoOfBeers, on = \u0026quot;brewery_name\u0026quot;] # And the winner is... BreweryAndAvgAbv[N \u0026gt; 100][order(-AvgAbvZ)][1:25] ## brewery_name AvgAbvZ N ## 1: Kuhnhenn Brewing Company 1.26003627 142 ## 2: Cigar City Brewing 0.87721989 171 ## 3: The Bruery 0.74965237 143 ## 4: Three Floyds Brewing Co. \u0026amp; Brewpub 0.71875251 128 ## 5: Flossmoor Station Restaurant \u0026amp; Brewery 0.53182569 114 ## 6: Brouwerij De Molen 0.51695738 119 ## 7: Jackie O\u0026#39;s Pub \u0026amp; Brewery 0.40179915 105 ## 8: Mikkeller ApS 0.37012567 184 ## 9: Founders Brewing Company 0.24770929 130 ## 10: Deschutes Brewery 0.19390121 127 ## 11: Port Brewing Company / Pizza Port 0.19357914 194 ## 12: Fitger\u0026#39;s Brewhouse 0.18931699 111 ## 13: Bullfrog Brewery 0.10590796 121 ## 14: Victory Brewing Company 0.09970522 107 ## 15: Iron Hill Brewery \u0026amp; Restaurant 0.03563186 269 ## 16: Goose Island Beer Co. 0.01475446 304 ## 17: Sly Fox Brewing Company -0.01733499 140 ## 18: Sierra Nevada Brewing Co. -0.07417651 121 ## 19: Stone Brewing Co. -0.08646265 119 ## 20: Cambridge Brewing Company -0.12326285 124 ## 21: Rock Bottom Restaurant \u0026amp; Brewery -0.26029542 522 ## 22: Willimantic Brewing Co. -0.28003004 134 ## 23: John Harvard\u0026#39;s Brewery \u0026amp; Ale House -0.29598861 151 ## 24: Minneapolis Town Hall Brewery -0.41963873 243 ## 25: \u0026lt;NA\u0026gt; NA NA ## brewery_name AvgAbvZ N Looking at this list, we get a totally different answer. It appears that on average Kuhnhenn Brewing Company brews their beers 1.26 standard deviations above the mean of all other beers in that category!\nBoth answers could be technically correct, but more importantly demonstrate how important it is to come up with how you frame your question first, and then try to answer it so you don’t end up going on a fishing expedition!\nMoving on to question #2!\n ","date":1515801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515801600,"objectID":"85c1bebc3be6e6a16bbb0ad5425cd057","permalink":"/post/hire-me-as-a-data-scientist-part-i/","publishdate":"2018-01-13T00:00:00Z","relpermalink":"/post/hire-me-as-a-data-scientist-part-i/","section":"post","summary":"Background I read Medium blog posts on “How to Become a Data Scientist” more often than I care to admit. Much of this comes from a fear that after doing all this work on the PhD and then hitting the Music Theory job market, I won’t fit the mold of the kind of theorist most schools want to hire.","tags":["R","5 Minute Reads"],"title":"Hire Me (as a Data Scientist!), Part I","type":"post"},{"authors":null,"categories":["R"],"content":" For the most part, Twitter is full of garbage. But I’m an optimist and a firm believer in Sturgeon’s Law so by that logic there must be some good on it. That good is academic twitter.\nWhile this isn’t a post advocating for academic Twitter, I did want to\n1. see if I could figure out how to write a post with some R code in it and 2. share how I scraped Twitter to find active users in the Musicology and Music Theory community\nSo here it goes…\nThe first thing that you have to do is get some tweets. Luckily some packages exist in the #rstats world that can help with this. For this project I used the twitteR package which lets you log into Twitter’s API via R and and search it. There are already some instructions on how to get started with it that you can find here, so I won’t go into tons of detail about setting it up. (Also please note you can’t just copy and paste my code verbatim since it requires credentials from your own Twitter account)\nLet’s first load the two packages we’ll need.\nlibrary(data.table) library(twitteR) Next up, we need to access Twitter’s API by entering in the details from the link above. I find it’s easiest to copy and paste each of my keys and tokens into a nice little character string, assign those to an object, then call those objects in the last command in this block.\nconsumer_key \u0026lt;- \u0026#39;YOUR CONSUMER KEY HERE\u0026#39; consumer_secret \u0026lt;- \u0026#39;COPY AND PASTE YOUR CONSUMER SECRET HERE\u0026#39; access_token \u0026lt;- \u0026#39;THEN PUT YOUR ACCESS TOKEN HERE\u0026#39; access_secret \u0026lt;- \u0026#39;AND YOUR ACCESS SECRET HERE\u0026#39; setup_twitter_oauth(consumer_key, consumer_secret, access_token=NULL, access_secret=NULL) Running that last line in the chunk should then direct you to your default browser. This will log you into your Twitter account and R will ask for Twitter’s permission to enter through the back door.\nThe next bit of code won’t run the way I have it set up because Twitter doesn’t let you download tweets older than a week old. So if you want to play with tweets from a conference’s hashtag or some event, make sure to think ahead to download them!!\namsTwitter \u0026lt;- searchTwitter(\u0026quot;#smt2017\u0026quot;, n = 700) amsTwitter \u0026lt;- searchTwitter(\u0026quot;#amsroc17\u0026quot;, n = 1600) This line above searches Twitter for anything matching the conference hashtags and saves the output of it in a list. You can also include an argument asking for a certain number of tweets, which I’ve also done. Luckily the twitteR package has a function that will take this list and convert it to a data frame.\namsTwitter.df \u0026lt;- twListToDF(amsTwitter) smtTwitter.df \u0026lt;- twListToDF(smtTwitter) With these nice data frames, we’ll soon be able to join them together and count up some tweets! In order to do this we can take advantage of the data.table package to join our two tables together. Of course there are other ways, but Ben over at Gorm Analytics sold me on data.table this past summer and since then I have really been loving its easy syntax.\namsTwitter.dt \u0026lt;- data.table(amsTwitter.df) smtTwitter.dt \u0026lt;- data.table(smtTwitter.df) amstweets \u0026lt;- amsTwitter.dt[, .(amsTweets = .N), by=screenName][order(-amsTweets)] smttweets \u0026lt;- smtTwitter.dt[, .(smtTweets = .N), by=screenName][order(-smtTweets)] totalTweets \u0026lt;- merge(smttweets,amstweets, on =\u0026quot;screenName\u0026quot;, all = TRUE) The first thing the above code does is swap our data frames over to data.tables. Once they are in the data.table format, we can count up the tweets by screen name, then list them from biggest to smallest all in the same line. From there we merge the two together via the shared column, making sure to grab every instance in each table since not every Tweeter tweeted with both hashtags.\nWe then need to clean up some of the NAs (which as a data.table are characters!) in our bigger dataset with R’s ifelse() function that basically works exactly like an ifelse statement would in Microsoft Excel. It looks over a column in your dataset, checks if a value is an NA, if it is then it gives it a 0, if not, it puts in the value that was there in the first place. After replacing NAs, I then make a new variable that adds together both columns then run our final line that prints out our final dataset from top to bottom.\ntotalTweets$smtTweets \u0026lt;- ifelse(test = is.na(totalTweets$smtTweets), yes = 0, no = totalTweets$smtTweets) totalTweets$amsTweets \u0026lt;- ifelse(test = is.na(totalTweets$amsTweets), yes = 0, no = totalTweets$amsTweets) totalTweets[, TotalTweets := smtTweets + amsTweets] totalTweets[order(-TotalTweets)] From here it was simply a matter of using an online converter to turn it our final table an html file and then ssh it up to our Music Cognition at LSU server! Since then I’ve also added both the 2017 AMS and SMT datasets that I used to generate the counts in case you want to try this for yourself.\nIf anyone has any questions on this, please tweet me!\n","date":1511136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511136000,"objectID":"1b9fdfd2a72ec41fce2525e0c9d163b0","permalink":"/post/looking-for-musicologists-on-twitter/","publishdate":"2017-11-20T00:00:00Z","relpermalink":"/post/looking-for-musicologists-on-twitter/","section":"post","summary":"For the most part, Twitter is full of garbage. But I’m an optimist and a firm believer in Sturgeon’s Law so by that logic there must be some good on it.","tags":["Twitter","R","Data Mining"],"title":"Looking For Musicologists on Twitter","type":"post"},{"authors":null,"categories":["R","Music Theory"],"content":" November is pretty much the worst month for people in higher education. There are too many deadlines and if you’re still in coursework (like myself) you have essays to write, presentations to make, and a backlog of homework assignments to grade. So if you can save time here or there, it’s usually a good choice.\nThis weekend I was working a homework assignment for my Transformational Theory seminar where we were given a number of pairs of pitch class sets and had to calculate the imultiset for each following Joseph Straus’ 2014 article on Total Voice Leading.\nAs I looked at the top of the assignment (pictured below) and started to crank out the first one by hand, I realized that the next 30 minutes of my life were going to be doing the same thing over and over again.\nUsually if I get that feeling my next thought is “Can I make a computer do this?” and after thinking about it for two minutes I realized the answer was yes.\nSo instead of doing all of these by hand, I wrote an R script and with the time saved figured I’d write a quick post about it.\nThe Problem In order to calculate the imultiset you need two pitch class sets, in this case X and Y. Each set can have any number of pitch classes in them and what you need to do is calculate the distance in Modulo 12 space between every possible combination of pitch classes from one set to the other. So for example, you could move from 4 (E in Mod 12 for you non-music theory readers) to 7, 11, 2, or 5 (G, B, D, or F) resulting in four intervals: {3,7,10,1}. These four numbers in the {curly braces} are what you get when you subtract each number in the second set from the note E in Mod 12 space. This action then needs to be completed for every pair.\nWhen you need to account for every pairing you need to do a cross join. A cross join connects each member of one set to each member of another set. This creates the sets of pairs seen below.\nThen all you need to do is subtract one from the other to get the distance. The only problem is that these subtractions need to happen in Mod 12 space so in any case where you are subtracting a bigger number from a smaller number you will get a negative result! This is easily fixed by just adding 12 to that number in order to get what we should have been our answer if we were doing Mod 12 arithmetic.\nAfter fixing the Mod 12 problem, you’ll have a nice list of intervals that just have to be sorted from top to bottom to have your imultiset. So let’s see how you would do this line by line.\n R Code First, let’s get two pitch class sets. In this case we have a C major triad and a G7 chord.\nX = {0,4,7} and Y = {7,11,2,5}\nLet’s first assign each chord to an object.\nX \u0026lt;- c(0,4,7) Y \u0026lt;- c(7,11,2,5) Next we need to do that cross join, which we can accomplish with R’s merge() function. This makes us a data frame with every combination from set X and set Y. Below we see the function’s output.\nExample \u0026lt;- merge(X,Y,all=TRUE) Example ## x y ## 1 0 7 ## 2 4 7 ## 3 7 7 ## 4 0 11 ## 5 4 11 ## 6 7 11 ## 7 0 2 ## 8 4 2 ## 9 7 2 ## 10 0 5 ## 11 4 5 ## 12 7 5 Once having each combination, we then subtract one set from the other. Since I don’t know how to put R into Music Theory Mode where it only operates in Mod 12, we can fix the problem of the negative numbers by just indexing through our answer with in ifelse() statement to replace any negative values with the answer we actually want by adding 12 to it.\nExample$diff \u0026lt;- Example$y - Example$x Example$diff ## [1] 7 3 0 11 7 4 2 -2 -5 5 1 -2 Example$mod12 \u0026lt;- ifelse(test = Example$diff \u0026lt; 0 , yes = Example$diff + 12, no = Example$diff) Example$mod12 ## [1] 7 3 0 11 7 4 2 10 7 5 1 10 With our numbers then in Mod 12 space, we just sort them and we get our imultiset.\nsort(Example$mod12) ## [1] 0 1 2 3 4 5 7 7 7 10 10 11 Of course you are not going to want to write this out every time you want to calculate an imultiset, so best to just write a function that does what we just did.\ncalculate.multiset \u0026lt;- function(x,y){ array.1 \u0026lt;- x array.2 \u0026lt;- y cross.join \u0026lt;- merge(array.1,array.2, all = TRUE) cross.join$diff \u0026lt;- cross.join$y - cross.join$x cross.join$mod12 \u0026lt;- ifelse(cross.join$diff \u0026lt; 0, cross.join$diff + 12, cross.join$diff) sort(cross.join$mod12) }  ","date":1511049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511049600,"objectID":"5d0f1cf3870d3a74a7f087e8adf6e57e","permalink":"/post/calculating-imultisets-in-r/","publishdate":"2017-11-19T00:00:00Z","relpermalink":"/post/calculating-imultisets-in-r/","section":"post","summary":"November is pretty much the worst month for people in higher education. There are too many deadlines and if you’re still in coursework (like myself) you have essays to write, presentations to make, and a backlog of homework assignments to grade.","tags":["R","Music Theory","Transformational Theory"],"title":"Calculating iMultisets in R","type":"post"},{"authors":null,"categories":null,"content":"","date":1505088000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505088000,"objectID":"025228013307f04f483ee12254d210b7","permalink":"/talk/yboa-aural-2017/","publishdate":"2017-09-11T00:00:00Z","relpermalink":"/talk/yboa-aural-2017/","section":"talk","summary":"","tags":null,"title":"Modeling Performance on Aural Skills Examinations","type":"talk"},{"authors":null,"categories":null,"content":"Poster presented at the Society for Music Perception and Cognition, 2017\n","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"1149462313f7b45af5c68edcb00ebdfb","permalink":"/talk/smpc-similarity-2017/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/talk/smpc-similarity-2017/","section":"talk","summary":"Poster presented at the Society for Music Perception and Cognition, 2017","tags":null,"title":"The Modeling and Perception of Melodic Similarity in Jazz Improvisation","type":"talk"},{"authors":null,"categories":null,"content":"Poster presented at the Society for Music Perception and Cognition, 2017\n","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"8b004c886e3ad0ea77330cf6610459e7","permalink":"/talk/smpc-gold-2017/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/talk/smpc-gold-2017/","section":"talk","summary":"Poster presented at the Society for Music Perception and Cognition, 2017","tags":null,"title":"Validating the Goldsmiths Musical Sophistication Index in a Sample of American Participants","type":"talk"},{"authors":null,"categories":null,"content":"Poster presented at the Society for Music Perception and Cognition, 2017\n","date":1501459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501459200,"objectID":"be8b89b47a12973287948fa9e1cd7fa6","permalink":"/talk/smpc-rock-2017/","publishdate":"2017-07-31T00:00:00Z","relpermalink":"/talk/smpc-rock-2017/","section":"talk","summary":"Poster presented at the Society for Music Perception and Cognition, 2017","tags":null,"title":"A Relational Network Approach to Rock Music Syntax","type":"talk"},{"authors":["D Baker","D Müllensiefen"],"categories":null,"content":"","date":1493856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493856000,"objectID":"940ba36cd38e806cbfd54974fc8e64ef","permalink":"/publication/wagner-frontiers/","publishdate":"2017-05-04T00:00:00Z","relpermalink":"/publication/wagner-frontiers/","section":"publication","summary":"The music of Richard Wagner tends to generate very diverse judgments indicative of the complex relationship between listeners and the sophisticated musical structures in Wagner’s music. This paper presents findings from two listening experiments using the music from Wagner’s Der Ring des Nibelungen that explores musical as well as individual listener parameters to better understand how listeners are able to hear leitmotives, a composition device closely associated with Wagner’s music. Results confirm findings from a previous experiment showing that specific expertise with Wagner’s music can account for a greater portion of the variance in an individual’s ability to recognize and remember musical material compared to measures of generic musical training. Results also explore how acoustical distance of the leitmotives affects memory recognition using a chroma similarity measure. In addition, we show how characteristics of the compositional structure of the leitmotives contributes to their salience and memorability. A final model is then presented that accounts for the aforementioned individual differences factors, as well as parameters of musical surface and structure. Our results suggest that future work in music perception may consider both individual differences variables beyond musical training, as well as symbolic features and audio commonly used in music information retrieval in order to build robust models of musical perception and cognition.","tags":null,"title":"Perception of Leitmotives in Richard Wagner's Der Ring des Nibelungen","type":"publication"},{"authors":["D Baker, T Trahan, D Müllensiefen"],"categories":null,"content":"","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"41850aacde0453bc2ef8ea68e7a458dc","permalink":"/publication/matching-music-to-brand/","publishdate":"2016-08-01T00:00:00Z","relpermalink":"/publication/matching-music-to-brand/","section":"publication","summary":"Advertising and branding practitioners in music industry are often faced with the task of finding and selecting music that matches a given brand profile to enhance the overall brand perception or impact of a commercial. Currently there are only few suggestions in the literature on how to best match music to brands (Brodsky, 2011; MacInnins \u0026 Park, 1991) and these largely lack the ability to quantify the brand-music relationships precisely. In order to create a practical tool for use in industry, a short psychometric tool was constructed to accurately quantify the perceived distance between brands and musical pieces in an emotional space. The tool combines research from music cognition (Asmus, 1985), music and advertising (Müllensiefen et. al 2013), as well as input from industry professionals. The semantic differential tool was created using previous data where 185 participants made 700 ratings of perceived affect for 16 music pieces across the 39 items representing a 3-factor structure suggested by Asmus (1985). On the basis of a factor analysis a set of 15 items was identified that represented the 3 emotional factors adequately. In a second step, new data was collected using these 15 items across 60 pieces of music presented via Soundout's SliceThePie.com interface (N=6005). A confirmatory factor analysis confirmed the 3-factor structure of the tool within acceptable bounds (RMR ","tags":null,"title":"Matching Music to Brand Personality: A Semantic Differential Tool for Measuring Emotional Space","type":"publication"},{"authors":["A Blust, D Baker, K Richard, D Shanahan"],"categories":null,"content":"","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"614da396919873efbc34475b61138138","permalink":"/publication/animacy-proceedings/","publishdate":"2016-08-01T00:00:00Z","relpermalink":"/publication/animacy-proceedings/","section":"publication","summary":"Our  understanding  of—and  preference  for—music  is dependent  upon  the  perception  of  human  agency.  Listeners  often speak  of how  computer-based  performances  lack  the  “soul”  of  a human  performer.  At  the  heart  of  perceived  animacy is  causality, which in music might be thought of as rubato, and other variations in timing. This study focuses on the role of variations in microtiming on the  perceived  animacy  of  a  musical  performance.    Recent  work has shown  that  the  perception  of  visual  animacy  is  likely  categorical, rather  than  gradual.  Although  a  number  of  studies have  examined auditory animacy, there has been very little research done on whether it might be thought of as a dichotomy of alive/not alive, rather than a continuum.  The  current  study  examines  the  specific  intricacies  of musical  animacy,  specifically,  how  microtiming  variations  of  inter-onset  intervals  contribute  to  the  perception  that  a  piece  was  human performed.  Additionally,  this  study  aims  to  examine  the  possible nature  of  categorical/continuous  perception  of  musical  animacy.  In Experiment  1:  “Rohum”,  computer  sequenced  MIDI  renditions  were manipulated   to   contain   set   random   fluctuations   of   inter-onset intervals.  In  Experiment  2:  “Humbot”,  participants  were  presented with  human  performances  digitally  recorded  using  MIDI  keyboards, and  were  asked  how  “alive”  each  performance  sounded  using  a  7-point   Likert   scale.   Human   performances   were   divided   into   ten degrees of quantization strength, increasing from raw performance to 100%  quantization.  Results  suggest  an  optimal  level  of  quantization strength  that  is  correlated  with  higher  perceived  animacy,  and  fixed random  fluctuations  of  IOI  are  not  a  good  indicator  of  human performance.   This   paper   discusses   the   role   of   external   stylistic assumptions  on  perceived  performances,  and  also  takes  into  account musical sophistication indices and experience.","tags":null,"title":"Music, animacy, and rubato: what makes music sound human?","type":"publication"},{"authors":["D Baker, A Rosado, D Shanahan, E Shanahan"],"categories":null,"content":"","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"786d71b14bf172bca0b4000d59298426","permalink":"/publication/bebop-proceedings/","publishdate":"2016-08-01T00:00:00Z","relpermalink":"/publication/bebop-proceedings/","section":"publication","summary":"Improvisational  styles  rely  on  an  implicit  knowledge  of  various musical  gesture,  ranging  from  not-to-note  transitions  to  those  that exist  on  a  more  structural  level.  These prototypical musical gesture are  linked  and  transformed  over  time,  and  are  intertwined  with instrumental  idiomaticisms,  which  are  derived  from  the  structure  of the  instrument  and  the  amount  of effort  required  by  certain  musical gestures (see Huron and Berec, 2009; Gjerdingen, 2009).  This study explores how these stylistic and physical constraints interact in bebop improvisation  –specifically  the  transcribed  solos  of  Charlie  Parker, Dizzy  Gillespie,  an  Clifford  Brown.    The  aims  of  this  study  are fourfold.  Firstly, we aim to replicate the finding of Huron and Berec (2009),  which  found  that  keys  chosen  for  compositions  correspond with the most idiomatic transitions.  Secondly, we hypothesis that an “arc    of    idiomaticism”    occurs    throughout    the    course    of    an improvisation.      Thirdly   this   study   examines   the   most   common structural  schemata  in  the  corpus,  looking  for  the  most  common occurrences   of   temporally   regular   gesture,   which   were   then categorized to create a schema-derived taxonomy.  Lastly, this study examines  the  interaction  of  the  idiomatic  gestures  with  the  most common    improvisatory    schemata in    the    corpus.    Examining relationships between stylistic usage and affordant idiomaticism.  This study  used  data  taken  from  trumpets  and  saxophonists  to  provide  a metric for the relative difficulty of not-to-note transitions, taken from the corpus.  A web-based study then asked self-identified trumpeters and   saxophonists   to   describe   the   relative   difficulty   of   certain note-to-note transitions.  A difficulty metric was then attached to each note-to-note transition in a corpus of bebop improvisations encoded in *kern  format  (Huron,  1995).    This  study  suggests  that  use  of improvisational  corpora  can  serve  as  a  novel  medium  to  further explore the relationship between stylistic and physical constraints.","tags":null,"title":"The Role of Idiomaticism and Affordances in Bebop Improvisation","type":"publication"},{"authors":null,"categories":["cognition"],"content":"This two-day workshop is designed for those who want to take their R Markdown skills to the next level. We’ll talk about many low-level details in the rmarkdown package and the whole R Markdown ecosystem. The two goals of this workshop are: 1) learn how to fully customize R Markdown output (HTML, LaTeX/PDF, Word, and PowerPoint); and 2) learn more about existing R Markdown extensions in the ecosystem, such as flexdashboard, bookdown, blogdown, pkgdown, xaringan, rticles, and learnr. We will also talk about how to use or develop new language engines (languages that are not R), how to develop HTML widgets, and integrate Shiny with R Markdown.\nSee the workshop website for more, and links to individual slides below.\n  Main interest as psychologist is how music can help inform how we learn about cognition.\n  Music is special for these reasons in particular.\n  but also collaborate on lots of other projects within OSF.\n  Can find link to actual papers and their affiliated blog posts (for the less inclined) on pages\n  Mem Cognition\n  EE Preprint\n  SDRT\n  Old SEM\n  Frontiers\n  Fixed Relative\n  IRT\n  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"419ac1bf059193f3e57264ff2f7e404b","permalink":"/my-projects/cognition/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/my-projects/cognition/","section":"my-projects","summary":"A two-day workshop for those who want to take their R Markdown skills to the next level","tags":["workshop","R","blogdown","xaringan","bookdown","flexdashboard","rmarkdown"],"title":"Advanced R Markdown","type":"my-projects"},{"authors":null,"categories":["industry"],"content":"This two-day workshop is designed for those who want to take their R Markdown skills to the next level. We’ll talk about many low-level details in the rmarkdown package and the whole R Markdown ecosystem. The two goals of this workshop are: 1) learn how to fully customize R Markdown output (HTML, LaTeX/PDF, Word, and PowerPoint); and 2) learn more about existing R Markdown extensions in the ecosystem, such as flexdashboard, bookdown, blogdown, pkgdown, xaringan, rticles, and learnr. We will also talk about how to use or develop new language engines (languages that are not R), how to develop HTML widgets, and integrate Shiny with R Markdown.\nSee the workshop website for more, and links to individual slides below.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"29cef1e2dd73174fd350fbc58401f99f","permalink":"/my-projects/mmd/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/my-projects/mmd/","section":"my-projects","summary":"A two-day workshop for those who want to take their R Markdown skills to the next level","tags":["workshop","R","blogdown","xaringan","bookdown","flexdashboard","rmarkdown"],"title":"Advanced R Markdown","type":"my-projects"},{"authors":null,"categories":["industry"],"content":"Description Here.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"b90621bd6e992821d5d21aeacafe0a9c","permalink":"/my-projects/brainwaves/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/my-projects/brainwaves/","section":"my-projects","summary":"A two-day workshop for those who want to take their R Markdown skills to the next level","tags":["workshop","R","blogdown","xaringan","bookdown","flexdashboard","rmarkdown"],"title":"brainwaves","type":"my-projects"},{"authors":["D Müllensiefen, D Baker, C Rhodes, T Crawford, L Dreyfus"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"35e7fde9f1fa01d7daebadde51c52feb","permalink":"/publication/wagner-irt/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/wagner-irt/","section":"publication","summary":"In this study we aim to understand listeners’ real-time processing ofmusical leitmotives. We probe participants’ memory for different leitmotives con-tained in a 10-minute passage from the opera Siegfried by Richard Wagner, anduse item response theory to estimate parameters for item difficulty and for par-ticipants’ individual recognition ability, as well as to construct novel measurementinstruments from questionnaire-based tests. We investigate the relationship betweenmodel parameters and objective factors, finding that prior Wagner expertise and mu-sical training were significant predictors of leitmotive recognition ability, while itemdifficulty is explained by chroma distance and perceived emotional content of the leitmotives.","tags":null,"title":"Recognition of Leitmotives in Richard Wagner’s Music: An Item Response Theory Approach","type":"publication"},{"authors":["D Müllensiefen, D Baker"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"0a59c6ec9146610f45e3396e117b3dc0","permalink":"/publication/aba-2015/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/aba-2015/","section":"publication","summary":"This paper summerises material given at a workshop presented as part of the Audio Branding Academy's Academy Day on 27 May 2015. At the workshop we presented empirical findings and insights from research in psychology, statistics, as well as marketing in order to contribute to a better understanding of how music works within advertising and branding environments. In addition we presented three practical tools (the Q Methodology, the Brand Music Matrix, and the Semantic Differential) that can be employed to assess music choices in practical music branding tasks. Data was collected with these tools at the workshop is presented and discussed. In a final section we briefly describe various ways in which industry professionals can pre-test the effects of music on advertising and brand perception using implicit measurement techniques.","tags":null,"title":"Music, Brands, \u0026 Advertising: Testing What Works","type":"publication"},{"authors":[],"categories":["industry","audio branding"],"content":"Veritonic  Add here  ### Sony\n Add here  Soundout Working with Soundout, I worked on a project with their lead data scientist Tabi Trahan and Dr. Daniel Müllensifen to develop a tool that mapped Music and Brand personality onto the same emotional space. The details of the project can be found in the proceedings of the 14th International Conference on Music Perception and Cognition. The tool served as the basis for what later became their BrandMatch tool.\nAudio Branding Academy In 2014, I presented with Dr. Daniel Müllensifen at the Audio Branding Academy annual conference in Berlin, Germany. We gave a half-day workshop on tools that help bridge gaps between academics, creatives, and industry minded people. A summary of our workshop can be found in their annual yearbook from that year.\n","date":1412869108,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1412869108,"objectID":"5ca6ae647d2f169a604aedf4fe9e2ae4","permalink":"/my-projects/music_industry_consultancy/","publishdate":"2014-10-09T08:38:28-07:00","relpermalink":"/my-projects/music_industry_consultancy/","section":"my-projects","summary":"Workshop presented at ABA Academy Day","tags":["industry"],"title":"Consultancy Projects","type":"my-projects"}]