<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on David John Baker</title>
    <link>/tags/r/</link>
    <description>Recent content in R on David John Baker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 David John Baker</copyright>
    <lastBuildDate>Mon, 15 Jan 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/r/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Hire Me (as a Data Scientist!), Part II</title>
      <link>/post/hire-me-as-a-data-scientist-part-ii/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/hire-me-as-a-data-scientist-part-ii/</guid>
      <description>&lt;div id=&#34;beer-reccomendation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beer Reccomendation&lt;/h2&gt;
&lt;p&gt;Continuing on from my earlier post, I’m now looking to tackle the question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you had to pick 3 beers to recommend using only this data, which would you pick?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a pretty open ended question, which is kind of fun. I also don’t really have a ton of experience (yet!) in recommendation systems, though I have done a little reading here or there on it.&lt;/p&gt;
&lt;p&gt;My goals in coming up with three beers to recommend were to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Try to find the most popular beer among super users of the website&lt;/li&gt;
&lt;li&gt;Find a &lt;a href=&#34;https://youtu.be/IcjSDZNbOs0?t=31s&#34;&gt;bizzaro&lt;/a&gt; beer that matched the profile of my first beer, but lives in the &lt;a href=&#34;https://books.google.com/books?hl=en&amp;amp;lr=&amp;amp;id=DTeZAAAAQBAJ&amp;amp;oi=fnd&amp;amp;pg=PT6&amp;amp;dq=anderson+2006+long+tail&amp;amp;ots=MpaGpMbdfD&amp;amp;sig=25QPk_RCCNU2yFoo9nsU0hrt0sc#v=onepage&amp;amp;q=anderson%202006%20long%20tail&amp;amp;f=false&#34;&gt;long tail&lt;/a&gt; of the ratings distribution&lt;/li&gt;
&lt;li&gt;Find the best Beer sans Booze (Highest Rating with lowest ABV)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So let’s begin! Here’s how I went about tackling this question.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;popular-with-super-users&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Popular with Super Users&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#=====================================================================================#
# Following suit of the last post... 
#=====================================================================================#
# Library
library(ggplot2)
library(data.table)
library(stringr)
#=====================================================================================#
beer &amp;lt;- fread(&amp;quot;data/beer_reviews.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
Read 83.8% of 1586614 rows
Read 1586614 rows and 13 (of 13) columns from 0.168 GB file in 00:00:03&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer.complete &amp;lt;- beer[complete.cases(beer)]
#=====================================================================================#&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having had more experience in experimental settings first, one of the first things that I needed to get used to when I started working with non-psychology datasets is the lack of complete sets in what feel like almost everything. Whereas in the &lt;a href=&#34;https://musiccog.lsu.edu/&#34;&gt;lab&lt;/a&gt; we spend lots of time trying to design balanced studies that hopefully don’t violate the litany of assumptions that classic null hypothesis significance testing demands, my first few attempts at analyzing large amounts of data made me realize it’s almost risible to think that you’re going to have even, independent data, ever. This dataset is no different.&lt;/p&gt;
&lt;p&gt;Of all of the unique users on the site, most of them only have done a couple of reviews, but some have essentially made a job out of this. Looking at the distribution of reviews, this is quite clear.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;review.counts &amp;lt;- beer[, .(.N), by = review_profilename][order(-N)]
review.counts &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        review_profilename    N
##     1:     northyorksammy 5817
##     2:      BuckeyeNation 4661
##     3:        mikesgroove 4617
##     4:          Thorpe429 3518
##     5:      womencantsail 3497
##    ---                        
## 33384:          beilfussd    1
## 33385:         MPHSours11    1
## 33386:         jennaizzel    1
## 33387:           hogshead    1
## 33388:            joeebbs    1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(beer[, .(.N), by = review_profilename][order(-N)]$N, 
     breaks = 200,
     xlab = &amp;quot;Number of Reviews&amp;quot;,
     main = &amp;quot;Distribution of Reviews Per User&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-ii_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is pretty important when it comes to modeling the data (discussed in Part III) and not being fully aware of where your ratings are coming from could seriously put the quality of your models at risk.&lt;/p&gt;
&lt;p&gt;So looking at this dataset, I was wondering if there were any sort of implicit assumptions I could make about the data here that might be able to help me find a good beer. One assumption that I don’t think is too wild is that a sample of this population who have gone out of their way and rated over 500 beers probably are more of beer experts than those who have only done a couple of reviews on the site.&lt;/p&gt;
&lt;p&gt;One thing I wanted to check is of all the 1.5 million reviews, where are all of these reviews coming from? Are there enough reviews among the super users that we could use? And what makes someone a super user? I could have been a bit more scientific setting an &lt;em&gt;a priori&lt;/em&gt; threshold, but for this I kind of just looked at that chart above, spit balled thinking 500 might be a good number to check, and then went to see how much of the data is accounted if I put my threshold there. I lucked out and got about half of it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(review.counts$N) # Number of Total Reviews &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1586614&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(review.counts[ N &amp;gt; 500]$N) # Number of Reviews from Super Users&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 731066&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;731066/1586614 # Percent of Total Reviews from 500+ Super Users&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4607712&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.users &amp;lt;- review.counts[ N &amp;gt;  500] # I can settle for .75 Million Reviews&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now we have a list of the users who have done over 500 reviews who make up 46% of our entire data. We can use this new table we’ve made to index through our dataset of all the reviews that we have (that have their ABV ratings!) so we are only dealing with these higher quality reviewers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews &amp;lt;- super.users[beer.complete, on = &amp;quot;review_profilename&amp;quot;, nomatch=0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we continue to chop down our dataset (since this is a very exploratory process compared with cleaning up an experiment), it’s important to do &lt;strong&gt;quality assesment&lt;/strong&gt; steps. One thing worth checking here is to see if we are actually dealing with beer omnivores in our super users and make sure that all different types of beers are being represented in our smaller subset. This can be done but just looking at the number of rows between the original dataset and our super user table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews[, .(beer_styles = unique(beer_style))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                          beer_styles
##   1:                      Hefeweizen
##   2:              English Strong Ale
##   3:          Foreign / Export Stout
##   4:                 German Pilsener
##   5:  American Double / Imperial IPA
##  ---                                
## 100:             Japanese Rice Lager
## 101:                      Roggenbier
## 102:                        Happoshu
## 103:                           Sahti
## 104: Bière de Champagne / Bière Brut&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer[, .(beer_styles = unique(beer_style))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                          beer_styles
##   1:                      Hefeweizen
##   2:              English Strong Ale
##   3:          Foreign / Export Stout
##   4:                 German Pilsener
##   5:  American Double / Imperial IPA
##  ---                                
## 100:                          Gueuze
## 101:                            Gose
## 102:                        Happoshu
## 103:                           Sahti
## 104: Bière de Champagne / Bière Brut&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Luckily they are the same. If I were to really do some more work on this dataset, I would also want to check things such as how many of the beers has each super user tried? Are there IPA experts in the group? If yes, should their opinion be taken more seriously if I had questions about IPA recommendation in the future? But for now, I just set out to see what is the highest rated beer among all the super users of this dataset.&lt;/p&gt;
&lt;p&gt;In order to answer that question, we need to be able to find out which beer in specific has the highest mean rating. The dataset as is comes with a &lt;code&gt;beer_id&lt;/code&gt; unique ID, but the data downloaded as is does not give us a key to this so we have to make it ourselves. This can be accomplished by just pasting together the brewery’s name, along with the beer name, and style into a new variable.&lt;/p&gt;
&lt;p&gt;As another &lt;strong&gt;quality assurance&lt;/strong&gt; step, it’s worth checking to see if this recreated the unique ID variable, which it didn’t do exactly… but it’s pretty close. I would chalk that up to some sort of encoding error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews[, beer_name_unique := paste(brewery_name,beer_name, beer_style)]

length(unique(super.reviews$beer_beerid))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 42825&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(unique(super.reviews$beer_name_unique))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 42703&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;42703/42805 # Pretty close&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9976171&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews.popular &amp;lt;- super.reviews[, .(most_reviewed_beers = .N), 
                                       by = beer_name_unique][order(-most_reviewed_beers)]

hist(super.reviews.popular$most_reviewed_beers,
     main = &amp;quot;Distribution of Number of Ratings by Super Users&amp;quot;,
     xlab = &amp;quot;Number of Reviews each Beer Recieves&amp;quot;,
     breaks = 200)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-ii_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again we see this is clearly &lt;strong&gt;not&lt;/strong&gt; anything resembling a repeated measures experiment and not all beers were rated equally.&lt;/p&gt;
&lt;p&gt;Continuing in the same fashion above let’s grab just the top 100 beers of our super users and then merge that on to our table from earlier that had all of the ratings from our super users. Then of that table, let’s just take the average of the overall rating and look at our top ten beers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews.popular.100 &amp;lt;- super.reviews[, .(most_reviewed_beers = .N), by = beer_name_unique][order(-most_reviewed_beers)][1:100]

super.reviews.cream.of.crop &amp;lt;- super.reviews.popular.100[super.reviews, 
                                                         on = &amp;quot;beer_name_unique&amp;quot;, 
                                                         nomatch=0]

super.reviews.cream.of.crop[, .(mean_review_overall = mean(review_overall)), by = beer_name_unique][order(-mean_review_overall)][1:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                                                      beer_name_unique
##  1:                      Russian River Brewing Company Pliny The Elder American Double / Imperial IPA
##  2:                  Bayerische Staatsbrauerei Weihenstephan Weihenstephaner Hefeweissbier Hefeweizen
##  3:                              Tröegs Brewing Company Tröegs Nugget Nectar American Amber / Red Ale
##  4:                                 Ballast Point Brewing Company Sculpin India Pale Ale American IPA
##  5:                 Three Floyds Brewing Co. &amp;amp; Brewpub Dreadnaught IPA American Double / Imperial IPA
##  6: Founders Brewing Company Founders KBS (Kentucky Breakfast Stout) American Double / Imperial Stout
##  7:                                                 Bell&amp;#39;s Brewery, Inc. Two Hearted Ale American IPA
##  8:                            Bell&amp;#39;s Brewery, Inc. Bell&amp;#39;s Hopslam Ale American Double / Imperial IPA
##  9:                    Three Floyds Brewing Co. &amp;amp; Brewpub Alpha King Pale Ale American Pale Ale (APA)
## 10:                Founders Brewing Company Founders Breakfast Stout American Double / Imperial Stout
##     mean_review_overall
##  1:            4.536630
##  2:            4.535072
##  3:            4.449084
##  4:            4.443287
##  5:            4.367580
##  6:            4.366876
##  7:            4.353270
##  8:            4.349810
##  9:            4.346652
## 10:            4.334526&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we have our winner! It’s &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/863/7971/&#34;&gt;Pliny The Elder&lt;/a&gt; from Russian River Brewing Company as my first beer recommendation!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bizzaro-beer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bizzaro Beer&lt;/h2&gt;
&lt;p&gt;Now Pliny The Elder seems to be a pretty popular beer. But what if we are trying to sketch out some ideas about what other beers that we could recommend to beer lovers who like Pliny The Elder? It needs to somewhat “look like” our target beer, but needs to have way less reviews.&lt;/p&gt;
&lt;p&gt;Playing with some of the fringe data here, I want to be careful not to again pick a beer with only one or two ratings on it. My rationale coming from assuming there is some sort of true “population mean” for this beer and having a beer with too little reviews will not approximate the mean correctly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Make Unique Beer Label for Larger Dataset
beer[, beer_name_unique := paste(brewery_name,beer_name, beer_style)]

## Count Number of Reviews Each Beer Has  
number.of.reviews &amp;lt;- beer[, .(NumberOfReviews = .N), by = beer_name_unique][order(-NumberOfReviews)]
 
## Only get beers with over 30 reviews
reliable.beers.list &amp;lt;- number.of.reviews[ NumberOfReviews &amp;gt;= 30 ]

## Join that to our big &amp;#39;beer&amp;#39; dataset only matching beers with over 30 reviews
beer.reliable &amp;lt;- reliable.beers.list[beer, on = &amp;quot;beer_name_unique&amp;quot;, nomatch=0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With our dataset chiseled down to only ‘reliable’ beers, I needed to find a way to get some sort of profile of each of the beers. While my first instinct was to do some sort of data reductive type thing like do a PCA on our continuous variables and use distances from certain scores as metrics of similarity (&lt;a href=&#34;https://musiccog.lsu.edu/davidjohnbaker/papers/Baker_Trahan_Mullensiefen_ProceedingsPaper.pdf&#34;&gt;which I have done before&lt;/a&gt; and it ended up actually being the inspiration for a tool currently used by &lt;a href=&#34;https://www.soundout.com/brandmatch&#34;&gt;Soundout&lt;/a&gt;!), doing that on so few predictors seemed &lt;a href=&#34;https://www.urbandictionary.com/define.php?term=extra&#34;&gt;extra&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So instead I figured why not just assume that there is some sort of wiggle room in my hastily made recommendation system and just match first on the overall review, then if there are some close contenders, look for matches on other metrics?&lt;/p&gt;
&lt;p&gt;The next bit of code creates a table of the metrics I am interested in, gets beers that have over 30 reviews, but less than 100, and also creates a vector so I can pull out all of the IPAs on my less reviewed beers table. I then join the tables for my candidates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # Get metrics used for distance calculations
beer.metrics &amp;lt;- beer.reliable[, .(mean_review_overall = mean(review_overall),
                                  mean_review_aroma = mean(review_overall),
                                   mean_review_appearance = mean(review_appearance),
                                   mean_review_palate = mean(review_palate),
                                   mean_reviw_taste = mean(review_taste),
                                   sd_review_overall = sd(review_overall),
                                   sd_review_aroma = sd(review_overall),
                                   sd_review_appearance = sd(review_appearance),
                                   sd_review_palate = sd(review_palate),
                                   sd_review_taste = sd(review_taste)),
                               by = beer_name_unique]

## Get only IPAs with less than 100 reviews
less.reviewed.beers &amp;lt;- number.of.reviews[NumberOfReviews &amp;lt;= 100 &amp;amp; NumberOfReviews &amp;gt;= 30]
## Make vector to help find IPAs
find.IPA &amp;lt;- str_detect(string = less.reviewed.beers$beer_name_unique, pattern = &amp;quot;Imperial IPA&amp;quot;)
bizzaro.candidates &amp;lt;- less.reviewed.beers[find.IPA]

#Create Table
bizzaro.candidates.metrics &amp;lt;- bizzaro.candidates[beer.metrics, on = &amp;quot;beer_name_unique&amp;quot;, nomatch=0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of these less reviewed beers, we now need to find the one that is “closest” on the few dimensions we have to work with. The simplest way to do this would be to just subtract our target beer (Pliny The Elder), from every other beer in our interested list, then check out the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get metrics for our target beer
rrbcpteadii.metrics &amp;lt;- beer[beer_name_unique == &amp;quot;Russian River Brewing Company Pliny The Elder American Double / Imperial IPA&amp;quot;,
                           .(mean_review_overall = mean(review_overall),
                             mean_review_aroma = mean(review_overall),
                             mean_review_appearance = mean(review_appearance),
                             mean_review_palate = mean(review_palate),
                             mean_reviw_taste = mean(review_taste),
                             sd_review_overall = sd(review_overall),
                             sd_review_aroma = sd(review_overall),
                             sd_review_appearance = sd(review_appearance),
                             sd_review_palate = sd(review_palate),
                             sd_review_taste = sd(review_taste))]
## Create vector for looping over
key.vector &amp;lt;- as.vector(rrbcpteadii.metrics)
## Pull off the tags of our search
search.vector &amp;lt;- bizzaro.candidates.metrics[, -c(1,2)]

## Sanity check that what we are going to subtract has same names
names(key.vector)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;mean_review_overall&amp;quot;    &amp;quot;mean_review_aroma&amp;quot;     
##  [3] &amp;quot;mean_review_appearance&amp;quot; &amp;quot;mean_review_palate&amp;quot;    
##  [5] &amp;quot;mean_reviw_taste&amp;quot;       &amp;quot;sd_review_overall&amp;quot;     
##  [7] &amp;quot;sd_review_aroma&amp;quot;        &amp;quot;sd_review_appearance&amp;quot;  
##  [9] &amp;quot;sd_review_palate&amp;quot;       &amp;quot;sd_review_taste&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(search.vector)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;mean_review_overall&amp;quot;    &amp;quot;mean_review_aroma&amp;quot;     
##  [3] &amp;quot;mean_review_appearance&amp;quot; &amp;quot;mean_review_palate&amp;quot;    
##  [5] &amp;quot;mean_reviw_taste&amp;quot;       &amp;quot;sd_review_overall&amp;quot;     
##  [7] &amp;quot;sd_review_aroma&amp;quot;        &amp;quot;sd_review_appearance&amp;quot;  
##  [9] &amp;quot;sd_review_palate&amp;quot;       &amp;quot;sd_review_taste&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## And that the apply function I am going to run is doing what I think it will
search.vector[1]- key.vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    mean_review_overall mean_review_aroma mean_review_appearance
## 1:          -0.7900277        -0.7900277             -0.6386031
##    mean_review_palate mean_reviw_taste sd_review_overall sd_review_aroma
## 1:         -0.6263257       -0.8393187        0.09103239      0.09103239
##    sd_review_appearance sd_review_palate sd_review_taste
## 1:            0.1273011          0.11882       0.1073594&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Run apply function
ipa.distances &amp;lt;- apply(search.vector, 1, function(x) x - key.vector)
ipa.distances.dt &amp;lt;- data.table(do.call(rbind.data.frame,ipa.distances))
## Combine this back with vector with names
bizzaro.candidates.distances &amp;lt;- cbind(bizzaro.candidates.metrics, ipa.distances.dt)
## Sort our data by overall and see if we have a good match!
bizzaro.candidates.distances[order(-mean_review_overall)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                                          beer_name_unique
##   1: Hill Farmstead Brewery Galaxy Imperial Single Hop IPA American Double / Imperial IPA
##   2:           Lawson&amp;#39;s Finest Liquids Double Sunshine IPA American Double / Imperial IPA
##   3:        Kern River Brewing Company 5th Anniversary Ale American Double / Imperial IPA
##   4:                           Alpine Beer Company Bad Boy American Double / Imperial IPA
##   5:             Iron Hill Brewery &amp;amp; Restaurant Kryptonite American Double / Imperial IPA
##  ---                                                                                     
## 132:                            BrewDog Sink The Bismarck! American Double / Imperial IPA
## 133:                   Blue Frog Grog &amp;amp; Grill The Big DIPA American Double / Imperial IPA
## 134:                            Hermitage Brewing Hoptopia American Double / Imperial IPA
## 135:                    Florida Beer Company Swamp Ape IPA American Double / Imperial IPA
## 136:            BrewDog Storm (Islay Whisky Cask Aged IPA) American Double / Imperial IPA
##      NumberOfReviews mean_review_overall mean_review_aroma
##   1:              76            4.592105          4.592105
##   2:              85            4.588235          4.588235
##   3:              41            4.475610          4.475610
##   4:              79            4.468354          4.468354
##   5:              44            4.443182          4.443182
##  ---                                                      
## 132:              76            3.197368          3.197368
## 133:              53            2.867925          2.867925
## 134:              32            2.687500          2.687500
## 135:              52            2.615385          2.615385
## 136:              92            2.440217          2.440217
##      mean_review_appearance mean_review_palate mean_reviw_taste
##   1:               4.368421           4.440789         4.559211
##   2:               4.317647           4.311765         4.552941
##   3:               4.329268           4.219512         4.500000
##   4:               4.234177           4.335443         4.474684
##   5:               4.284091           4.318182         4.420455
##  ---                                                           
## 132:               3.835526           3.401316         3.539474
## 133:               3.433962           3.047170         2.801887
## 134:               3.500000           2.890625         2.500000
## 135:               3.442308           3.009615         2.586538
## 136:               2.902174           2.836957         2.706522
##      sd_review_overall sd_review_aroma sd_review_appearance
##   1:         0.3337716       0.3337716            0.3861642
##   2:         0.3289275       0.3289275            0.3845766
##   3:         0.3865103       0.3865103            0.3641730
##   4:         0.3698733       0.3698733            0.3741267
##   5:         0.3768892       0.3768892            0.3796836
##  ---                                                       
## 132:         1.2438621       1.2438621            0.7136206
## 133:         0.8995241       0.8995241            0.6866707
## 134:         1.2427207       1.2427207            0.4918694
## 135:         0.9108033       0.9108033            0.5994593
## 136:         0.9829379       0.9829379            0.6843587
##      sd_review_palate sd_review_taste mean_review_overall
##   1:        0.3997258       0.3826844         0.002077562
##   2:        0.4293993       0.3620669        -0.001792407
##   3:        0.4749840       0.4873397        -0.114417945
##   4:        0.4060561       0.3660140        -0.121673270
##   5:        0.4586495       0.4026537        -0.146845883
##  ---                                                     
## 132:        1.1431528       1.1277209        -1.392659280
## 133:        0.7355280       0.8165336        -1.722103173
## 134:        0.8005479       0.9418581        -1.902527701
## 135:        0.7637009       1.0035288        -1.974643085
## 136:        0.7883377       1.0086226        -2.149810310
##      mean_review_aroma mean_review_appearance mean_review_palate
##   1:       0.002077562            -0.02018203        -0.01053621
##   2:      -0.001792407            -0.07095603        -0.13956098
##   3:      -0.114417945            -0.05933479        -0.23181349
##   4:      -0.121673270            -0.15442587        -0.11588264
##   5:      -0.146845883            -0.10451218        -0.13314386
##  ---                                                            
## 132:      -1.392659280            -0.55307677        -1.05000989
## 133:      -1.722103173            -0.95464082        -1.40415587
## 134:      -1.902527701            -0.88860309        -1.56070068
## 135:      -1.974643085            -0.94629539        -1.44171030
## 136:      -2.149810310            -1.48642917        -1.61436916
##      mean_reviw_taste sd_review_overall sd_review_aroma
##   1:      -0.07177483       -0.12136909     -0.12136909
##   2:      -0.07804418       -0.12621327     -0.12621327
##   3:      -0.13098536       -0.06863039     -0.06863039
##   4:      -0.15630181       -0.08526747     -0.08526747
##   5:      -0.21053081       -0.07825155     -0.07825155
##  ---                                                   
## 132:      -1.09151167        0.78872139      0.78872139
## 133:      -1.82909857        0.44438341      0.44438341
## 134:      -2.13098536        0.78758001      0.78758001
## 135:      -2.04444690        0.45566254      0.45566254
## 136:      -1.92446362        0.52779720      0.52779720
##      sd_review_appearance sd_review_palate sd_review_taste
##   1:          -0.01935577      -0.04762635     -0.03307969
##   2:          -0.02094339      -0.01795284     -0.05369722
##   3:          -0.04134702       0.02763182      0.07157561
##   4:          -0.03139329      -0.04129607     -0.04975012
##   5:          -0.02583641       0.01129741     -0.01311039
##  ---                                                      
## 132:           0.30810063       0.69580063      0.71195677
## 133:           0.28115074       0.28817587      0.40076950
## 134:           0.08634938       0.35319581      0.52609404
## 135:           0.19393929       0.31634876      0.58776473
## 136:           0.27883874       0.34098558      0.59285853&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course if we were building a real recommendation machine we could start talking about what factors are more important for what users and what factors are more predictive than others, but this seems like an OK enough solution to at least have completed my &lt;em&gt;a priori&lt;/em&gt; goal.&lt;/p&gt;
&lt;p&gt;Based on this solution, it looks like I will have to find myself a bottle of &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/863/7971/&#34;&gt;Pliny The Elder&lt;/a&gt; and the &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/22511/67760/&#34;&gt;Hill Farmstead Brewery Galaxy Imperial Single Hop IPA American Double / Imperial IPA&lt;/a&gt; and do some of my own empirical work to see if this was a good idea.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;beer-sans-booze&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beer sans Booze&lt;/h2&gt;
&lt;p&gt;The last beer that I think wanted to recommend would be one that tastes great, but does not have a lot of alcohol with it. The reason this question kind of interests me is because if we are &lt;em&gt;really&lt;/em&gt; going to talk about how tasty a beer is, it would be nice to be able to factor out of the equation how drunk we are actually getting from it.&lt;/p&gt;
&lt;p&gt;We can see first of all IF this relationship exists if we look at the mean overall rating of a beer as a function of its ABV content.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer.complete[, beer_name_unique := paste(brewery_name,beer_name, beer_style) ]

# ABVs and Mean Scores
abv.vs.mean &amp;lt;- beer.complete[, .(Abv = mean(beer_abv), MeanOverall = mean(review_overall)), by = beer_name_unique]

ggplot(abv.vs.mean[Abv &amp;lt; 20], aes(x = Abv, y = MeanOverall)) + 
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;, color = &amp;quot;blue&amp;quot;) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, formula = y ~ poly(x,2), color = &amp;quot;orange&amp;quot;) +
  labs(title = &amp;quot;Rating as Function of ABV (Beers with than 20% ABV)&amp;quot;,
       x = &amp;quot;ABV Content&amp;quot;,
       y = &amp;quot;Mean Overall Rating&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-ii_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Surprisingly, when we run some quick and dirty regression models (that yes, I know violate tons of assumptions) we see that only a very small amount of variance is being explained by its ABV. Note that although the models are significant, the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; values hover around 3-5%!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# &amp;quot;The More Booze The Better&amp;quot; Model
abv.linear &amp;lt;- lm(MeanOverall ~ Abv, data = abv.vs.mean[Abv &amp;lt; 20]) 
# The &amp;quot;Diminishing Returns Model &amp;quot;
abv.poly &amp;lt;- lm(MeanOverall ~ poly(Abv,2), data = abv.vs.mean[Abv &amp;lt; 20])
# The &amp;quot;Dissapoint Amount of Variance Explained Summaries&amp;quot;
summary(abv.linear)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = MeanOverall ~ Abv, data = abv.vs.mean[Abv &amp;lt; 20])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.06793 -0.27278  0.09684  0.38850  1.72081 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 3.276138   0.008995  364.20   &amp;lt;2e-16 ***
## Abv         0.060975   0.001369   44.55   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.6013 on 48822 degrees of freedom
## Multiple R-squared:  0.03906,    Adjusted R-squared:  0.03904 
## F-statistic:  1984 on 1 and 48822 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(abv.poly)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = MeanOverall ~ poly(Abv, 2), data = abv.vs.mean[Abv &amp;lt; 
##     20])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.88738 -0.28351  0.09695  0.37907  2.17585 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)     3.658087   0.002706 1351.73   &amp;lt;2e-16 ***
## poly(Abv, 2)1  26.785156   0.597970   44.79   &amp;lt;2e-16 ***
## poly(Abv, 2)2 -13.922969   0.597970  -23.28   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.598 on 48821 degrees of freedom
## Multiple R-squared:  0.04961,    Adjusted R-squared:  0.04957 
## F-statistic:  1274 on 2 and 48821 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This actually surprised me and might be worth looking into at a deeper level another time, but for now I want to keep going on and find a beer knowing that how much booze is in it doesn’t really affect how good it is.&lt;/p&gt;
&lt;p&gt;So let’s take one final dive into the dataset, grab only our quality reviews then plot a subset of our data so we can see beers that have a very high overall rating with a very small amount of booze in them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Quality Assurance Step
reliable.and.abv.beers &amp;lt;- reliable.beers.list[beer.complete, on = &amp;quot;beer_name_unique&amp;quot;, nomatch=0]

## Get mean ratings and keep ABV (which won&amp;#39;t change if I average it)
dd.beers &amp;lt;- reliable.and.abv.beers[, .(mean_overall = mean(review_overall), abv = mean(beer_abv)), by = &amp;quot;beer_name_unique&amp;quot;]

# Only Beers that Fit Our Criterion
dd.beers.2 &amp;lt;- dd.beers[mean_overall &amp;gt; 4.6 &amp;amp; abv &amp;lt; 10]

# Plot It!
ggplot(dd.beers.2, aes(x = abv, y = mean_overall, label = beer_name_unique, color = beer_name_unique)) +
  geom_point() +
  geom_text(aes(label=beer_name_unique),hjust=-.01, vjust=0) +
  labs(title = &amp;quot;High Quality Beers with Low ABV&amp;quot;,
       x = &amp;quot;ABV&amp;quot;,
       y = &amp;quot;Mean Overall Rating&amp;quot;) + theme(legend.position = &amp;quot;none&amp;quot;) +
  xlim(0, 20) +
  scale_y_continuous(breaks = c(seq(4.6,5,.1)), limits = c(4.6,4.85))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-ii_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These are all OK choices (most of the beers are still above 5% ABV), but we do have one beer clocking in at 2.0% ABV giving us our final beer recommendation – the &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/1628/8626/&#34;&gt;Southampton Publick House Southampton Berliner Weisse Berliner Weissbier&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;After all of this I know have three beers to check out. &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/863/7971/&#34;&gt;Pliny The Elder&lt;/a&gt; is our winner for the top rated beer among our Super Users of the site, the &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/22511/67760/&#34;&gt;Hill Farmstead Brewery Galaxy Imperial Single Hop IPA American Double / Imperial IPA&lt;/a&gt; is a beer to maybe follow up on from our first choice, and then lastly we have the the &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/1628/8626/&#34;&gt;Southampton Publick House Southampton Berliner Weisse Berliner Weissbier&lt;/a&gt; which supposedly tastes great, despite its lack of alcohol content.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hire Me (as a Data Scientist!), Part I</title>
      <link>/post/hire-me-as-a-data-scientist-part-i/</link>
      <pubDate>Sat, 13 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/hire-me-as-a-data-scientist-part-i/</guid>
      <description>&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;I read &lt;a href=&#34;https://medium.com/&#34;&gt;Medium&lt;/a&gt; blogs posts on “How to Become a Data Scientist” more often than I care to admit. Much of this comes from a fear that after doing all this work in the PhD and then hitting the Music Theory job market that I won’t fit the mold of the kind of theorist most schools want to hire. Not coming from &lt;a href=&#34;https://pushpullfork.com/music-theory-job-market/&#34;&gt;one of five schools that seem to have a monopoly on the tenure track jobs&lt;/a&gt; can be a bit discouraging, but I also won’t deny that having a non-academic job with a regular 9-5 schedule and a &lt;a href=&#34;https://www.glassdoor.com/Salaries/data-scientist-salary-SRCH_KO0,14.htm&#34;&gt;decent salary&lt;/a&gt; is pretty tempting after spending the vast majority of my twenties in school. And even if I don’t go on over to industry after the PhD, I’ll probably always be looking for a bit more work in summer.&lt;/p&gt;
&lt;p&gt;On top of all of that, I believe that skills that acquired in a PhD (especially if you do computational musicology and music cognition!) are very transferable to most jobs and it’s just a matter of being a bit more pro-active in promoting myself that might help me one day land a stable, non-academic job.&lt;/p&gt;
&lt;p&gt;That said, one tweet that I saw last week by &lt;a href=&#34;https://twitter.com/kierisi&#34;&gt;Jesse Meagan&lt;/a&gt; linked to this really interesting Linked-In post by &lt;a href=&#34;https://twitter.com/tanyacash21&#34;&gt;Tanya Cashorali&lt;/a&gt; that proported to have a &lt;a href=&#34;https://www.linkedin.com/pulse/how-hire-test-data-skills-one-size-fits-all-interview-tanya-cashorali/&#34;&gt;one size fits all data-science interview process&lt;/a&gt; which has candidates take home a big dataset with a bunch of beer reviews and answer four very broad questions. Considering myself an aficionado of How-To-Become-a-Data Scientist articles, this of course caught my eye.&lt;/p&gt;
&lt;p&gt;After reading the article I figured why not give it a go? It’s the start of the semester, I’m basically ABD, need more of a portfolio beyond &lt;a href=&#34;www.github.com/davidjohnbaker1&#34;&gt;my github&lt;/a&gt;, and I have nothing to do with my Saturday morning. So why not see what I can produce in 4 or 5 hours? At the very least I’ll hopefully just have something to point to if a future employer wants to see how I think through data-science problems.&lt;/p&gt;
&lt;p&gt;And if anyone is reading this that does have comments on my code or thought process… please let me know what you think on &lt;a href=&#34;www.twitter.com/DavidJohnBaker&#34;&gt;Twitter&lt;/a&gt;! I’d love some feedback!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-the-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring the Dataset&lt;/h2&gt;
&lt;p&gt;The first thing I did was to grab this dataset which you can get &lt;a href=&#34;https://s3.amazonaws.com/demo-datasets/beer_reviews.tar.gz?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3BDqDjqRycTlq5seB4xN3ocA%3D%3D&#34;&gt;here&lt;/a&gt; and then I set up my R script with a few of my favorite packages (again, big love to Ben at &lt;a href=&#34;https://gormanalysis.com/&#34;&gt;GormAnalysis&lt;/a&gt; for helping me learn &lt;a href=&#34;https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html&#34;&gt;data.table&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#=====================================================================================#
# Beer Script
#=====================================================================================#
# Library
library(ggplot2)
library(data.table)
library(stringr)
#=====================================================================================#
beer &amp;lt;- fread(&amp;quot;data/beer_reviews.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
Read 71.2% of 1586614 rows
Read 1586614 rows and 13 (of 13) columns from 0.168 GB file in 00:00:03&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#=====================================================================================#&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset has about 1.5 million observations across 14 different observations, so don’t try to open it in LibreOffice. The reviews come from a variety of different users that have rated the beers based on five different attributes (Appearance, Pallete, Aroma, Taste, Overall) and then each beer has a few other variables listed such as its ABV, the brewery it comes from, the beer’s name (duh), and what kind of beer it is.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(beer)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;brewery_id&amp;quot;         &amp;quot;brewery_name&amp;quot;       &amp;quot;review_time&amp;quot;       
##  [4] &amp;quot;review_overall&amp;quot;     &amp;quot;review_aroma&amp;quot;       &amp;quot;review_appearance&amp;quot; 
##  [7] &amp;quot;review_profilename&amp;quot; &amp;quot;beer_style&amp;quot;         &amp;quot;review_palate&amp;quot;     
## [10] &amp;quot;review_taste&amp;quot;       &amp;quot;beer_name&amp;quot;          &amp;quot;beer_abv&amp;quot;          
## [13] &amp;quot;beer_beerid&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer[, .(.N = unique(beer$brewery_name))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                 .N
##    1:              Vecchio Birraio
##    2:      Caldera Brewing Company
##    3:       Amstel Brouwerij B. V.
##    4:        Broad Ripple Brew Pub
##    5:   Moon River Brewing Company
##   ---                             
## 5739:        Gattopardo Cervejaria
## 5740:         Brauerei Lasser GmbH
## 5741:        Wissey Valley Brewery
## 5742:      Outback Brewery Pty Ltd
## 5743: Georg Meinel Bierbrauerei KG&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer[, .(.N = unique(beer$review_profilename))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    .N
##     1:        stcules
##     2: johnmichaelsen
##     3:        oline73
##     4:      Reidrover
##     5:   alpinebryant
##    ---               
## 33384:     jennaizzel
## 33385:    mine2design
## 33386:       hogshead
## 33387:     NyackNicky
## 33388:        joeebbs&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer[, .(.N = unique(beer$beer_name))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                            .N
##     1:           Sausa Weizen
##     2:               Red Moon
##     3: Black Horse Black Beer
##     4:             Sausa Pils
##     5:          Cauldron DIPA
##    ---                       
## 56853:      Bear Mountain Ale
## 56854:        Highland Porter
## 56855:       Baron Von Weizen
## 56856:          Resolution #2
## 56857:     The Horseman&amp;#39;s Ale&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer[, .(.N = unique(beer$beer_style))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                   .N
##   1:                      Hefeweizen
##   2:              English Strong Ale
##   3:          Foreign / Export Stout
##   4:                 German Pilsener
##   5:  American Double / Imperial IPA
##  ---                                
## 100:                          Gueuze
## 101:                            Gose
## 102:                        Happoshu
## 103:                           Sahti
## 104: Bière de Champagne / Bière Brut&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From a bird’s eye view we have 56,857 unique beers in 104 different categories from 5,743 different breweries and 33,388 unique beer aficionados who have gone out of their way to tell this website what they think about the beers they drink.&lt;/p&gt;
&lt;p&gt;Before diving in further, it’s worth doing a preliminary check of the quality of the data (aka we should know if this is BAD (Best Available Data) or has undergone a fair deal of cleaning). As someone who comes from a more psychology background, I’ve noticed what certain people consider “clean” when it comes to data varies a lot.&lt;/p&gt;
&lt;p&gt;The first thing I check for is if there is any kind of data missing and if there is, is it due to chance? Or is it due to some sort of systematic variation?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(complete.cases(beer))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   FALSE    TRUE 
##   67785 1518829&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;67785/1518829&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04462978&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So about 4% of our rows don’t have every entry, so probably not too much cause for concern unless we start getting into specific questions about specific beers. Looking into this a bit further it seems like it’s just beers missing the ABV of the beer. Anyone that has made some beer ratings has made ratings on all five variables. And although it’s only 4% of our entire ratings that don’t have their ABV, comparing that to every beer we have, we see we are actually missing ~25% of the ABV ratings of all of our beers. That could be a problem later, but it’s good to know about it sooner rather than later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer.complete &amp;lt;- beer[complete.cases(beer)]

beer[!complete.cases(beer)][, .(.N = unique(beer_name))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                             .N
##     1: Cauldron Espresso Stout
##     2:    The Highland Stagger
##     3:              Alpha Beta
##     4:          Imperial Stout
##     5:               Megalodon
##    ---                        
## 14106:       English Nut Brown
## 14107:              Hop Common
## 14108:     Very Hoppy Pale Ale
## 14109:       Prohibition Lager
## 14110:           Resolution #2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;14110/56857&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2481665&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer[969]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    brewery_id             brewery_name review_time review_overall
## 1:      12770 City Grille and Brewhaus  1145738954              4
##    review_aroma review_appearance review_profilename
## 1:            3                 4         UncleJimbo
##                 beer_style review_palate review_taste     beer_name
## 1: American Pale Ale (APA)           3.5            4 City Pale Ale
##    beer_abv beer_beerid
## 1:       NA       30088&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More problems might come up here or there, but let’s move on the the first question.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;which-brewery-produces-the-strongest-beers-by-abv&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Which brewery produces the strongest beers by ABV%?&lt;/h2&gt;
&lt;p&gt;Answering the first question on the list is pretty straight forward. Essentially all you need to do is grab all of the observations that have an ABV associated with their rating, then get the average ABV of all the beers that that brewery produces.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Use object before that has only ratings with ABVs 
beer.complete[, .(AvgABV = mean(beer_abv)), by = brewery_name]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       brewery_name   AvgABV
##    1:              Vecchio Birraio 5.675000
##    2:      Caldera Brewing Company 6.168849
##    3:       Amstel Brouwerij B. V. 3.816373
##    4:        Broad Ripple Brew Pub 6.006202
##    5:   Moon River Brewing Company 5.724103
##   ---                                      
## 5152:        Gattopardo Cervejaria 6.033333
## 5153:         Brauerei Lasser GmbH 5.200000
## 5154:        Wissey Valley Brewery 5.133333
## 5155:      Outback Brewery Pty Ltd 4.787879
## 5156: Georg Meinel Bierbrauerei KG 5.850000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create table that has means and standard deviations of beers by brewery 
# Order them from most to least
abv.counter &amp;lt;- beer.complete[, .(AvgABV = mean(beer_abv), 
                                 SdABV = sd(beer_abv)) , 
                             by = brewery_name][order(-AvgABV)]
abv.counter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                              brewery_name     AvgABV      SdABV
##    1:                        Schorschbräu 19.2288235 12.3273042
##    2:                       Shoes Brewery 15.2000000  0.0000000
##    3:                Rome Brewing Company 13.8400000  1.9718012
##    4:                   Hurlimann Brewery 13.7500000  0.5752237
##    5:            Alt-Oberurseler Brauhaus 13.2000000         NA
##   ---                                                          
## 5152:             Cerveceria Vegana, S.A.  2.2608696  2.2455490
## 5153: Moskovskaya Pivovarennaya Kompaniya  2.1500000  1.6881943
## 5154:                      Fentimans Ltd.  1.3750000  1.6201852
## 5155:                        Borodino ZAO  0.9666667  0.4041452
## 5156:                    All Stars Bakery  0.5000000  0.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having this table could be good enough for government work, but looking at the output there are clearly problems and one thing to consider in this table (and pretty much this whole dataset) is “Is this data point a good representation of what I am trying to measure?”. Note for example the huge variability as measured by the standard deviation in our top answer as well as the fact that some of the SDs have NAs and there is a value of 0. Given that, I think it’d be good to put on some sort of threshold that would up the quality of our answers. One way to do this would be to see exactly how many beers each brewery makes and use that as a proxy for how big the brewery is.&lt;/p&gt;
&lt;p&gt;The code below does just that and reveals the variability in terms of size of breweries within this dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create table that counts number of beers 
NoOfBeers &amp;lt;- beer.complete[, .(NameOfBeer = unique(beer_name)), 
                           by = brewery_name][, .(.N), by = brewery_name]
NoOfBeers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       brewery_name  N
##    1:              Vecchio Birraio  4
##    2:      Caldera Brewing Company 25
##    3:       Amstel Brouwerij B. V.  9
##    4:        Broad Ripple Brew Pub 40
##    5:   Moon River Brewing Company 34
##   ---                                
## 5152:        Gattopardo Cervejaria  3
## 5153:         Brauerei Lasser GmbH  1
## 5154:        Wissey Valley Brewery  3
## 5155:      Outback Brewery Pty Ltd  6
## 5156: Georg Meinel Bierbrauerei KG  2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make table that lists each beer with it&amp;#39;s ABV and the name of the brewery
abv.table &amp;lt;- NoOfBeers[abv.counter, on = &amp;quot;brewery_name&amp;quot;]
abv.table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                              brewery_name  N     AvgABV      SdABV
##    1:                        Schorschbräu 10 19.2288235 12.3273042
##    2:                       Shoes Brewery  1 15.2000000  0.0000000
##    3:                Rome Brewing Company  2 13.8400000  1.9718012
##    4:                   Hurlimann Brewery  3 13.7500000  0.5752237
##    5:            Alt-Oberurseler Brauhaus  1 13.2000000         NA
##   ---                                                             
## 5152:             Cerveceria Vegana, S.A.  2  2.2608696  2.2455490
## 5153: Moskovskaya Pivovarennaya Kompaniya  2  2.1500000  1.6881943
## 5154:                      Fentimans Ltd.  3  1.3750000  1.6201852
## 5155:                        Borodino ZAO  2  0.9666667  0.4041452
## 5156:                    All Stars Bakery  1  0.5000000  0.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create z scores 
abv.table[, zAvgABV := scale(AvgABV)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After visually inspecting the graph on the size of breweries (below), I figured I could just look at breweries that make over five beers (which hopefully wipes out your hipster friend’s “micro brewery” in his basement where he is just trying to make the most potent IPA ever) and then only look at beers that score 4 standard deviations above the mean of all beers in terms of ABV content to narrow down possible.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# How many beers to count for a big brewery? 
hist(NoOfBeers$N, breaks= 200, 
     main = &amp;quot;Distribution of Size of Breweries&amp;quot;, 
     xlab = &amp;quot;Number of Beers Produced by a Brewery&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-i_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NoOfBeers[N &amp;gt; 200] # Clearly some big breweries here! &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                        brewery_name   N
## 1:    Minneapolis Town Hall Brewery 243
## 2:            Goose Island Beer Co. 304
## 3:   Iron Hill Brewery &amp;amp; Restaurant 269
## 4: Rock Bottom Restaurant &amp;amp; Brewery 522&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abv.table[N &amp;gt;= 5, ][order(-AvgABV)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                              brewery_name   N    AvgABV
##    1:                                        Schorschbräu  10 19.228824
##    2: Brasserie Grain d&amp;#39; Orge (Brasserie Jeanne d&amp;#39;Arc SA)  10 12.445860
##    3:                          Brauerei Schloss Eggenberg  14 11.779681
##    4:                     Brasserie Dubuisson Frères sprl  14 11.432746
##    5:                            Kuhnhenn Brewing Company 142 11.345839
##   ---                                                                  
## 2539:                             Berliner Kindl Brauerei  12  3.532627
## 2540:  Yanjing Pijiu (Guilin Liquan) Gufen Youxian Gongsi   5  3.440000
## 2541:                                            Ochakovo  16  3.203150
## 2542:                        Grogg&amp;#39;s Pinnacle Brewing Co.   6  3.200000
## 2543:                                        Deka Brewery   7  2.620000
##            SdABV   zAvgABV
##    1: 12.3273042 10.235332
##    2:  1.7054879  5.048898
##    3:  3.0759353  4.539520
##    4:  1.6583471  4.274244
##    5:  3.5788003  4.207793
##   ---                     
## 2539:  1.0372607 -1.766396
## 2540:  0.6426508 -1.837221
## 2541:  1.2735986 -2.018323
## 2542:  0.0000000 -2.020731
## 2543:  1.6356553 -2.464215&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(abv.table[N &amp;gt;= 5, ][order(-AvgABV)]$zAvgABV,
     xlab = &amp;quot;z Score of ABV&amp;quot;, 
     main = &amp;quot;Distribution of ABV in Breweries that make more than 5 Beers&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-i_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abv.table[N &amp;gt;= 5 &amp;amp; zAvgABV &amp;gt; 4, ][order(-AvgABV)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                           brewery_name   N   AvgABV
## 1:                                        Schorschbräu  10 19.22882
## 2: Brasserie Grain d&amp;#39; Orge (Brasserie Jeanne d&amp;#39;Arc SA)  10 12.44586
## 3:                          Brauerei Schloss Eggenberg  14 11.77968
## 4:                     Brasserie Dubuisson Frères sprl  14 11.43275
## 5:                            Kuhnhenn Brewing Company 142 11.34584
##        SdABV   zAvgABV
## 1: 12.327304 10.235332
## 2:  1.705488  5.048898
## 3:  3.075935  4.539520
## 4:  1.658347  4.274244
## 5:  3.578800  4.207793&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Doing it this way puts Schorschbräu as the highest ABV brewery, which makes sense because &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/6513/51466/?ba=wordemupg&#34;&gt;they claim to make the world’s strongest beer&lt;/a&gt;. Making a quick plot of the data for our winner and the second place finisher, we see how strong Schorschbräu really is.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;schor.abv &amp;lt;- beer.complete[brewery_name == &amp;quot;Schorschbräu&amp;quot;, 
                           .(beer_name = unique(beer_name)), by = beer_abv]

ggplot(schor.abv, aes(x = beer_name, y = beer_abv)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;)  + 
  labs( title = &amp;quot;Schorschbräu Beer ABV&amp;quot;, x = &amp;quot;Beer Name&amp;quot;, y = &amp;quot;ABV&amp;quot;) +
   theme(axis.text.x=element_text(angle = -90, hjust = 0)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-i_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;brassOrg.abv &amp;lt;- beer.complete[brewery_name == &amp;quot;Brasserie Grain d&amp;#39; Orge (Brasserie Jeanne d&amp;#39;Arc SA)&amp;quot;, 
                              .(beer_name = unique(beer_name)), by = beer_abv]

ggplot(brassOrg.abv, aes(x = beer_name, y = beer_abv)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;)  + 
  labs( title = &amp;quot;Brasserie Grain d&amp;#39; Orge Beer ABV&amp;quot;, 
        x = &amp;quot;Beer Name&amp;quot;, 
        y = &amp;quot;ABV&amp;quot;) +
   theme(axis.text.x=element_text(angle = -90, hjust = 0)) + ylim(0, 60) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-i_files/figure-html/unnamed-chunk-8-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I think that saying Schorschbräu is technically correct here, but after sharing my findings with my one beer drinking friend he pointed out that one thing that this analysis did not take into account was the how that beers that are traditionally brewed to have a higher ABV (like IPAs and Belgiums) might skew my results. So if you are a big IPA brewery, you are going to have higher average ABV because of the beers you decide to brew!&lt;/p&gt;
&lt;p&gt;So in the true spirit of that data science venn diagram noting that data scientists need to be flexible in incorporating others’ domain knowledge, I did another analysis to just show how much an answer can change depending on how you change your operationalization of the question!&lt;/p&gt;
&lt;p&gt;Let’s do another one!&lt;/p&gt;
&lt;p&gt;First up for this one is making a plot of the data to see how much beers actually vary from type to type. (Excuse the messy plot for now, I thought it would render fine, I’ll fix it.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get mean and SD of each beer type 
abv.by.type &amp;lt;- beer.complete[ , .(MeanAbvType = mean(beer_abv), 
                                  SdAbvType = sd(beer_abv)), 
                              by = beer_style]

# For Graphing, order, set style as factor 
ordered.abv.by.type &amp;lt;- abv.by.type[order(-MeanAbvType)]
ordered.abv.by.type$beer_style &amp;lt;- factor(ordered.abv.by.type$beer_style, levels = ordered.abv.by.type$beer_style)

# Average ABV by beer type
ggplot(ordered.abv.by.type, aes(x = beer_style, y = MeanAbvType)) + 
  geom_bar(stat=&amp;quot;identity&amp;quot;) +   coord_flip() +
  labs(title = &amp;quot;Average ABV by Type of Beer&amp;quot;,
       x = &amp;quot;Beer Style&amp;quot;,
       y = &amp;quot;Mean ABV, bars represent SD&amp;quot;) +
  geom_errorbar(aes(ymin=MeanAbvType-SdAbvType, ymax=MeanAbvType+SdAbvType)) +
  theme_bw() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-i_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So you can see here that if you wanted to have a higher ABV on average for your brewery, you’d benefit from having more IPAs, Barley Wines, and Belgian Stouts.&lt;/p&gt;
&lt;p&gt;Now with average ABV for each beer, let’s then match that to our big list, find how each beer fairs against its own category, sort them, and then combine them with our information from before on how big the brewery is. For the purposes of this example, let’s only look at breweries that have over 100 beers in the database and look the top 20.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Combine ABV per type data with complete data
beer.complete.avg.abv &amp;lt;- abv.by.type[beer.complete, on = &amp;quot;beer_style&amp;quot;]
# Make new z score variable based on other beers in group
beer.complete.avg.abv[, zABV := (beer_abv-MeanAbvType)/SdAbvType]  
# Get averages per brewery on z variable
zAvgBeers &amp;lt;- beer.complete.avg.abv[, .(AvgAbvZ = mean(zABV)), by = brewery_name][order(-AvgAbvZ)]
# Combine back with our data on proxy of size of brewery
BreweryAndAvgAbv &amp;lt;- zAvgBeers[NoOfBeers, on = &amp;quot;brewery_name&amp;quot;]
# And the winner is...
BreweryAndAvgAbv[N &amp;gt; 100][order(-AvgAbvZ)][1:25]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                               brewery_name     AvgAbvZ   N
##  1:               Kuhnhenn Brewing Company  1.26003627 142
##  2:                     Cigar City Brewing  0.87721989 171
##  3:                             The Bruery  0.74965237 143
##  4:     Three Floyds Brewing Co. &amp;amp; Brewpub  0.71875251 128
##  5: Flossmoor Station Restaurant &amp;amp; Brewery  0.53182569 114
##  6:                     Brouwerij De Molen  0.51695738 119
##  7:               Jackie O&amp;#39;s Pub &amp;amp; Brewery  0.40179915 105
##  8:                          Mikkeller ApS  0.37012567 184
##  9:               Founders Brewing Company  0.24770929 130
## 10:                      Deschutes Brewery  0.19390121 127
## 11:      Port Brewing Company / Pizza Port  0.19357914 194
## 12:                     Fitger&amp;#39;s Brewhouse  0.18931699 111
## 13:                       Bullfrog Brewery  0.10590796 121
## 14:                Victory Brewing Company  0.09970522 107
## 15:         Iron Hill Brewery &amp;amp; Restaurant  0.03563186 269
## 16:                  Goose Island Beer Co.  0.01475446 304
## 17:                Sly Fox Brewing Company -0.01733499 140
## 18:              Sierra Nevada Brewing Co. -0.07417651 121
## 19:                      Stone Brewing Co. -0.08646265 119
## 20:              Cambridge Brewing Company -0.12326285 124
## 21:       Rock Bottom Restaurant &amp;amp; Brewery -0.26029542 522
## 22:                Willimantic Brewing Co. -0.28003004 134
## 23:     John Harvard&amp;#39;s Brewery &amp;amp; Ale House -0.29598861 151
## 24:          Minneapolis Town Hall Brewery -0.41963873 243
## 25:                                     NA          NA  NA
##                               brewery_name     AvgAbvZ   N&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at this list, we get a totally different answer. It appears that on average &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/2097/&#34;&gt;Kuhnhenn Brewing Company&lt;/a&gt; brews their beers 1.26 standard deviations above the mean of all other beers in that category!&lt;/p&gt;
&lt;p&gt;Both answers could be technically correct, but more imporantly demonstrate how important it is to come up with how you frame your question first, and then try to answer it so you don’t end up going on a fishing expedition!&lt;/p&gt;
&lt;p&gt;Moving on to question #2!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Looking For Musicologists on Twitter</title>
      <link>/post/looking-for-musicologists-on-twitter/</link>
      <pubDate>Mon, 20 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/looking-for-musicologists-on-twitter/</guid>
      <description>&lt;p&gt;For the most part, Twitter is full of garbage. But I’m an optimist and a firm believer in &lt;a href=&#34;https://en.wikipedia.org/wiki/Sturgeon%27s_law&#34;&gt;Sturgeon’s Law&lt;/a&gt; so by that logic there must be some good on it. That good is academic twitter.&lt;/p&gt;
&lt;p&gt;While this isn’t a post advocating for academic Twitter, I did want to&lt;br /&gt;
1. see if I could figure out how to write a post with some R code in it and 2. share how I scraped Twitter to find active users in the Musicology and Music Theory community&lt;/p&gt;
&lt;p&gt;So here it goes…&lt;/p&gt;
&lt;p&gt;The first thing that you have to do is get some tweets. Luckily some packages exist in the #rstats world that can help with this. For this project I used the &lt;a href=&#34;https://cran.r-project.org/web/packages/twitteR/twitteR.pdf&#34;&gt;twitteR&lt;/a&gt; package which lets you log into Twitter’s API via R and and search it. There are already some instructions on how to get started with it that you can find &lt;a href=&#34;https://davetang.org/muse/2013/04/06/using-the-r_twitter-package/&#34;&gt;here&lt;/a&gt;, so I won’t go into tons of detail about setting it up. (Also please note you can’t just copy and paste my code verbatim since it requires credentials from &lt;em&gt;your&lt;/em&gt; own Twitter account)&lt;/p&gt;
&lt;p&gt;Let’s first load the two packages we’ll need.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)
library(twitteR)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next up, we need to access Twitter’s API by entering in the details from the link above. I find it’s easiest to copy and paste each of my keys and tokens into a nice little character string, assign those to an object, then call those objects in the last command in this block.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;consumer_key &amp;lt;- &amp;#39;YOUR CONSUMER KEY HERE&amp;#39;
consumer_secret &amp;lt;- &amp;#39;COPY AND PASTE YOUR CONSUMER SECRET HERE&amp;#39;
access_token &amp;lt;- &amp;#39;THEN PUT YOUR ACCESS TOKEN HERE&amp;#39;
access_secret &amp;lt;- &amp;#39;AND YOUR ACCESS SECRET HERE&amp;#39;
setup_twitter_oauth(consumer_key, consumer_secret, access_token=NULL, access_secret=NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running that last line in the chunk should then direct you to your default browser. This will log you into your Twitter account and R will ask for Twitter’s permission to enter through the back door.&lt;/p&gt;
&lt;p&gt;The next bit of code won’t run the way I have it set up because Twitter doesn’t let you download tweets older than a week old. So if you want to play with tweets from a conference’s hashtag or some event, make sure to think ahead to download them!!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;amsTwitter &amp;lt;- searchTwitter(&amp;quot;#smt2017&amp;quot;, n = 700)
amsTwitter &amp;lt;- searchTwitter(&amp;quot;#amsroc17&amp;quot;, n = 1600)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This line above searches Twitter for anything matching the conference hashtags and saves the output of it in a list. You can also include an argument asking for a certain number of tweets, which I’ve also done. Luckily the twitteR package has a function that will take this list and convert it to a data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;amsTwitter.df &amp;lt;- twListToDF(amsTwitter)
smtTwitter.df &amp;lt;- twListToDF(smtTwitter)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these nice data frames, we’ll soon be able to join them together and count up some tweets! In order to do this we can take advantage of the &lt;a href=&#34;https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html&#34;&gt;data.table&lt;/a&gt; package to join our two tables together. Of course there are other ways, but Ben over at &lt;a href=&#34;https://gormanalysis.com/&#34;&gt;Gorm Analytics&lt;/a&gt; sold me on data.table this past summer and since then I have really been loving its easy syntax.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;amsTwitter.dt &amp;lt;- data.table(amsTwitter.df)
smtTwitter.dt &amp;lt;- data.table(smtTwitter.df)
amstweets &amp;lt;- amsTwitter.dt[, .(amsTweets = .N), by=screenName][order(-amsTweets)]
smttweets &amp;lt;- smtTwitter.dt[, .(smtTweets = .N), by=screenName][order(-smtTweets)]
totalTweets &amp;lt;- merge(smttweets,amstweets, on =&amp;quot;screenName&amp;quot;, all = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first thing the above code does is swap our data frames over to data.tables. Once they are in the data.table format, we can count up the tweets by screen name, then list them from biggest to smallest all in the same line. From there we merge the two together via the shared column, making sure to grab every instance in each table since not every Tweeter tweeted with both hashtags.&lt;/p&gt;
&lt;p&gt;We then need to clean up some of the NAs (which as a data.table are characters!) in our bigger dataset with R’s ifelse() function that basically works exactly like an ifelse statement would in Microsoft Excel. It looks over a column in your dataset, checks if a value is an NA, if it is then it gives it a 0, if not, it puts in the value that was there in the first place. After replacing NAs, I then make a new variable that adds together both columns then run our final line that prints out our final dataset from top to bottom.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;totalTweets$smtTweets &amp;lt;- ifelse(test = is.na(totalTweets$smtTweets),
                                yes = 0,
                                no = totalTweets$smtTweets) 
totalTweets$amsTweets &amp;lt;- ifelse(test = is.na(totalTweets$amsTweets),
                                yes = 0,
                                no = totalTweets$amsTweets) 

totalTweets[, TotalTweets := smtTweets + amsTweets]
totalTweets[order(-TotalTweets)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here it was simply a matter of using an &lt;a href=&#34;http://www.convertcsv.com/csv-to-html.htm&#34;&gt;online converter&lt;/a&gt; to turn it our final table an html file and then ssh it up to our &lt;a href=&#34;http://musiccog.lsu.edu/&#34;&gt;Music Cognition at LSU&lt;/a&gt; server! Since then I’ve also added both the 2017 &lt;a href=&#34;https://musiccog.lsu.edu/davidjohnbaker/data/amsmt17twitterdata/AmtTwitterData.csv&#34;&gt;AMS&lt;/a&gt; and &lt;a href=&#34;https://musiccog.lsu.edu/davidjohnbaker/data/amsmt17twitterdata/SmtTwitterData.csv&#34;&gt;SMT&lt;/a&gt; datasets that I used to generate the counts in case you want to try this for yourself.&lt;/p&gt;
&lt;p&gt;If anyone has any questions on this, please &lt;a href=&#34;https://twitter.com/DavidJohnBaker&#34;&gt;tweet me&lt;/a&gt;!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Calculating iMultisets in R</title>
      <link>/post/calculating-imultisets-in-r/</link>
      <pubDate>Sun, 19 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/calculating-imultisets-in-r/</guid>
      <description>&lt;p&gt;November is pretty much the worst month for people in higher education. There are too many deadlines and if you’re still in coursework (like myself) you have essays to write, presentations to make, and a backlog of homework assigments to grade. So if you can save time here or there, it’s usually a good choice.&lt;/p&gt;
&lt;p&gt;This weekend I was working a homework assingment for my Transformational Theory seminar where we were given a number of pairs of pitch class sets and had to calculate the imultiset for each following Joseph Straus’ 2014 article on &lt;a href=&#34;http://www.mtosmt.org/issues/mto.14.20.2/mto.14.20.2.straus.html&#34;&gt;Total Voice Leading&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As I looked at the top of the assigment (pictured below) and started to crank out the first one by hand, I realized that the next 30 minutes of my life were going to be doing the same thing over and over again.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/imultiset.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Usually if I get that feeling my next thought is “Can I make a computer do this?” and after thinking about it for two minutes I realized the answer was yes.&lt;/p&gt;
&lt;p&gt;So instead of doing all of these by hand, I wrote an R script and with the time saved figured I’d write a quick post about it.&lt;/p&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;In order to calculate the imultiset you need two pitch class sets, in this case X and Y. Each set can have any number of pitch classes in them and what you need to do is calculate the distance in &lt;a href=&#34;http://openmusictheory.com/mod12.html&#34;&gt;Modulo 12 space&lt;/a&gt; between every possible combination of pitch classes from one set to the other. So for example, you could move from 4 (E in Mod 12 for you non-music theory readers) to 7, 11, 2, or 5 (G, B, D, or F) resulting in four intervals: {3,7,10,1}. These four numbers in the {curly braces} are what you get when you subtract each number in the second set from the note E in Mod 12 space. This action then needs to be completed for every pair.&lt;/p&gt;
&lt;p&gt;When you need to account for every pairing you need to do a &lt;a href=&#34;https://www.w3resource.com/sql/joins/cross-join.php&#34;&gt;cross join&lt;/a&gt;. A cross join connects each member of one set to each member of another set. This creates the sets of pairs seen below.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/imultisetcj.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Then all you need to do is subtract one from the other to get the distance. The only problem is that these subtractions need to happen in Mod 12 space so in any case where you are subtracting a bigger number from a smaller number you will get a negative result! This is easily fixed by just adding 12 to that number in order to get what we &lt;em&gt;should&lt;/em&gt; have been our answer if we were doing Mod 12 arithmatic.&lt;/p&gt;
&lt;p&gt;After fixing the Mod 12 problem, you’ll have a nice list of intervals that just have to be sorted from top to bottom to have your imultiset. So let’s see how you would do this line by line.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R Code&lt;/h2&gt;
&lt;p&gt;First, let’s get two pitch class sets. In this case we have a C major triad and a G dominant chord.&lt;/p&gt;
&lt;p&gt;X = {0,4,7} and Y = {7,11,2,5}&lt;/p&gt;
&lt;p&gt;Let’s first assign each chord to an object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- c(0,4,7)
Y &amp;lt;- c(7,11,2,5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we need to do that cross join, which we can accompish with R’s merge() function. This makes us a data frame with every combination from set X and set Y. Below we see the function’s output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Example &amp;lt;- merge(X,Y,all=TRUE)
Example&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    x  y
## 1  0  7
## 2  4  7
## 3  7  7
## 4  0 11
## 5  4 11
## 6  7 11
## 7  0  2
## 8  4  2
## 9  7  2
## 10 0  5
## 11 4  5
## 12 7  5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once having each combination, we then subtract one set from the other. Since I don’t know how to put R into Music Theory Mode where it only operates in Mod 12, we can fix the problem of the negative numbers by just indexing through our answer with in ifelse() statement to replace any negative values with the answer we actually want by adding 12 to it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Example$diff &amp;lt;- Example$y - Example$x
Example$diff&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  7  3  0 11  7  4  2 -2 -5  5  1 -2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Example$mod12 &amp;lt;- ifelse(test = Example$diff &amp;lt; 0 , 
                        yes = Example$diff + 12, 
                        no = Example$diff)
Example$mod12&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  7  3  0 11  7  4  2 10  7  5  1 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With our numbers then in Mod 12 space, we just sort them and we get our imultiset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sort(Example$mod12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  0  1  2  3  4  5  7  7  7 10 10 11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course you are not going to want to write this out every time you want to calculate an imultiset, so best to just write a function that does what we just did.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calculate.multiset &amp;lt;- function(x,y){
  array.1 &amp;lt;- x 
  array.2 &amp;lt;- y
  cross.join &amp;lt;- merge(array.1,array.2, all = TRUE)
  cross.join$diff &amp;lt;- cross.join$y - cross.join$x
  cross.join$mod12 &amp;lt;- ifelse(cross.join$diff &amp;lt; 0, cross.join$diff + 12, cross.join$diff)
  sort(cross.join$mod12)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
