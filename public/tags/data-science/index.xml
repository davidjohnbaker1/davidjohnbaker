<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science | David John Baker</title>
    <link>/tags/data-science/</link>
      <atom:link href="/tags/data-science/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Science</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020</copyright><lastBuildDate>Tue, 30 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Data Science</title>
      <link>/tags/data-science/</link>
    </image>
    
    <item>
      <title>Begin the Fugue, Part I</title>
      <link>/post/begin-the-fugue-part-i/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/begin-the-fugue-part-i/</guid>
      <description>


&lt;p&gt;The past few months have been pretty exhausting.
On top of all things lockdown and the global pandemic, a little over a month ago I was told that my job was “at risk” of redundancy.
After going through the full collective consultation period, today is my last official day working as an employee of Flatiron School.
While this has been a bit of a shock, I’ve been thinking about my time at Flatiron these past few weeks and as my first prelude comes to its final &lt;a href=&#34;https://en.wikipedia.org/wiki/Cadence#:~:text=In%20a%20perfect%20authentic%20cadence,found%20at%20structurally%20defining%20moments.&#34;&gt;PAC&lt;/a&gt; and brace for the subsequent fugue&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, I always find that writing about what is on my mind really helps me see the bigger picture.&lt;/p&gt;
&lt;p&gt;Looking over everything I did mull over, it’s quite long so I’ve broken it up into three parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What Did I Just Do?: A prosaic setting of my resume&lt;/li&gt;
&lt;li&gt;Vocationtional Vicissitudes: My thoughts on why vocationalism versus the liberal arts helps me understand higher education&lt;/li&gt;
&lt;li&gt;My Dream AD (Applicant’s Description): Things I Might Want to do Next with My Job&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As always, thanks for caring about me and reading my blog posts.&lt;/p&gt;
&lt;div id=&#34;what-did-i-just-do&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What Did I Just Do?&lt;/h2&gt;
&lt;p&gt;For the past nine months I have been a Lead Instructor of Data Science at the London campus of the Flatiron School.
During my time there, I’ve led four different cohorts of students through an intensive fifteen week program where students with not much experience in coding or statistics or machine learning immerse themselves in order to learn as much as they possibly can from their instructors, the written curricula, as well as their peers.&lt;/p&gt;
&lt;p&gt;The bootcamp approach to learning is extremely different from that of traditional, liberal arts, higher-ed and while it can get a bad reputation for not being as exhaustive as spending four years doing something (duh), there are many things that bootcamps really excel at such as having so many learners concentrated into one space, all at about the same level of understanding, with near constant access to instructors to help guide a herd of overly eager learners.&lt;/p&gt;
&lt;p&gt;At Flatiron School, I was mainly responsible for delivering material for the first two modules they took which were “Python for Data Science” and “Probability and Statistics” and was supported by a team of four wonderful people.&lt;/p&gt;
&lt;p&gt;Many of us are now also on the job market now and if you are reading this and want to pick up the scraps of a fantastic team, go add these people on LinkedIn. All you have to do is click the links below. They’re all fantastic educators and even better data scientists.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/wachirandaiga/&#34;&gt;Wachira Ndagia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/danielsanzbecerril/&#34;&gt;Dan Sanz Becerril&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/javier-f-s/&#34;&gt;Javi Fernandez Suarez&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/ioanapr/&#34;&gt;Ioana P&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I also lectured a bit on other topics that I personally like a lot such as clustering, principal components analysis, and was always the one to give my obligatory Introduction to R and the Tidyverse workshop. We taught mostly Python, so I wouldn’t have been able to sleep at night without introducing them to R.&lt;/p&gt;
&lt;p&gt;As of today, we’ve graduated a sizeable batch of students and have placed many of them in jobs.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;
We’re not at 100% yet like our &lt;a href=&#34;https://hi.flatironschool.com/rs/072-UWY-209/images/Flatiron%20School%20Jobs%20Report.pdf?_ga=2.75036818.1013998130.1593420746-1423311562.1570117877&#34;&gt;Software Engineering program was able to do&lt;/a&gt;, but I get messages from students when they eventually get jobs and it’s always the best part of my day.
Our students have gone on to work from companies ranging from start-ups, to educational programs like Flatiron School, to more high visibility brands like Nielsen and the Bank of England.&lt;/p&gt;
&lt;p&gt;In addition to all things teaching, I also got my first taste of being part of senior management, so I got to be on the hiring side of things for once.
I now understand what people mean when they say &lt;a href=&#34;https://jobs.telegraph.co.uk/article/does-your-cv-pass-the-30-second-test-/#:~:text=Research%20shows%20that%20most%20employers,CV%20was%20just%206.25%20seconds.&#34;&gt;people will only look at your resume less than a minute&lt;/a&gt;.
Being on the other side of the interviewing power-dynamic was surprisingly one of the most valuable experiences I will remember going forward.
I’ve also felt how the volatility of a business’ performance directly affects one’s work since our parent company was WeWork.
There were definitely ups and downs.&lt;/p&gt;
&lt;p&gt;Near the end of my time, I also got to help out with a bit of the curriculum development (I got to write some of the R materials for our graduate’s continuing education) that you can check out on the newly minted &lt;a href=&#34;https://davidjohnbaker.rbind.io/teaching/&#34;&gt;Teaching&lt;/a&gt; section of my website.&lt;/p&gt;
&lt;p&gt;It was a lot of work a lot of the time (especially at the start of the role and then again when we had to jump online) and it was 180 degrees in the direction of what I was doing before (writing my dissertation alone in my flat).
And although it wasn’t what I thought I would be doing straight after my PhD, I learned a ton from the work and found many aspects of the job very enjoyable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-good&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Good&lt;/h2&gt;
&lt;p&gt;By far, the best part of the job was working with the bootcamp students.
This group of students was by far the most engaged collective of learners I have ever worked with.
To contextualise this, prior to teaching data science at Flatiron, the students from the two areas subjects in higher education I had experience with were both music theory and aural skills (a subset of music notorious for having undergraduates historically question the content area’s utility) as well as statistics for doctoral level psychologists (where a majority of the class was clinical psychologists who, for good reason, question both the quantification, reficication, and reductionism of quantitative methods).&lt;/p&gt;
&lt;p&gt;In contrast to undergraduate music majors and clinical psychologists&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, none of the students at Flatiron were in my classes because they were “required to for graduation” and instead most of them had quit their jobs hoping to change the entire trajectory of their career.
You fork out a lot of cash to do programs like this and that’s not something they took lightly as students and thus it was not something we took lightly as instructors.&lt;/p&gt;
&lt;p&gt;As a result of this large time and financial investment, the students would absolutely rinse myself and the other instructors for questions every day to the point that by 6:00PM I was totally exhausted from so much face-to-face work.
This constant engaging with instructors and asking as many questions as possible is actually one of the big appeals of this kind of flavor of bootcamp; there are not many other educational settings where students have so much direct access to instructors and peers who are all on the same page.&lt;/p&gt;
&lt;p&gt;While we did our best to set boundaries with students it was hard not to engage with people who were so eager to learn.
The real trick that I eventually learned was to try to get the students to essentially bootstrap each other’s learning and move away from the sage-on-the-stage approach and move over towards as much group and active learning as possible.
I learned a lot about giving up control in the classroom and what you get when you start to acknowledge the fact that when you have adult learners, they bring with them far more life experience than those fresh out of school.&lt;/p&gt;
&lt;p&gt;As a result of all this rapid learning, the last major thing that I loved about this job was getting to see so many students grow so quickly during their time as a bootcamp student.
Of course it’s true that you’re never going to learn everything in a bootcamp, but these environments are designed to get someone from near zero to competence so they can get in their own self-directed learning feedback loops where they can start critically thinking rather than adopting a pumping them full of knowledge approach.&lt;/p&gt;
&lt;p&gt;Now teaching in a bootcamp setting was not like anything I had taught in before.
The goal wasn’t to teach people this history of science or competing philosophies of science throughout the 20th century (though I did weave a lot of that in my lectures, not gonna lie); it was straight up about getting people the tools they need to get a job.&lt;/p&gt;
&lt;p&gt;For this reason, I’ve now spent a lot of time thinking about the idea of vocationalism versus liberal arts education which I elaborate on in &lt;a href=&#34;https://davidjohnbaker.rbind.io/post/begin-the-fugue-part-ii/&#34;&gt;Part II&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;In keeping with the metaphor I used when I wrote about joining Flatiron&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I don’t want to go into the numbers I keep here since that probably counts as sensative information to post publicly, but the students are doing well. We’ll have to wait for our next third part audit for the official numbers.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Obviously there were some in each group that were really into each class, I don’t mean to portray this as apathy&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Begin the Fugue, Part II</title>
      <link>/post/begin-the-fugue-part-ii/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/begin-the-fugue-part-ii/</guid>
      <description>


&lt;div id=&#34;vocationtional-vicissitudes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vocationtional Vicissitudes&lt;/h2&gt;
&lt;p&gt;After existing in this bootcamp space for the past few months, I feel like I am walking away with a very fresh take on what it means to teach in a vocational, rather than a liberal arts setting.
When the goal of the teaching is unabashedly “be useful in a neoliberal vocation” context, there’s a lot less theory (of course something I personally wasn’t the biggest fan of for hopefully obvious reasons) and much more honesty about what this specific type of student wanted out of their education: new opportunities for employment.&lt;/p&gt;
&lt;p&gt;I want to be very clear that I do not think that this approach should be the default goal of education (even though that is the new constant pressure), but now having experienced living in this bizarro world where success is defined solely by employment I now have a lot more opinions about the “value/values” demarcation in education.
In many ways, I very much agree with many goals of Flatiron in that there is a huge hole in the way things are set up now for vocational job training that doesn’t fit the traditional guild/apprentice model (electrician, plumber) and there should be more opportunities for people to accomplish this if they want.
This is especially true if it will lead to a marked difference in quality of life when it comes to their income.
No one told me that working in tech can lead to such and it’s especially wrong for that field to have been dominated by white men for so long.
This &lt;a href=&#34;https://insights.stackoverflow.com/survey/2020&#34;&gt;absurdly large differences in income&lt;/a&gt; has been benefiting people (white dudes) in tech for a long time, and opening it up to others via education is imporant.
t makes me very happy to have worked for a company that has made this a priority and has allocated &lt;a href=&#34;https://flatironschool.com/access/&#34;&gt;scholarship money for those that are underrepresented in tech&lt;/a&gt; and to have been able to pass on those opportunities to people I know personally.&lt;/p&gt;
&lt;p&gt;Of course the idealist in me would go so far as to try to steer the next part of this conversation away from idea of social mobility (why don’t we just pay people all more than living wages and avoid classist ideas that value white collar above blue collar work) but the realist in me knows that this structure is entrenched in Western society right now so if there is a way to open the door to making more money and better quality of life for people through work, then people who can open that door need to open it as wide as humanly possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;emulating-ideas-from-vocational-education&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Emulating Ideas from Vocational Education&lt;/h2&gt;
&lt;p&gt;Reflecting on the vocational versus liberal arts really has me thinking about that and what the future of what higher education could be.
Many of my academic colleagues know the pressure of being asked to demonstrate “transferable skills” in their curricula.
Students rightly want to be able to make a living for themselves when they graduate.&lt;/p&gt;
&lt;p&gt;But if we just follow the job market and placements as our only metric for success, I feel as though we really betray the entire goal of education.
&lt;a href=&#34;https://en.wikipedia.org/wiki/Bullshit_Jobs&#34;&gt;If you do think that, you need to get yourself a copy of David Graeber’s Bullshit Jobs immedietly&lt;/a&gt;.
Now I don’t want to fall prey to Rule Number 2 of grad school (“Be on the lookout for false dichotomies”) thinking we can’t have both, but as more pressure is put on higher education to use employment as a &lt;a href=&#34;https://www.google.com/search?q=kpi&amp;amp;oq=kpi&amp;amp;aqs=chrome..69i57j0l6j46.664j0j4&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&#34;&gt;KPI&lt;/a&gt; (a new term I learned this year) there is a lot higher ed can and should take from the vocational/bootcamp model.
If so much can be accomplished in so little time that can lead to better chances at employment, I am sure there is time during a four year undergraduate degree to equip them with what they need for employment (if they want to do this) that is not just a few resume review sessions.&lt;/p&gt;
&lt;p&gt;I know this might not be as big of a deal for some of my STEM colleagues or students, but I do think about what this would look like if there was some sort of vocational option for music students (my home field).
The irony is also not lost on me that when I talk about music students, I understand the historical links to a music conservatory, which again is all about going on to “work” as a professional musician (having lived that life myself).
But the reality that every one of my conservatory friends found post-graduation is that it is really, really hard to have a “normal life” as a performing musician.
You can gig a lot and do a fair bit of teaching, but you often find that many of your colleagues are bank rolled by their wealthy spouses or parents.
And on top of that, the hours don’t align with the “working day” (which I know not everyone wants anyway).
Or you can go on tour and basically give up your “normal life”.&lt;/p&gt;
&lt;p&gt;If you do this, you can be in the position to make a living, but it’s not exactly normal.
I know things are a bit different for Music Ed, but I’m more thinking about the many jobs that people with really fantastic music educations could do, but either don’t know exist or have zero job preparation to get themselves that role.
For example, the graduate student in musicology who has their finger on the pulse of the fantastic ideas going around academic circles, but would never have anything in their curricula about putting theory into practice at something like an arts organisation.&lt;/p&gt;
&lt;p&gt;I could easily imagine what a summer intensive might look like for music students (especially grad students) looking to pick up the skills they would need to go alt.
I am sure there are tons of major orchestras or larger arts organizations that would benefit tremendously from music students who are like what I just described, but lack the mental model as to what power tools they need to be effective.
Just learning basic data literacy, enough programming to wrangle data and make convincing visualizations for the board, and tools for thinking to not make rookie inferential mistakes could all be learned in a setting like this.
Of course how I am thinking about it takes much more of a quant focus than what would traditionally be offered in a music school setting, but that’s kind of the point in that you’d be able to do both.
I can’t emphasize enough how much computer and research/data literacy (not even mastery) can add value to your organization when you couple it with the pre-existing expertise you have of your domain.&lt;/p&gt;
&lt;p&gt;I am sure a small group of students who just wanted to learn these in-demand vocational skills over the summer or an alt-semester, they’d be ripe and ready by the end of it to go on and help arts organizations or work somewhere else.
Maybe even set it up to lead to internships or something?
The new skill set would give them comfort in knowing all their eggs are not in the one basket that is music performance or the “academic job market” while not losing the culture of total immersion in music.
Just a little bizarro world like the one I have been living in for the past year for them to enter for a few months to give them something different.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Of course I don’t know who is going to pay for that or if students would be interested, but it’s got my mind ticking after seeing what can be done in three months when you just stare into vocationalism.&lt;/p&gt;
&lt;p&gt;The whole experience teaching in this context thing really felt like if you’ve had the chance to learn a second language (non-natively) like being a white kid learning Spanish in secondary school and even though you’re learning Spanish, you’re also learning more about English because you had no idea what you took for granted about your own language.
I will definitely be re-doing many parts of my teaching statement after the past nine months here.&lt;/p&gt;
&lt;p&gt;I won’t be able to implement these zany ideas anytime soon, but I did want to put them out there now since I think this push for vocational skills within the liberal arts will only become stronger as funding continues to drop for higher education and students withdraw from enrolling until they can go back on campus.&lt;/p&gt;
&lt;p&gt;I’ll just end this section again here with repeating that I do not think that this emphasis on jobs should be the new standard for higher ed, but rather stealing ideas from the bootcamp model, annexing it as something separate for students who are interested, and then actually pulling this idea that liberal arts degrees need to be about employability out of the curricula so we can focus on deeper issues than just getting jobs.&lt;/p&gt;
&lt;p&gt;Now knowing I won’t be doing this next, what might I be?&lt;/p&gt;
&lt;p&gt;This takes me to &lt;a href=&#34;https://davidjohnbaker.rbind.io/post/begin-the-fugue-part-iii/&#34;&gt;Part III of this series&lt;/a&gt; of me having a lot of time to think the past month, but no ability whatsoever to concentrate on my professional research.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I can already hear the response of “There is no room for this in the curricula! We barely can let go of Set Theory!” Maybe one day it can be a fun Twitter conversation.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Begin the Fugue, Part III</title>
      <link>/post/begin-the-fugue-part-iii/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/begin-the-fugue-part-iii/</guid>
      <description>


&lt;div id=&#34;whats-next&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What’s Next&lt;/h2&gt;
&lt;p&gt;Having coached many people to apply and get their first jobs in the world of data science, I have now seen A LOT of job postings and understand that a JD (job description) is more of a wish list of an ideal candidate rather than a required listing of skills.
On top of all of that, the characteristics you want out of your future colleagues (like all of the patience and kindness required to sit with a student as you explain how git works, a skill that our instructors seemed to have infinite amounts of, again add ,&lt;a href=&#34;https://www.linkedin.com/in/wachirandaiga/&#34;&gt;Wachira Ndagia&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/danielsanzbecerril/&#34;&gt;Dan Sanz Becerril&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/javier-f-s/&#34;&gt;Javi Fernandez Suarez&lt;/a&gt; or &lt;a href=&#34;https://www.linkedin.com/in/ioanapr/&#34;&gt;Ioana P&lt;/a&gt; on LinkedIn if you want this, you don’t even have to send a message) do not show up on resumes.&lt;/p&gt;
&lt;p&gt;So in that spirit, I want to end this post with posting my own description of what I’d like in my own work, maybe I can think of it as an AD (applicant’s dream!).&lt;/p&gt;
&lt;p&gt;I know I won’t hit everything that I’ll list here for my next line of work, but I have a way better idea of what I want out of my next employment having worked in my past role.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;applicants-dream-ad&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Applicants Dream (AD)&lt;/h2&gt;
&lt;p&gt;The first, and probably most important, aspect I want of my next job is one where I am challenged in a way that leads to daily personal growth.
I found my work at Flatiron School to be challenging, but doing the same fifteen week loop over and over again so quickly made it feel more like I was getting better at some insane Shaun T HIIT work-out in terms of my teaching endurance rather than doing some sort of brain bulk (I hope my next employer likes colorful metaphors).&lt;/p&gt;
&lt;p&gt;The second thing I really want is to be around people who are obsessed with what they do.
As soon as I was told that my job was “at risk” of being redundant, my first thought was thinking about applying for some sort of academic jobs.
Rationally, this is probably not the greatest idea considering the incoming COVID Higher Ed crisis, but I still felt this pull.&lt;/p&gt;
&lt;p&gt;Why?&lt;/p&gt;
&lt;p&gt;Well, I spent a lot of time reflecting and thinking about this very irrational thought and after sitting with it for a couple of days I think that the main reason I feel this pull is because I know that academia selects for people who like and value the same weird stuff that I do.&lt;/p&gt;
&lt;p&gt;At this point in life I feel OK telling people that I have extremely nerdy and niche interests compared to many people.
In conversations with many of my friends, I know that I like to dive much deeper into my interests and am the kind of person who reads textbooks for fun. (#iamverysmart&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;)
I want to be around other people who are like that.&lt;/p&gt;
&lt;p&gt;Of course this has its serious downsides, I find it really, really difficult to have hobbies that don’t turn into obsessions and projects, but at least I am aware of that and OK with that now.
That is kind of what academia is and it’s nice to know that I am going to have traits like that in common with my future colleagues from Day One if I were to work in that setting, but I have learnt (in more of a feeling, rather than intellectual way) that academia is not the only place that this type of personality exists.
Again, this is very much like the learning Spanish analogy from before having worked somewhere where I didn’t think I would have.&lt;/p&gt;
&lt;p&gt;Now I think it’s becoming a lot more common for many academics to work in industry after coming to this realisation.
At this juncture in life (and considering the future of the academic “job market”), it seems a bit wiser to still keep an eye open to whatever opportunities come up that are close to what I find meaningful, rather than getting too parochial too soon.&lt;/p&gt;
&lt;p&gt;So right now, what I really want to do next is keep investing in my technical skills, ideally in the area of where my domain expertise is which is music.
At Flatiron School, every so often when students would ask me a question where I didn’t know the answer right away, I had to remind them that the knowledge that I have about the world of data science is really only a by-product of wanting to answer questions that I have found personally meaningful in my own work.&lt;/p&gt;
&lt;p&gt;At the end of the day, I didn’t learn all this coding and stats stuff so I could just teach it (never in 100 years would I have thought I’d be doing that in my first year of my PhD); I learned it so I could ask really obscure questions about the the hierarchical structure of western tonal music in a subset of a WEIRD population known as Western conservatory musicians who are learning solfege.
That’s what I currently really like.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Is there market value in knowing that? Can I make the “business case” for that?
At this point, I’m sure I could spin it so there was something, but I’m OK with it standing on its own.&lt;/p&gt;
&lt;p&gt;But I know that no one will pay me to do that (unless I can maybe secure some postdoc funding in that?) but I’m planning on using a lot of my time in the future to devote to my WEIRD interests.&lt;/p&gt;
&lt;p&gt;Then lastly, going forward to my next line of work it would be really nice to have a manager who was as kind as my old manager.
I remember being totally dumbfounded at the idea of “onboarding” coming from an academic background (hello baptism by fire with hidden curricula abound!).
The amount of support and questions and effort my manager went through to get to know me as I was starting was something I will take with me if I am ever in that position.&lt;/p&gt;
&lt;p&gt;Then lastly, it would be nice, but obviously not necessary to have all the perks of working in industry like having a salary that allows you to be more generous with your money, no obligation to work off the clock, free coffee and beer (that will probably never happen again), and remote work options.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Now a lot of this sounds like maybe the next step here is to maybe do a postdoc or try to secure a job doing music industry, but I would not be so bold with the amount of uncertainty that everyone is facing right now to say I know exactly what is next.
Even though I might know what I want, I’m fully aware that I need temper my expectations.&lt;/p&gt;
&lt;p&gt;And if I can’t land something right away, I have a lot of personal research projects I want to attend to in the near future and the fact that I want to spend my free time working on these probably is a big hint of what I should look at next.
I also have three fat textbooks I want to read this summer and have plans to brush the dirt off a lot of my programming.&lt;/p&gt;
&lt;p&gt;Lastly, I’d just like to end by saying that I had a great experience in something that I didn’t anticipate and just wanted to say that out loud for anyone else that reads this and might be in a similar position.
It felt like such a commitment signing my contract here, like I’d given up on what I wanted to do.
But here I am again, thinking about weird research in a much more stable position, using my free time to write papers and submit to conferences I find interesting.&lt;/p&gt;
&lt;p&gt;Though I guess the real end of this should be if anyone knows of any gigs or jobs, either in teaching or research, please get in touch.
I’m cleared to work both in the United Kingdom and the United States.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;This is ironic for people who are not on the internet as much as I am.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I love teaching too, don’t get me wrong, but I’m hoping my pendulum swings a bit more near towards the research side next.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hire Me (as a Data Scientist!), Part IV</title>
      <link>/post/hire-me-as-a-data-scientist-part-iv/</link>
      <pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/post/hire-me-as-a-data-scientist-part-iv/</guid>
      <description>


&lt;p&gt;Since this is my last post in the beer review series, I’ll keep this short and sweet in terms of analysis.
Having done all of this, I do have a few reflections I would like to share after doing the &lt;a href=&#34;https://www.linkedin.com/pulse/how-hire-test-data-skills-one-size-fits-all-interview-tanya-cashorali/&#34;&gt;One-Size-Fits-All-Data-Science Interview&lt;/a&gt; that I have included at the end.&lt;/p&gt;
&lt;p&gt;Our final question to answer is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lastly, if I typically enjoy a beer due to its aroma and appearance, which beer style should I try?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a pretty broad question and should be able to be answered without many pitfalls, so let’s get started.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(data.table)
beer &amp;lt;- fread(&amp;quot;data/beer_reviews.csv&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From a bird’s eye view, it seems like the most sensible thing to do would be to look at our data
from the highest level, then just zoom in until we have the level of granularity we feel answers
the question well.
Let’s first average all the beer styles to get a rough estimate of how a beer style fairs
on the variables we are interested in, and additionally see how much variability is associated
with that measurement.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sem &amp;lt;-function(x) {sd(x)/sqrt(length(x))}

question4.means &amp;lt;- beer[, .(mean_aroma = mean(review_aroma), mean_appearance = mean(review_appearance), mean_overall = mean(review_overall),
                            sem_aroma = sem(review_aroma), sem_appearance = sem(review_appearance), sem_overall = sem(review_overall),
                            sd_aroma = sd(review_aroma), sd_appeareance = sd(review_appearance),sd_overall = sd(review_overall)), 
                        by = beer_style]
question4.means&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                           beer_style mean_aroma mean_appearance mean_overall
##   1:                      Hefeweizen   3.761735        3.828293     3.929626
##   2:              English Strong Ale   3.749427        3.852469     3.783288
##   3:          Foreign / Export Stout   3.828366        4.039015     3.877679
##   4:                 German Pilsener   3.387159        3.572444     3.731573
##   5:  American Double / Imperial IPA   4.097782        4.078916     3.998017
##  ---                                                                        
## 100:                          Gueuze   4.117574        4.034864     4.086287
## 101:                            Gose   3.783528        3.908163     3.965015
## 102:                        Happoshu   2.595436        2.925311     2.914938
## 103:                           Sahti   3.827992        3.655985     3.700283
## 104: Bière de Champagne / Bière Brut   3.734704        4.045889     3.648184
##        sem_aroma sem_appearance sem_overall  sd_aroma sd_appeareance sd_overall
##   1: 0.003668940    0.003595729 0.004051038 0.6129217      0.6006912  0.6767538
##   2: 0.008118012    0.007674182 0.009323636 0.5623738      0.5316275  0.6458931
##   3: 0.007222404    0.006830955 0.008163490 0.5581381      0.5278874  0.6308640
##   4: 0.004611304    0.004323963 0.005097580 0.6863721      0.6436027  0.7587521
##   5: 0.001937927    0.001600133 0.002171618 0.5682357      0.4691883  0.6367582
##  ---                                                                           
## 100: 0.007225256    0.006450020 0.008273156 0.5600855      0.4999910  0.6413163
## 101: 0.019413627    0.015860893 0.023754558 0.5084740      0.4154222  0.6221699
## 102: 0.048722437    0.051373864 0.063538226 0.7563756      0.7975368  0.9863785
## 103: 0.019516104    0.017677122 0.021691778 0.6356980      0.5757968  0.7065662
## 104: 0.021782360    0.018920362 0.026781986 0.7044834      0.6119209  0.8661809&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Knowing how each beer style fluctuates on our variables of interest (with our overall score
thrown in for good measure!), let’s plot our results.
Note that I have included standard error of the mean error bars as a sanity check to make sure
that each beer’s ratings is not going to bleed into the others’ dimensions.
By doing this, we can have a bit more confidence that we are looking at actually has
some meaning.
This plot shows the standard error on each of the two variables we are interested in, and for the
most part it looks as if they are relatively well contained.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(question4.means, aes(x = mean_aroma, y = mean_appearance, color = beer_style)) + 
  geom_point() +
  geom_errorbar(aes(ymin=mean_appearance-sem_appearance, ymax=mean_appearance+sem_appearance), width=.1) +
  geom_errorbarh(aes(xmin=mean_aroma-sem_aroma, xmax=mean_aroma+sem_aroma)) + theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(title = &amp;quot;Mean Appearance and Aroma&amp;quot;, y = &amp;quot;Mean Aroma&amp;quot;, x = &amp;quot;Mean Appearance&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-29-hire-me-as-a-data-scientist-part-iv_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This graph has a lot of beers, but what we are interested in is that top right quadrant where both
the average appearance and aroma are maxed out.
Let’s zoom in on it and throw in a sizing variable of the overall rating and inspect our graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(question4.means[mean_appearance &amp;gt; 4 &amp;amp; mean_aroma &amp;gt; 4], 
       aes(x = mean_aroma, y = mean_appearance, color = beer_style, size = mean_overall)) + 
  geom_point() + xlim(4,4.2) + ylim(4,4.25) +  theme(legend.position=&amp;quot;none&amp;quot;) +
 # geom_errorbar(aes(ymin=mean_appearance-sem_appearance, ymax=mean_appearance+sem_appearance), width=.1) +
#  geom_errorbarh(aes(xmin=mean_aroma-sem_aroma, xmax=mean_aroma+sem_aroma)) + theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(title = &amp;quot;Mean Appearance and Aroma&amp;quot;, y = &amp;quot;Mean Aroma&amp;quot;, x = &amp;quot;Mean Appearance&amp;quot;) + 
  geom_text(aes(label=beer_style, hjust = .5, vjust = -.75)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-29-hire-me-as-a-data-scientist-part-iv_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at this subsection, it appears we have a few choices for beers that score highly on
both Appearance and Aroma.
The American Double / Imperial Stout looks like a good option as it scores higher on how it
looks, but our Russian Imperial Stout has a higher Aroma score.
We could start crunching more numbers here to find “the best” option, or at this point we could
take off our data science hats and put our psychology ones back on (assuming that’s what we were wearing…) and run some double-blind experiments on ourselves to make sure that our data is actually lining up with something in the real world.&lt;/p&gt;
&lt;div id=&#34;reflections&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reflections&lt;/h2&gt;
&lt;p&gt;What started out as a fun weekend project actually turned into a really great learning experience.
I’ve definitely put a few solid hours into this and have gotten a lot out of it.
I learned that my friends actually know a TON about beer and data science, that &lt;a href=&#34;https://git-lfs.github.com/&#34;&gt;git-lfs&lt;/a&gt; is something I wish I would have known about earlier,
and that I’m actually looking forward to doing more blogging in the future.&lt;/p&gt;
&lt;p&gt;I will probably have to go MIA for the next two weeks since my General Exams are coming up in
early February, but I’m sure I will be back at it come late March.
Until then, all I can hope for is that someone who is looking to hire a junior data scientist over the
summer will come across these posts and think I might be a good temporary addition to
their team.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hire Me (as a Data Scientist!), Part III</title>
      <link>/post/hire-me-as-a-datascientist-part-iii/</link>
      <pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/post/hire-me-as-a-datascientist-part-iii/</guid>
      <description>


&lt;p&gt;Two questions down, two to go!
For the third post I’ll explore the question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Which of the factors (aroma, taste, appearance, palate) are most important in determining the overall quality of a beer?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Whereas the posts before were questions on sorting data, this is our first attempt to make some statistical models.
In this case, we’re going to be doing a bit of regression modeling.&lt;/p&gt;
&lt;p&gt;There are a couple of ways to tackle this problem.
We could run some basic linear regression models and spend the whole post talking beta coefficients and what assumptions we violated.
We could do a linear mixed effects model, then realize that doing so would be a terrible choice (I tried it for the fun of it, bad idea).
Or we do a fun non-parametric, machine learning model that is on the easier-to-interpret side.
Since machine learning is so hot right now, let’s stick with that.&lt;/p&gt;
&lt;div id=&#34;machine-learning-and-non-parametric-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Machine Learning and Non-Parametric Models&lt;/h2&gt;
&lt;p&gt;Non-parametric models make no assumptions about the data.
The models do not assume that our points come from beautiful, normally distributed ideal populations; they just seek to find some way to give us a good rule of thumb about what is happening under the hood.&lt;/p&gt;
&lt;p&gt;In this case, we need to make a model to predict the quality of beer (our dependent variable, &lt;code&gt;review_overall&lt;/code&gt;) based on four different independent variables (&lt;code&gt;review_aroma&lt;/code&gt;, &lt;code&gt;review_taste&lt;/code&gt;, &lt;code&gt;review_appearance&lt;/code&gt;, &lt;code&gt;review_palate&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;After discussing the pros and cons of certain methods for tackling this problem, my friend &lt;a href=&#34;https://www.linkedin.com/in/tabitha-trahan-172471b4/&#34;&gt;Tabi&lt;/a&gt;, the data scientist over at &lt;a href=&#34;https://www.soundout.com/&#34;&gt;Soundout&lt;/a&gt;, suggested that an easy way to get the answer I was looking for was to use a random forest model.
Some great explanations about how these models work can be found &lt;a href=&#34;http://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://www-bcf.usc.edu/~gareth/ISL/&#34;&gt;here&lt;/a&gt;, and &lt;a href=&#34;https://gormanalysis.com/random-forest-from-top-to-bottom/&#34;&gt;here&lt;/a&gt; and since this isn’t a post about how random forest models work, I’ll just note that basically the idea is that it is an algorithm that partitions your dataset into dimensions that help us either classify or predict observations in our dataset based on the variables you feed it.
Relevant to our question: the ways in which the algorithm splits our dataset is going to help us figure out what are the important variables.&lt;/p&gt;
&lt;p&gt;Before running this model though, we need to talk about a small dependence problem in our dataset.
In my earlier post, I talked about how we probably should not just run a model on the data &lt;em&gt;as is&lt;/em&gt;.
Last time we found there were tons of NAs in our dataset, that not all beers were equally represented, and that not all reviewers were making even amounts of reviews.
In addition to these problems, there was also the problem that some reviewers might rate generally higher or lower &lt;em&gt;all the time&lt;/em&gt;.
Since we know things like this exist, we wanted to account for them.&lt;/p&gt;
&lt;p&gt;The simplest plan of action would be to try and make each of the points we want to model independent by averaging ratings across all beers so we don’t have cases where one person’s influence is spread across multiple beers.
We also make sure to only use beers that have over 30 ratings as a quality check.
Let’s index out the data we need!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(data.table)
beer &amp;lt;- fread(&amp;quot;data/beer_reviews.csv&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make READABLE unique beer ID
beer[, beer_name_unique := paste(brewery_name,beer_name, beer_style)]

# Only beers with over 30 reviews 
reliable.beers &amp;lt;- beer[, .(ReviewsBeerHas = .N), by = beer_name_unique][order(-ReviewsBeerHas)][ReviewsBeerHas &amp;gt; 30]

# Merge with Inner Join 
beer.reliable &amp;lt;- reliable.beers[beer, on = &amp;quot;beer_name_unique&amp;quot;, nomatch=0]

# Make independent ratings 
prediction.data &amp;lt;- beer.reliable[, .(AvgOverall = mean(review_overall),
         AvgAroma = mean(review_aroma),
         AvgTaste = mean(review_taste),
         AvgApp = mean(review_appearance),
         AvgPal = mean(review_palate)), by = beer_name_unique]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have some better data, let’s fit a random forest model.
We are now attempting to predict the average overall rating from our four others measures.
We’ll use the &lt;code&gt;randomForest&lt;/code&gt; package and make sure to ask it to include a measure of variable importance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(randomForest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## randomForest 4.6-14&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Type rfNews() to see new features/changes/bug fixes.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;randomForest&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     margin&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(666) 
random.forrest.fit &amp;lt;- randomForest(AvgOverall ~ AvgAroma + AvgTaste + AvgApp + AvgPal, data = prediction.data, importance = TRUE)
random.forrest.fit$importance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             %IncMSE IncNodePurity
## AvgAroma 0.02293972      219.2851
## AvgTaste 0.09678963      304.5727
## AvgApp   0.01566894      198.3329
## AvgPal   0.05588260      276.6567&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The randomForest object gives us two different measures when it comes to variable importance.
The first one, &lt;code&gt;%IncMSE&lt;/code&gt;, is a measure that tells us how much our model’s Mean Square Error (MSE) would change if we were to take that variable out of our model.
Average taste here is trouncing the other variables in terms of importance.
The second variable, &lt;code&gt;IncNodePurity&lt;/code&gt; gives us a measure of node purity from all of the trees that were used in creating our model.
Here Taste again leads in terms of importance, but our Palate variable seems to be close behind.
We see this visually below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;varImpPlot(random.forrest.fit, main = &amp;quot;Variable Importance Metrics&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-28-hire-me-as-a-datascientist-part-iii_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The result that Taste seems to be taking the cake here is to be expected.
Using some of the more basic tools of statistics and data science, we can look at the correlation between our taste variable and our overall, and it’s quite high.
Actually looking at our average ratings, all of the correlations are really high!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(prediction.data[,-1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            AvgOverall  AvgAroma  AvgTaste    AvgApp    AvgPal
## AvgOverall  1.0000000 0.8809340 0.9518373 0.8441311 0.9360873
## AvgAroma    0.8809340 1.0000000 0.9593160 0.8971965 0.9370526
## AvgTaste    0.9518373 0.9593160 1.0000000 0.8955728 0.9742888
## AvgApp      0.8441311 0.8971965 0.8955728 1.0000000 0.9124038
## AvgPal      0.9360873 0.9370526 0.9742888 0.9124038 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Doing the analysis with this data presents some strange issues of collinearity.
Having an &lt;em&gt;r&lt;/em&gt; = .951 for our Overall and Taste variables is obnoxiously high.
An &lt;em&gt;r&lt;/em&gt; = .974 for Palette and Taste is also strangely large.
If you come from more of a psychology background, you would almost never see correlations this high in the wild; it would be an immediate sign for concern.&lt;/p&gt;
&lt;p&gt;The correlations go down a bit if you end up looking at the non-aggregated sets too (see below), but again remember that these values have those dependence and outlier issues with them.
Still, &lt;code&gt;review_taste&lt;/code&gt; and &lt;code&gt;review_overall&lt;/code&gt; are the highest correlated variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(beer[, .(review_overall,review_aroma,review_appearance,review_palate,review_taste)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   review_overall review_aroma review_appearance review_palate
## review_overall         1.0000000    0.6160131         0.5017324     0.7019139
## review_aroma           0.6160131    1.0000000         0.5610290     0.6169469
## review_appearance      0.5017324    0.5610290         1.0000000     0.5666339
## review_palate          0.7019139    0.6169469         0.5666339     1.0000000
## review_taste           0.7898156    0.7167761         0.5469804     0.7341351
##                   review_taste
## review_overall       0.7898156
## review_aroma         0.7167761
## review_appearance    0.5469804
## review_palate        0.7341351
## review_taste         1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This effect is probably due to the fact that higher quality beers tend to score higher on everything.
It would be pretty strange in practice to give a beer a 5/5 overall, but then think it’s deserving of a 2/5 in Taste.
If I were on a team at Beer Advocate, I might suggest incorporating either a larger range of ratings (maybe a seven point scale) or maybe thinking about different dimensions to ask people to rate like ‘hoppiness’ or bang-for-your-buck.
Variables like these would allow us to learn more about the beers since they would have less collinearity issues.&lt;/p&gt;
&lt;p&gt;So the take-home here is that the taste variable is the most important variable for determining the overall rating and again we’re reminded about how messy this data is!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hire Me (as a Data Scientist!), Part II</title>
      <link>/post/hire-me-as-a-data-scientist-part-ii/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/post/hire-me-as-a-data-scientist-part-ii/</guid>
      <description>


&lt;p&gt;Continuing on from my earlier post, I’m now looking to tackle the question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you had to pick 3 beers to recommend using only this data, which would you pick?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a pretty open ended question, which is kind of fun.
I also don’t really have a ton of experience (yet!) in recommendation systems, though I have done a little reading here or there on it.&lt;/p&gt;
&lt;p&gt;My goals in coming up with three beers to recommend were to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Try to find the most popular beer among super users of the website&lt;/li&gt;
&lt;li&gt;Find a &lt;a href=&#34;https://youtu.be/IcjSDZNbOs0?t=31s&#34;&gt;bizzaro&lt;/a&gt; beer that matched the profile of my first beer, but lives in the &lt;a href=&#34;https://books.google.com/books?hl=en&amp;amp;lr=&amp;amp;id=DTeZAAAAQBAJ&amp;amp;oi=fnd&amp;amp;pg=PT6&amp;amp;dq=anderson+2006+long+tail&amp;amp;ots=MpaGpMbdfD&amp;amp;sig=25QPk_RCCNU2yFoo9nsU0hrt0sc#v=onepage&amp;amp;q=anderson%202006%20long%20tail&amp;amp;f=false&#34;&gt;long tail&lt;/a&gt; of the ratings distribution&lt;/li&gt;
&lt;li&gt;Find the best Beer sans Booze (Highest Rating with lowest ABV)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So let’s begin!
Here’s how I went about tackling this question.&lt;/p&gt;
&lt;div id=&#34;popular-with-super-users&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Popular with Super Users&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#=====================================================================================#
# Following suit of the last post... 
#=====================================================================================#
# Library
library(ggplot2)
library(data.table)
library(stringr)
#=====================================================================================#
beer &amp;lt;- fread(&amp;quot;data/beer_reviews.csv&amp;quot;)
beer.complete &amp;lt;- beer[complete.cases(beer)]
#=====================================================================================#&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having more experience in experimental settings, one of the first things I needed to get used to when I started working with non-psychology datasets was the lack of complete sets in what felt like almost everything.
Whereas in the &lt;a href=&#34;https://musiccog.lsu.edu/&#34;&gt;lab&lt;/a&gt; we spend lots of time trying to design balanced studies that hopefully don’t violate the litany of assumptions that classic null hypothesis significance testing demands, my first few attempts at analyzing large amounts of data made me realize it’s almost risible to think that you’re going to have even, independent data, ever.
This dataset is no different.&lt;/p&gt;
&lt;p&gt;Of all of the unique users on the site, most of them have done only a couple of reviews, but some have essentially made a job out of this.
Looking at the distribution of reviews, this is quite clear.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;review.counts &amp;lt;- beer[, .(.N), by = review_profilename][order(-N)]
review.counts &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        review_profilename    N
##     1:     northyorksammy 5817
##     2:      BuckeyeNation 4661
##     3:        mikesgroove 4617
##     4:          Thorpe429 3518
##     5:      womencantsail 3497
##    ---                        
## 33384:          beilfussd    1
## 33385:         MPHSours11    1
## 33386:         jennaizzel    1
## 33387:           hogshead    1
## 33388:            joeebbs    1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(beer[, .(.N), by = review_profilename][order(-N)]$N, 
     breaks = 200,
     xlab = &amp;quot;Number of Reviews&amp;quot;,
     main = &amp;quot;Distribution of Reviews Per User&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-ii_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is pretty important when it comes to modeling the data (discussed in Part III), and not being fully aware of where your ratings are coming from could put the quality of your models at serious risk.&lt;/p&gt;
&lt;p&gt;So looking at this dataset, I wondered if there were any sort of implicit assumptions I could make about this data that might be able to help me find a good beer.
One assumption that I didn’t think was too wild was that a sample of this population that had gone out of its way to rate over 500 beers was probably more of a beer expert than those who have only done a couple of reviews on the site.&lt;/p&gt;
&lt;p&gt;One thing I wanted to check was: of all the 1.5 million reviews, where were they coming from? Were there enough reviews among the super users that I could use?
And what made someone a super user?
I could have been a bit more scientific, setting an &lt;em&gt;a priori&lt;/em&gt; threshold, but for this I kind of just looked at that chart above, spit balled thinking 500 might be a good number to check, and then went to see how much of the data would be accounted if I put my threshold there.
I lucked out and got about half of it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(review.counts$N) # Number of Total Reviews &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1586614&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(review.counts[ N &amp;gt; 500]$N) # Number of Reviews from Super Users&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 731066&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;731066/1586614 # Percent of Total Reviews from 500+ Super Users&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4607712&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.users &amp;lt;- review.counts[ N &amp;gt;  500] # I can settle for .75 Million Reviews&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now I had a list of the users who had completed over 500 reviews and made up 46% of our entire data.
I could use this new table I had made to index through our dataset of all the reviews that I had (that have their ABV ratings!) so I was then only dealing with these higher quality reviewers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews &amp;lt;- super.users[beer.complete, on = &amp;quot;review_profilename&amp;quot;, nomatch=0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As I continued to chop down the dataset (since this was a very exploratory process compared with cleaning up an experiment), it was important to do &lt;strong&gt;quality assessment&lt;/strong&gt; steps.
One thing worth checking here was to see if I was actually dealing with beer omnivores in our super users and make sure that all different types of beers were being represented in our smaller subset.
This was done by just looking at the number of rows between the original dataset and our super user table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews[, .(beer_styles = unique(beer_style))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                          beer_styles
##   1:                      Hefeweizen
##   2:              English Strong Ale
##   3:          Foreign / Export Stout
##   4:                 German Pilsener
##   5:  American Double / Imperial IPA
##  ---                                
## 100:             Japanese Rice Lager
## 101:                      Roggenbier
## 102:                        Happoshu
## 103:                           Sahti
## 104: Bière de Champagne / Bière Brut&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer[, .(beer_styles = unique(beer_style))]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                          beer_styles
##   1:                      Hefeweizen
##   2:              English Strong Ale
##   3:          Foreign / Export Stout
##   4:                 German Pilsener
##   5:  American Double / Imperial IPA
##  ---                                
## 100:                          Gueuze
## 101:                            Gose
## 102:                        Happoshu
## 103:                           Sahti
## 104: Bière de Champagne / Bière Brut&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Luckily they were the same.
If I were to really do some more work on this dataset, I would also want to check things such as how many of the beers had each super user tried?
Were there IPA experts in the group?
If yes, should their opinions be taken more seriously if I had questions about IPA recommendations in the future?
But for now, I just set out to see what the highest rated beer among all the super users of this dataset was.&lt;/p&gt;
&lt;p&gt;In order to answer that question, I had to find out which beer in specific had the highest mean rating.
The dataset ‘as is’ comes with a &lt;code&gt;beer_id&lt;/code&gt; unique ID, but the data downloaded as is does not give us a key to this, so I had to make it myself.
This was accomplished by just pasting together the brewery’s name, along with the beer name, and style into a new variable.&lt;/p&gt;
&lt;p&gt;As another &lt;strong&gt;quality assurance&lt;/strong&gt; step, it was worth checking to see if this recreated the unique ID variable, which it didn’t do exactly… but it was pretty close.
I would chalk that up to some sort of encoding error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews[, beer_name_unique := paste(brewery_name,beer_name, beer_style)]

length(unique(super.reviews$beer_beerid))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 42825&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(unique(super.reviews$beer_name_unique))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 42703&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;42703/42805 # Pretty close&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9976171&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews.popular &amp;lt;- super.reviews[, .(most_reviewed_beers = .N), 
                                       by = beer_name_unique][order(-most_reviewed_beers)]

hist(super.reviews.popular$most_reviewed_beers,
     main = &amp;quot;Distribution of Number of Ratings by Super Users&amp;quot;,
     xlab = &amp;quot;Number of Reviews each Beer Recieves&amp;quot;,
     breaks = 200)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-ii_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again I saw this was clearly &lt;strong&gt;not&lt;/strong&gt; anything resembling a repeated measures experiment and not all beers were rated equally.&lt;/p&gt;
&lt;p&gt;Continuing in the same fashion above, I just grabbed the top 100 beers of our super users and merged that on to our table from earlier that had all of the ratings from our super users.
Then from that table, I took the average of the overall rating and looked at our top ten beers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;super.reviews.popular.100 &amp;lt;- super.reviews[, .(most_reviewed_beers = .N), by = beer_name_unique][order(-most_reviewed_beers)][1:100]

super.reviews.cream.of.crop &amp;lt;- super.reviews.popular.100[super.reviews, 
                                                         on = &amp;quot;beer_name_unique&amp;quot;, 
                                                         nomatch=0]

super.reviews.cream.of.crop[, .(mean_review_overall = mean(review_overall)), by = beer_name_unique][order(-mean_review_overall)][1:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                                                      beer_name_unique
##  1:                      Russian River Brewing Company Pliny The Elder American Double / Imperial IPA
##  2:                  Bayerische Staatsbrauerei Weihenstephan Weihenstephaner Hefeweissbier Hefeweizen
##  3:                              Tröegs Brewing Company Tröegs Nugget Nectar American Amber / Red Ale
##  4:                                 Ballast Point Brewing Company Sculpin India Pale Ale American IPA
##  5:                 Three Floyds Brewing Co. &amp;amp; Brewpub Dreadnaught IPA American Double / Imperial IPA
##  6: Founders Brewing Company Founders KBS (Kentucky Breakfast Stout) American Double / Imperial Stout
##  7:                                                 Bell&amp;#39;s Brewery, Inc. Two Hearted Ale American IPA
##  8:                            Bell&amp;#39;s Brewery, Inc. Bell&amp;#39;s Hopslam Ale American Double / Imperial IPA
##  9:                    Three Floyds Brewing Co. &amp;amp; Brewpub Alpha King Pale Ale American Pale Ale (APA)
## 10:                Founders Brewing Company Founders Breakfast Stout American Double / Imperial Stout
##     mean_review_overall
##  1:            4.536630
##  2:            4.535072
##  3:            4.449084
##  4:            4.443287
##  5:            4.367580
##  6:            4.366876
##  7:            4.353270
##  8:            4.349810
##  9:            4.346652
## 10:            4.334526&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we have our winner!
It’s &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/863/7971/&#34;&gt;Pliny The Elder&lt;/a&gt; from Russian River Brewing Company as my first beer recommendation!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bizzaro-beer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bizzaro Beer&lt;/h2&gt;
&lt;p&gt;Now Pliny The Elder seemed to be a pretty popular beer.
But what if I was trying to sketch out some ideas about what other beers I could recommend to beer lovers who like Pliny The Elder?
It needed to somewhat “look like” the target beer, but have way less reviews.&lt;/p&gt;
&lt;p&gt;Playing with some of the fringe data here, I wanted to be careful not to again pick a beer with only one or two ratings on it.
My rationale was coming from assuming there is some sort of true “population mean” for this beer and having a beer with too little reviews will not approximate the mean correctly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Make Unique Beer Label for Larger Dataset
beer[, beer_name_unique := paste(brewery_name,beer_name, beer_style)]

## Count Number of Reviews Each Beer Has  
number.of.reviews &amp;lt;- beer[, .(NumberOfReviews = .N), by = beer_name_unique][order(-NumberOfReviews)]
 
## Only get beers with over 30 reviews
reliable.beers.list &amp;lt;- number.of.reviews[ NumberOfReviews &amp;gt;= 30 ]

## Join that to our big &amp;#39;beer&amp;#39; dataset only matching beers with over 30 reviews
beer.reliable &amp;lt;- reliable.beers.list[beer, on = &amp;quot;beer_name_unique&amp;quot;, nomatch=0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With our dataset chiseled down to only ‘reliable’ beers, I needed to find a way to get some sort of profile of each of the beers.
While my first instinct was to do some sort of data reductive type thing like a PCA on our continuous variables and use distances from certain scores as metrics of similarity (&lt;a href=&#34;https://musiccog.lsu.edu/davidjohnbaker/papers/Baker_Trahan_Mullensiefen_ProceedingsPaper.pdf&#34;&gt;which I have done before&lt;/a&gt; and it ended up actually being the inspiration for a tool currently used by &lt;a href=&#34;https://www.soundout.com/brandmatch&#34;&gt;Soundout&lt;/a&gt;!), doing that on so few predictors seemed &lt;a href=&#34;https://www.urbandictionary.com/define.php?term=extra&#34;&gt;extra&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So instead, I figured why not just assume that there is some sort of wiggle room in my hastily made recommendation system and just match first on the overall review, then if there are some close contenders, look for matches on other metrics?&lt;/p&gt;
&lt;p&gt;The next bit of code creates a table of the metrics I am interested in, gets beers that have over 30 reviews, but less than 100, and also creates a vector so I can pull out all of the IPAs on my less reviewed beers table.
I then joined the tables for my candidates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# # Get metrics used for distance calculations
beer.metrics &amp;lt;- beer.reliable[, .(mean_review_overall = mean(review_overall),
                                  mean_review_aroma = mean(review_overall),
                                   mean_review_appearance = mean(review_appearance),
                                   mean_review_palate = mean(review_palate),
                                   mean_reviw_taste = mean(review_taste),
                                   sd_review_overall = sd(review_overall),
                                   sd_review_aroma = sd(review_overall),
                                   sd_review_appearance = sd(review_appearance),
                                   sd_review_palate = sd(review_palate),
                                   sd_review_taste = sd(review_taste)),
                               by = beer_name_unique]

## Get only IPAs with less than 100 reviews
less.reviewed.beers &amp;lt;- number.of.reviews[NumberOfReviews &amp;lt;= 100 &amp;amp; NumberOfReviews &amp;gt;= 30]
## Make vector to help find IPAs
find.IPA &amp;lt;- str_detect(string = less.reviewed.beers$beer_name_unique, pattern = &amp;quot;Imperial IPA&amp;quot;)
bizzaro.candidates &amp;lt;- less.reviewed.beers[find.IPA]

#Create Table
bizzaro.candidates.metrics &amp;lt;- bizzaro.candidates[beer.metrics, on = &amp;quot;beer_name_unique&amp;quot;, nomatch=0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of these less reviewed beers, I now needed to find the one that was “closest” on the few dimensions I had to work with.
The simplest way to do this would be to just subtract our target beer (Pliny The Elder), from every other beer in our interested list, then check out the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get metrics for our target beer
rrbcpteadii.metrics &amp;lt;- beer[beer_name_unique == &amp;quot;Russian River Brewing Company Pliny The Elder American Double / Imperial IPA&amp;quot;,
                           .(mean_review_overall = mean(review_overall),
                             mean_review_aroma = mean(review_overall),
                             mean_review_appearance = mean(review_appearance),
                             mean_review_palate = mean(review_palate),
                             mean_reviw_taste = mean(review_taste),
                             sd_review_overall = sd(review_overall),
                             sd_review_aroma = sd(review_overall),
                             sd_review_appearance = sd(review_appearance),
                             sd_review_palate = sd(review_palate),
                             sd_review_taste = sd(review_taste))]
## Create vector for looping over
key.vector &amp;lt;- as.vector(rrbcpteadii.metrics)
## Pull off the tags of our search
search.vector &amp;lt;- bizzaro.candidates.metrics[, -c(1,2)]

## Sanity check that what we are going to subtract has same names
names(key.vector)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;mean_review_overall&amp;quot;    &amp;quot;mean_review_aroma&amp;quot;      &amp;quot;mean_review_appearance&amp;quot;
##  [4] &amp;quot;mean_review_palate&amp;quot;     &amp;quot;mean_reviw_taste&amp;quot;       &amp;quot;sd_review_overall&amp;quot;     
##  [7] &amp;quot;sd_review_aroma&amp;quot;        &amp;quot;sd_review_appearance&amp;quot;   &amp;quot;sd_review_palate&amp;quot;      
## [10] &amp;quot;sd_review_taste&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(search.vector)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;mean_review_overall&amp;quot;    &amp;quot;mean_review_aroma&amp;quot;      &amp;quot;mean_review_appearance&amp;quot;
##  [4] &amp;quot;mean_review_palate&amp;quot;     &amp;quot;mean_reviw_taste&amp;quot;       &amp;quot;sd_review_overall&amp;quot;     
##  [7] &amp;quot;sd_review_aroma&amp;quot;        &amp;quot;sd_review_appearance&amp;quot;   &amp;quot;sd_review_palate&amp;quot;      
## [10] &amp;quot;sd_review_taste&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## And that the apply function I am going to run is doing what I think it will
search.vector[1]- key.vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    mean_review_overall mean_review_aroma mean_review_appearance
## 1:          -0.7900277        -0.7900277             -0.6386031
##    mean_review_palate mean_reviw_taste sd_review_overall sd_review_aroma
## 1:         -0.6263257       -0.8393187        0.09103239      0.09103239
##    sd_review_appearance sd_review_palate sd_review_taste
## 1:            0.1273011          0.11882       0.1073594&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Run apply function
ipa.distances &amp;lt;- apply(search.vector, 1, function(x) x - key.vector)
ipa.distances.dt &amp;lt;- data.table(do.call(rbind.data.frame,ipa.distances))
## Combine this back with vector with names
bizzaro.candidates.distances &amp;lt;- cbind(bizzaro.candidates.metrics, ipa.distances.dt)
## Sort our data by overall and see if we have a good match!
bizzaro.candidates.distances[order(-mean_review_overall)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                                          beer_name_unique
##   1: Hill Farmstead Brewery Galaxy Imperial Single Hop IPA American Double / Imperial IPA
##   2:           Lawson&amp;#39;s Finest Liquids Double Sunshine IPA American Double / Imperial IPA
##   3:        Kern River Brewing Company 5th Anniversary Ale American Double / Imperial IPA
##   4:                           Alpine Beer Company Bad Boy American Double / Imperial IPA
##   5:             Iron Hill Brewery &amp;amp; Restaurant Kryptonite American Double / Imperial IPA
##  ---                                                                                     
## 132:                            BrewDog Sink The Bismarck! American Double / Imperial IPA
## 133:                   Blue Frog Grog &amp;amp; Grill The Big DIPA American Double / Imperial IPA
## 134:                            Hermitage Brewing Hoptopia American Double / Imperial IPA
## 135:                    Florida Beer Company Swamp Ape IPA American Double / Imperial IPA
## 136:            BrewDog Storm (Islay Whisky Cask Aged IPA) American Double / Imperial IPA
##      NumberOfReviews mean_review_overall mean_review_aroma
##   1:              76            4.592105          4.592105
##   2:              85            4.588235          4.588235
##   3:              41            4.475610          4.475610
##   4:              79            4.468354          4.468354
##   5:              44            4.443182          4.443182
##  ---                                                      
## 132:              76            3.197368          3.197368
## 133:              53            2.867925          2.867925
## 134:              32            2.687500          2.687500
## 135:              52            2.615385          2.615385
## 136:              92            2.440217          2.440217
##      mean_review_appearance mean_review_palate mean_reviw_taste
##   1:               4.368421           4.440789         4.559211
##   2:               4.317647           4.311765         4.552941
##   3:               4.329268           4.219512         4.500000
##   4:               4.234177           4.335443         4.474684
##   5:               4.284091           4.318182         4.420455
##  ---                                                           
## 132:               3.835526           3.401316         3.539474
## 133:               3.433962           3.047170         2.801887
## 134:               3.500000           2.890625         2.500000
## 135:               3.442308           3.009615         2.586538
## 136:               2.902174           2.836957         2.706522
##      sd_review_overall sd_review_aroma sd_review_appearance sd_review_palate
##   1:         0.3337716       0.3337716            0.3861642        0.3997258
##   2:         0.3289275       0.3289275            0.3845766        0.4293993
##   3:         0.3865103       0.3865103            0.3641730        0.4749840
##   4:         0.3698733       0.3698733            0.3741267        0.4060561
##   5:         0.3768892       0.3768892            0.3796836        0.4586495
##  ---                                                                        
## 132:         1.2438621       1.2438621            0.7136206        1.1431528
## 133:         0.8995241       0.8995241            0.6866707        0.7355280
## 134:         1.2427207       1.2427207            0.4918694        0.8005479
## 135:         0.9108033       0.9108033            0.5994593        0.7637009
## 136:         0.9829379       0.9829379            0.6843587        0.7883377
##      sd_review_taste mean_review_overall mean_review_aroma
##   1:       0.3826844         0.002077562       0.002077562
##   2:       0.3620669        -0.001792407      -0.001792407
##   3:       0.4873397        -0.114417945      -0.114417945
##   4:       0.3660140        -0.121673270      -0.121673270
##   5:       0.4026537        -0.146845883      -0.146845883
##  ---                                                      
## 132:       1.1277209        -1.392659280      -1.392659280
## 133:       0.8165336        -1.722103173      -1.722103173
## 134:       0.9418581        -1.902527701      -1.902527701
## 135:       1.0035288        -1.974643085      -1.974643085
## 136:       1.0086226        -2.149810310      -2.149810310
##      mean_review_appearance mean_review_palate mean_reviw_taste
##   1:            -0.02018203        -0.01053621      -0.07177483
##   2:            -0.07095603        -0.13956098      -0.07804418
##   3:            -0.05933479        -0.23181349      -0.13098536
##   4:            -0.15442587        -0.11588264      -0.15630181
##   5:            -0.10451218        -0.13314386      -0.21053081
##  ---                                                           
## 132:            -0.55307677        -1.05000989      -1.09151167
## 133:            -0.95464082        -1.40415587      -1.82909857
## 134:            -0.88860309        -1.56070068      -2.13098536
## 135:            -0.94629539        -1.44171030      -2.04444690
## 136:            -1.48642917        -1.61436916      -1.92446362
##      sd_review_overall sd_review_aroma sd_review_appearance sd_review_palate
##   1:       -0.12136909     -0.12136909          -0.01935577      -0.04762635
##   2:       -0.12621327     -0.12621327          -0.02094339      -0.01795284
##   3:       -0.06863039     -0.06863039          -0.04134702       0.02763182
##   4:       -0.08526747     -0.08526747          -0.03139329      -0.04129607
##   5:       -0.07825155     -0.07825155          -0.02583641       0.01129741
##  ---                                                                        
## 132:        0.78872139      0.78872139           0.30810063       0.69580063
## 133:        0.44438341      0.44438341           0.28115074       0.28817587
## 134:        0.78758001      0.78758001           0.08634938       0.35319581
## 135:        0.45566254      0.45566254           0.19393929       0.31634876
## 136:        0.52779720      0.52779720           0.27883874       0.34098558
##      sd_review_taste
##   1:     -0.03307969
##   2:     -0.05369722
##   3:      0.07157561
##   4:     -0.04975012
##   5:     -0.01311039
##  ---                
## 132:      0.71195677
## 133:      0.40076950
## 134:      0.52609404
## 135:      0.58776473
## 136:      0.59285853&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course if I were building a real recommendation machine I could start talking about what factors are more important for what users and what factors are more predictive than others, but this seems like an OK enough solution to at least have completed my &lt;em&gt;a priori&lt;/em&gt; goal.&lt;/p&gt;
&lt;p&gt;Based on this solution, it looks like I will have to find myself a bottle of &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/863/7971/&#34;&gt;Pliny The Elder&lt;/a&gt; and the &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/22511/67760/&#34;&gt;Hill Farmstead Brewery Galaxy Imperial Single Hop IPA American Double / Imperial IPA&lt;/a&gt; and do some of my own empirical work to see if this was a good idea.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;beer-sans-booze&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Beer sans Booze&lt;/h2&gt;
&lt;p&gt;The last beer that I think I wanted to recommend would be one that tastes great, but does not have a lot of alcohol in it.
The reason this question kind of interests me is because if we are &lt;em&gt;really&lt;/em&gt; going to talk about how tasty a beer is, it would be nice to be able to factor out of the equation how drunk we are actually getting from it.&lt;/p&gt;
&lt;p&gt;I can see first of all IF this relationship exists if we look at the mean overall rating of a beer as a function of its ABV content.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer.complete[, beer_name_unique := paste(brewery_name,beer_name, beer_style) ]

# ABVs and Mean Scores
abv.vs.mean &amp;lt;- beer.complete[, .(Abv = mean(beer_abv), MeanOverall = mean(review_overall)), by = beer_name_unique]

ggplot(abv.vs.mean[Abv &amp;lt; 20], aes(x = Abv, y = MeanOverall)) + 
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;, color = &amp;quot;blue&amp;quot;) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, formula = y ~ poly(x,2), color = &amp;quot;orange&amp;quot;) +
  labs(title = &amp;quot;Rating as Function of ABV (Beers with than 20% ABV)&amp;quot;,
       x = &amp;quot;ABV Content&amp;quot;,
       y = &amp;quot;Mean Overall Rating&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-ii_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Surprisingly, when I ran some quick and dirty regression models (that yes, I know violate tons of assumptions) I saw that only a very small amount of variance was being explained by its ABV.
Note that although the models were significant, the R squared values hovered around 3-5%!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# &amp;quot;The More Booze The Better&amp;quot; Model
abv.linear &amp;lt;- lm(MeanOverall ~ Abv, data = abv.vs.mean[Abv &amp;lt; 20]) 
# The &amp;quot;Diminishing Returns Model &amp;quot;
abv.poly &amp;lt;- lm(MeanOverall ~ poly(Abv,2), data = abv.vs.mean[Abv &amp;lt; 20])
# The &amp;quot;Dissapointing Amount of Variance Explained Summaries&amp;quot;
summary(abv.linear)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = MeanOverall ~ Abv, data = abv.vs.mean[Abv &amp;lt; 20])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.06793 -0.27278  0.09684  0.38850  1.72081 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 3.276138   0.008995  364.20   &amp;lt;2e-16 ***
## Abv         0.060975   0.001369   44.55   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.6013 on 48822 degrees of freedom
## Multiple R-squared:  0.03906,    Adjusted R-squared:  0.03904 
## F-statistic:  1984 on 1 and 48822 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(abv.poly)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = MeanOverall ~ poly(Abv, 2), data = abv.vs.mean[Abv &amp;lt; 
##     20])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.88738 -0.28351  0.09695  0.37907  2.17585 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)     3.658087   0.002706 1351.73   &amp;lt;2e-16 ***
## poly(Abv, 2)1  26.785156   0.597970   44.79   &amp;lt;2e-16 ***
## poly(Abv, 2)2 -13.922969   0.597970  -23.28   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.598 on 48821 degrees of freedom
## Multiple R-squared:  0.04961,    Adjusted R-squared:  0.04957 
## F-statistic:  1274 on 2 and 48821 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This actually surprised me and might be worth looking into at a deeper level another time, but for now I want to keep going on and find a beer knowing that how much booze is in it doesn’t really affect how good it is.&lt;/p&gt;
&lt;p&gt;So let’s take one final dive into the dataset, grab only our quality reviews then plot a subset of our data so I can see beers that have a very high overall rating with a very small amount of booze in them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Quality Assurance Step
reliable.and.abv.beers &amp;lt;- reliable.beers.list[beer.complete, on = &amp;quot;beer_name_unique&amp;quot;, nomatch=0]

## Get mean ratings and keep ABV (which won&amp;#39;t change if I average it)
dd.beers &amp;lt;- reliable.and.abv.beers[, .(mean_overall = mean(review_overall), abv = mean(beer_abv)), by = &amp;quot;beer_name_unique&amp;quot;]

# Only Beers that Fit Our Criterion
dd.beers.2 &amp;lt;- dd.beers[mean_overall &amp;gt; 4.6 &amp;amp; abv &amp;lt; 10]

# Plot It!
ggplot(dd.beers.2, aes(x = abv, y = mean_overall, label = beer_name_unique, color = beer_name_unique)) +
  geom_point() +
  geom_text(aes(label=beer_name_unique),hjust=-.01, vjust=0) +
  labs(title = &amp;quot;High Quality Beers with Low ABV&amp;quot;,
       x = &amp;quot;ABV&amp;quot;,
       y = &amp;quot;Mean Overall Rating&amp;quot;) + theme(legend.position = &amp;quot;none&amp;quot;) +
  xlim(0, 20) +
  scale_y_continuous(breaks = c(seq(4.6,5,.1)), limits = c(4.6,4.85))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-hire-me-as-a-data-scientist-part-ii_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These are all OK choices (most of the beers are still above 5% ABV), but we do have one beer clocking in at 2.0% ABV giving us our final beer recommendation – the &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/1628/8626/&#34;&gt;Southampton Publick House Southampton Berliner Weisse Berliner Weissbier&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;After all of this, I know have three beers to check out.
&lt;a href=&#34;https://www.beeradvocate.com/beer/profile/863/7971/&#34;&gt;Pliny The Elder&lt;/a&gt; is our winner for the top rated beer among our Super Users of the site, the &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/22511/67760/&#34;&gt;Hill Farmstead Brewery Galaxy Imperial Single Hop IPA American Double / Imperial IPA&lt;/a&gt; is a beer to maybe follow up on from our first choice, and then lastly we have the the &lt;a href=&#34;https://www.beeradvocate.com/beer/profile/1628/8626/&#34;&gt;Southampton Publick House Southampton Berliner Weisse Berliner Weissbier&lt;/a&gt; which supposedly tastes great, despite its lack of alcohol content.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
